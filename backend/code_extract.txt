================================================================================
AI_ModelEval CODE EXTRACTION
Generated: 2026-02-21 16:40:09
Total files: 71
================================================================================

--------------------------------------------------------------------------------
FILE 1/71: __init__.py
--------------------------------------------------------------------------------

"""AI Model Evaluation Framework."""

__version__ = '1.0.0'

--------------------------------------------------------------------------------
FILE 2/71: API.md
--------------------------------------------------------------------------------

# API Documentation

## Overview

This document provides detailed API documentation for the AI Model Evaluation Framework.

## Core Modules

### 1. Entities (`src.entities`)

Data models representing core concepts in the evaluation framework.

#### User

```python
from src.entities import User

user = User(
    username='user_name',
    email='user@example.com',
    password='password123'
)
```

#### Evaluation

```python
from src.entities import Evaluation

evaluation = Evaluation(
    name='My Evaluation',
    model='ollama',
    dataset='humaneval'
)
```

#### Problem

```python
from src.entities import Problem

problem = Problem(
    title='Problem Title',
    description='Problem description',
    input_spec='Input specification',
    output_spec='Output specification'
)
```

#### EvaluationResult

```python
from src.entities import EvaluationResult

result = EvaluationResult(
    problem_id='p1',
    solution='def f():\n    pass',
    status='passed'
)
```

### 2. Managers (`src.managers`)

Business logic for managing evaluations and aggregating results.

#### EvaluationManager

```python
from src.managers import EvaluationManager

manager = EvaluationManager()
results = manager.run_evaluation(
    loader=dataset_loader,
    adapter=model_adapter,
    timeout=30
)
```

#### ResultAggregator

```python
from src.managers import ResultAggregator

aggregator = ResultAggregator()
aggregated = aggregator.aggregate(results)
stats = aggregator.calculate_statistics(results)
```

### 3. Adapters (`src.adapters`)

Interface implementations for different model providers.

#### ModelAdapter (Abstract Base)

```python
from src.adapters import ModelAdapter

class CustomAdapter(ModelAdapter):
    def load_model(self):
        # Load your model
        pass

    def generate(self, prompt):
        # Generate output
        return response
```

#### ModelRegistry

```python
from src.adapters import ModelRegistry, OllamaAdapter

registry = ModelRegistry()
registry.register('ollama', OllamaAdapter)
adapter_class = registry.get('ollama')
```

### 4. Loaders (`src.loaders`)

Dataset loaders for different benchmarks.

#### DatasetLoader (Abstract Base)

```python
from src.loaders import DatasetLoader

class CustomLoader(DatasetLoader):
    def load(self):
        # Load dataset
        pass

    def get_problems(self):
        # Return problems
        return problems
```

#### HumanEvalLoader

```python
from src.loaders import HumanEvalLoader

loader = HumanEvalLoader()
problems = loader.get_problems()
```

### 5. Executors (`src.executors`)

Safe code execution with resource management.

#### SandboxExecutor

```python
from src.executors import SandboxExecutor

executor = SandboxExecutor()
result = executor.execute('print("hello")')
result = executor.execute_with_timeout('code', timeout=30)
```

#### ResourceManager

```python
from src.executors import ResourceManager

manager = ResourceManager()
manager.set_memory_limit('4GB')
manager.set_cpu_limit('2')
manager.set_timeout(30)
```

### 6. Calculators (`src.calculators`)

Metric calculation for different evaluation types.

```python
from src.calculators import (
    FunctionalMetrics,
    QualityMetrics,
    SemanticMetrics
)

# Calculate Pass@k
functional = FunctionalMetrics()
pass_k = functional.calculate(results, k=1)

# Calculate quality metrics
quality = QualityMetrics()
metrics = quality.calculate(code)

# Calculate semantic metrics
semantic = SemanticMetrics()
similarity = semantic.calculate(generated, reference)
```

### 7. Analyzers (`src.analyzers`)

Error analysis and pattern detection.

```python
from src.analyzers import (
    ErrorAnalyzer,
    PatternDetector,
    FixSuggester
)

# Analyze errors
analyzer = ErrorAnalyzer()
analysis = analyzer.analyze(error)

# Detect patterns
detector = PatternDetector()
patterns = detector.detect_patterns(results)

# Suggest fixes
suggester = FixSuggester()
suggestions = suggester.suggest_fixes(error)
```

### 8. Generators (`src.generators`)

Report generation in multiple formats.

```python
from src.generators import ReportGenerator

generator = ReportGenerator()
report = generator.generate_report(results)
generator.export_report(report, format='html')
```

### 9. Prompts (`src.prompts`)

Prompt strategies and generation.

```python
from src.prompts import PromptEngine

engine = PromptEngine()
prompt = engine.generate_prompt(problem, strategy='zero_shot')
```

### 10. Dashboard (`src.dashboard`)

Web interface for visualization.

```python
from src.dashboard import create_app

app = create_app()
app.run(host='0.0.0.0', port=5000)
```

## Configuration

### Configuration Management

```python
from src.config import config

# Get configuration
timeout = config.get('evaluation.timeout', 30)

# Set configuration
config.set('evaluation.timeout', 60)

# Dictionary-style access
timeout = config['evaluation.timeout']
config['evaluation.memory_limit'] = '8GB'
```

## Utilities

### Disk Space Manager

```python
from src.utils import DiskSpaceManager

manager = DiskSpaceManager()
space = manager.check_available_space()
manager.cleanup_old_results()
```

### Debug Timer

```python
from src.utils import DebugTimer

# Context manager usage
with DebugTimer('operation'):
    # Your code here
    pass

# Manual timing
timer = DebugTimer('operation')
timer.start()
# Your code here
timer.stop()
```

### Validators

```python
from src.utils import Validators

Validators.validate_problem(problem)
Validators.validate_result(result)
Validators.validate_config(config)
```

## Exception Handling

All modules may raise custom exceptions. Handle them appropriately:

```python
try:
    result = executor.execute(code)
except TimeoutError:
    print("Code execution timed out")
except RuntimeError as e:
    print(f"Execution error: {e}")
```

## Examples

See [examples.py](examples.py) for complete working examples.

## Best Practices

1. **Always use context managers** for resource-intensive operations
2. **Validate inputs** before passing to evaluators
3. **Use configuration** for environment-specific settings
4. **Handle exceptions** appropriately in production
5. **Monitor resource usage** when running evaluations
6. **Log important events** for debugging and monitoring

## Troubleshooting

### Common Issues

**Q: Configuration not loading**
A: Ensure `src/config/settings.yaml` exists and is valid YAML

**Q: Model adapter not found**
A: Register the adapter with ModelRegistry before use

**Q: Code execution timeout**
A: Adjust timeout value in configuration or ResourceManager

**Q: Memory issues**
A: Lower memory limits or increase available system memory

## Support

For more help, see:

- [README.md](README.md) - Project overview
- [CONTRIBUTING.md](CONTRIBUTING.md) - Contribution guidelines
- GitHub Issues - Report problems or request features

--------------------------------------------------------------------------------
FILE 3/71: CHANGELOG.md
--------------------------------------------------------------------------------

# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.0] - 2026-02-15

### Added

- Initial release of AI Model Evaluation Framework
- Multi-model support (Ollama, HuggingFace adapters)
- Multiple evaluation metrics (Pass@k, CodeBLEU, code quality)
- Sandboxed code execution with resource management
- Error analysis with pattern detection
- Report generation in multiple formats (HTML, PDF, CSV, JSON)
- Interactive web dashboard
- Comprehensive test suite
- Configuration management system

### Features

- Adapter pattern for extensible model support
- DataLoader abstraction for multiple benchmarks
- Metric calculators for different evaluation types
- Error analysis and fix suggestions
- Resource management and monitoring
- Full logging and debugging capabilities

## [0.1.0] - 2026-02-01

### Added

- Project structure and initial setup
- CRC card based architecture
- Basic entity models

--------------------------------------------------------------------------------
FILE 4/71: conftest.py
--------------------------------------------------------------------------------

"""Pytest configuration and fixtures."""

import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

import pytest


@pytest.fixture
def temp_dir(tmp_path):
    """Fixture providing a temporary directory."""
    return tmp_path


@pytest.fixture
def mock_config():
    """Fixture providing a mock configuration."""
    return {
        'models': ['test-model'],
        'timeout': 30,
        'memory_limit': '4GB'
    }


@pytest.fixture
def sample_problem():
    """Fixture providing a sample problem."""
    return {
        'id': 'test_1',
        'title': 'Test Problem',
        'description': 'A test problem',
        'input_spec': 'int -> int',
        'output_spec': 'int',
        'test_cases': [
            {'input': '5', 'output': '25'}
        ]
    }


@pytest.fixture
def sample_solution():
    """Fixture providing a sample solution."""
    return '''def solution(x):
    return x * x
'''

--------------------------------------------------------------------------------
FILE 5/71: CONTRIBUTING.md
--------------------------------------------------------------------------------

# Contribution Guidelines

Thank you for considering contributing to AI Model Evaluation Framework!

## Getting Started

1. Fork the repository
2. Clone your fork: `git clone https://github.com/yourusername/AI_ModelEval.git`
3. Create a virtual environment: `python -m venv venv`
4. Activate it: `source venv/bin/activate` (or `venv\Scripts\activate` on Windows)
5. Install dev dependencies: `pip install -r requirements-dev.txt`

## Development Workflow

1. Create a feature branch: `git checkout -b feature/your-feature`
2. Make your changes and commit: `git commit -am 'Add your feature'`
3. Push to your fork: `git push origin feature/your-feature`
4. Submit a pull request

## Code Standards

- Follow PEP 8 style guide
- Use type hints for all functions
- Write unit tests for new features
- Ensure all tests pass: `pytest`
- Format code with black: `black src tests`
- Check with pylint: `pylint src`

## Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src

# Run specific test file
pytest tests/test_entities/

# Run specific test
pytest tests/test_entities/__init__.py::TestUser::test_user_creation
```

## Reporting Bugs

Please use the GitHub issue tracker to report bugs. Include:

- Description of the bug
- Steps to reproduce
- Expected vs actual behavior
- Environment details (OS, Python version, etc.)

## Requesting Features

Open a GitHub issue with:

- Clear description of the feature
- Use cases and benefits
- Any relevant implementation ideas

## License

By contributing, you agree that your contributions will be licensed under the MIT License.

--------------------------------------------------------------------------------
FILE 6/71: DOCKER.md
--------------------------------------------------------------------------------

# DOCKER SUPPORT

This document describes how to run the AI Model Evaluation Framework in Docker.

## Quick Start with Docker

### 1. Build the Docker Image

```bash
docker build -t ai-model-eval:latest .
```

### 2. Run the Container

```bash
docker run -it \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/results:/app/results \
  -p 5000:5000 \
  ai-model-eval:latest
```

### 3. Run Specific Commands

```bash
# Run evaluation
docker run ai-model-eval:latest python main.py eval --models ollama

# Start dashboard
docker run -p 5000:5000 ai-model-eval:latest python main.py dashboard
```

## Docker Compose

### Development Setup

```bash
docker-compose up
```

This will start:

- Main application container
- Ollama container (if configured)
- Result storage volumes

### Production Setup

```bash
docker-compose -f docker-compose.prod.yml up
```

## Dockerfile Example

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

ENV PYTHONUNBUFFERED=1
ENV LOG_LEVEL=INFO

CMD ["python", "main.py", "dashboard"]
```

## Volume Mapping

- `/app/data` - Dataset storage
- `/app/results` - Evaluation results
- `/app/config` - Configuration files

## Environment Variables in Docker

```bash
docker run \
  -e LOG_LEVEL=DEBUG \
  -e TIMEOUT_SECONDS=60 \
  -e MEMORY_LIMIT_GB=8 \
  ai-model-eval:latest
```

## Networking

For multi-container setups, services communicate via network names:

```yaml
services:
  app:
    networks:
      - eval-network
  ollama:
    networks:
      - eval-network
networks:
  eval-network:
    driver: bridge
```

## Debugging in Docker

```bash
# View logs
docker logs <container_id>

# Interactive shell
docker exec -it <container_id> /bin/bash

# Check running processes
docker top <container_id>
```

## Resource Limits

```bash
docker run \
  -m 4g \
  --cpus 2 \
  ai-model-eval:latest
```

## GPU Support

```bash
docker run --gpus all \
  ai-model-eval:latest
```

Requires NVIDIA Container Toolkit.

## Health Checks

```dockerfile
HEALTHCHECK --interval=30s --timeout=10s \
  CMD curl -f http://localhost:5000/health || exit 1
```

## Troubleshooting

### Out of memory

- Increase Docker memory limits
- Reduce batch sizes in configuration
- Use external cache storage

### Slow performance

- Check resource allocation
- Verify volume mount performance
- Monitor network latency

### Port conflicts

- Change port mapping: `-p 8000:5000`
- Use dynamic port assignment: `-p :5000`

--------------------------------------------------------------------------------
FILE 7/71: examples.py
--------------------------------------------------------------------------------

"""
Example usage of the AI Model Evaluation Framework.
"""

from src.entities import User, Evaluation, Problem
from src.managers import EvaluationManager
from src.adapters import ModelRegistry, OllamaAdapter, HuggingFaceAdapter
from src.loaders import HumanEvalLoader
from src.generators import ReportGenerator
from src.prompts import PromptEngine
from src.config import config


def example_basic_usage():
    """Example: Basic framework usage."""
    print("=" * 50)
    print("Example 1: Basic Usage")
    print("=" * 50)
    
    # Create a user
    user = User(username='demo_user', email='demo@example.com')
    print(f"Created user: {user.username}")
    
    # Create an evaluation
    evaluation = Evaluation(name='Demo Evaluation', model='demo-model')
    print(f"Created evaluation: {evaluation.name}")
    
    # Create a problem
    problem = Problem(
        title='Hello World',
        description='Write a function that returns "Hello World"',
        input_spec='None',
        output_spec='str'
    )
    print(f"Created problem: {problem.title}")


def example_model_registry():
    """Example: Using model registry."""
    print("\n" + "=" * 50)
    print("Example 2: Model Registry")
    print("=" * 50)
    
    # Create registry and register adapters
    registry = ModelRegistry()
    registry.register('ollama', OllamaAdapter)
    registry.register('huggingface', HuggingFaceAdapter)
    
    print("Registered adapters:")
    for name in ['ollama', 'huggingface']:
        adapter = registry.get(name)
        print(f"  - {name}: {adapter.__name__ if adapter else 'Not found'}")


def example_configuration():
    """Example: Configuration management."""
    print("\n" + "=" * 50)
    print("Example 3: Configuration Management")
    print("=" * 50)
    
    # Access configuration
    timeout = config.get('evaluation.timeout', 30)
    memory = config.get('evaluation.memory_limit', '4GB')
    
    print(f"Evaluation timeout: {timeout}s")
    print(f"Memory limit: {memory}")
    
    # Set configuration
    config.set('evaluation.timeout', 60)
    print(f"Updated timeout: {config.get('evaluation.timeout')}")


def example_evaluation_flow():
    """Example: Complete evaluation flow."""
    print("\n" + "=" * 50)
    print("Example 4: Evaluation Flow (Simplified)")
    print("=" * 50)
    
    # Initialize components
    manager = EvaluationManager()
    loader = HumanEvalLoader()
    registry = ModelRegistry()
    
    print("Initialized evaluation components:")
    print("  - EvaluationManager")
    print("  - HumanEvalLoader")
    print("  - ModelRegistry")
    
    # In a real scenario, you would:
    # 1. Load dataset
    # 2. Register models
    # 3. Run evaluations
    # 4. Aggregate results
    # 5. Generate reports


def example_prompting():
    """Example: Prompt strategies."""
    print("\n" + "=" * 50)
    print("Example 5: Prompt Strategies")
    print("=" * 50)
    
    engine = PromptEngine()
    
    problem = {
        'title': 'Sum two numbers',
        'description': 'Return the sum of two numbers'
    }
    
    print("Problem:", problem['title'])
    print("Strategies available:")
    print("  - Zero-shot")
    print("  - Few-shot")
    print("  - Chain-of-thought")


if __name__ == '__main__':
    print("AI Model Evaluation Framework - Examples")
    print("")
    
    example_basic_usage()
    example_model_registry()
    example_configuration()
    example_evaluation_flow()
    example_prompting()
    
    print("\n" + "=" * 50)
    print("Examples completed!")
    print("=" * 50)

--------------------------------------------------------------------------------
FILE 8/71: main.py
--------------------------------------------------------------------------------

import os
import sys
import asyncio
import logging
import argparse
from pathlib import Path
from typing import Optional

# Add src to path
sys.path.append(str(Path(__file__).parent / 'src'))

# Fix imports - use absolute imports
from src.entities import User, Evaluation, Report, EvaluationResult, Benchmark
from src.managers import EvaluationManager, ResultAggregator
from src.adapters import ModelRegistry
from src.loaders import HumanEvalLoader
from src.executors import SandboxExecutor, ResourceManager
from src.calculators import (
    MetricCalculator,
    FunctionalMetricsCalculator,
    QualityMetricsCalculator,
    SemanticMetricsCalculator
)
from src.analyzers import ErrorAnalyzer, PatternDetector, FixSuggester
from src.generators import ReportGenerator
from src.prompts import PromptEngine
from src.utils import DiskSpaceManager, DebugTimer, Validators
from src.config.config_manager import ConfigManager

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ai_modeleval.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


class CombinedMetricCalculator(MetricCalculator):
    """Combines multiple metric calculators into one"""
    
    def __init__(self, config: Optional[dict] = None):
        super().__init__(config or {})
        self.functional = FunctionalMetricsCalculator(config)
        self.quality = QualityMetricsCalculator(config)
        self.semantic = SemanticMetricsCalculator(config)
        
        # Combine supported metrics
        self.supported_metrics = (
            self.functional.supported_metrics +
            self.quality.supported_metrics +
            self.semantic.supported_metrics
        )
        
        # Combine normalization rules
        self.normalization_rules.update(self.functional.normalization_rules)
        self.normalization_rules.update(self.quality.normalization_rules)
        self.normalization_rules.update(self.semantic.normalization_rules)
        
        # Combine thresholds
        self.thresholds.update(self.functional.thresholds)
        self.thresholds.update(self.quality.thresholds)
        self.thresholds.update(self.semantic.thresholds)
        
        # Combine weights
        self.weights.update(self.functional.weights)
        self.weights.update(self.quality.weights)
        self.weights.update(self.semantic.weights)
    
    def calculate(self, results):
        """Calculate all metrics"""
        metrics = {}
        
        # Calculate from each calculator
        if self.functional:
            metrics.update(self.functional.calculate(results))
        if self.quality:
            metrics.update(self.quality.calculate(results))
        if self.semantic:
            metrics.update(self.semantic.calculate(results))
        
        return metrics
    
    def calculate_for_result(self, result):
        """Calculate metrics for a single result"""
        metrics = {}
        
        if self.functional:
            metrics.update(self.functional.calculate_for_result(result))
        if self.quality:
            metrics.update(self.quality.calculate_for_result(result))
        if self.semantic:
            metrics.update(self.semantic.calculate_for_result(result))
        
        return metrics
    
    def calculate_aggregate_metrics(self, results):
        """Calculate aggregate metrics across all results"""
        return {
            'functional': self.functional.calculate(results) if self.functional else {},
            'quality': self.quality.calculate(results) if self.quality else {},
            'semantic': self.semantic.calculate(results) if self.semantic else {}
        }


class AI_ModelEval:
    """Main application class for AI_ModelEval"""
    
    def __init__(self, config_path: str = "config/settings.yaml"):
        with DebugTimer("Initialization"):
            logger.info("Initializing AI_ModelEval...")
            
            # Load configuration
            self.config = ConfigManager(config_path)
            self.config.ensure_dirs()
            
            # Initialize utilities
            self.disk_manager = DiskSpaceManager()
            
            # Initialize components
            self._init_components()
            
            logger.info("AI_ModelEval initialized successfully")

    def _init_components(self):
        """Initialize all components"""
        # Model registry
        self.model_registry = ModelRegistry()
        self.model_registry.load_from_config(self.config.config)
        
        # Dataset loaders
        self.dataset_loader = HumanEvalLoader({
            'repo_url': self.config.get('paths.repo_url'),
            'data_dir': self.config.get('paths.data_dir'),
            'name': 'HumanEval'
        })
        
        # Executors
        self.resource_manager = ResourceManager()
        self.sandbox = SandboxExecutor(
            timeout=self.config.get('evaluation.timeout_seconds', 30),
            memory_limit=f"{self.config.get('evaluation.max_memory_mb', 512)}m"
        )
        
        # Get analyzer config from main config
        analyzer_config = self.config.get('analyzers', {})
        
        # Calculators - Use concrete implementations
        self.functional_metrics = FunctionalMetricsCalculator(self.config.get('metrics', {}))
        self.quality_metrics = QualityMetricsCalculator(self.config.get('metrics', {}))
        self.semantic_metrics = SemanticMetricsCalculator(self.config.get('metrics', {}))
        
        # Create combined calculator for EvaluationManager
        self.metric_calculator = CombinedMetricCalculator(self.config.get('metrics', {}))
        
        # Analyzers - Pass config dictionary (not None)
        self.error_analyzer = ErrorAnalyzer(analyzer_config)
        self.pattern_detector = PatternDetector(analyzer_config)
        self.fix_suggester = FixSuggester(analyzer_config)
        
        # Generators
        self.report_generator = ReportGenerator({
            'output_dir': self.config.get('paths.results_dir')
        })
        
        # Prompts
        self.prompt_engine = PromptEngine(self.config.get('prompts', {}))
        
        # Managers
        self.evaluation_manager = EvaluationManager(
            model_registry=self.model_registry,
            dataset_loader=self.dataset_loader,
            sandbox_executor=self.sandbox,
            metric_calculator=self.metric_calculator,
            error_analyzer=self.error_analyzer,
            max_workers=self.config.get('evaluation.resource_limits.max_concurrent', 4)
        )
        
        self.result_aggregator = ResultAggregator()

    async def run_evaluation(
        self,
        model_ids: Optional[list] = None,
        num_samples: Optional[int] = None,
        dataset_id: Optional[str] = None
    ) -> Evaluation:
        """Run a complete evaluation"""
        logger.info("Starting evaluation pipeline...")
        
        # Check disk space
        available_gb = self.disk_manager.get_available_space_gb()
        if available_gb and available_gb < 10:
            logger.warning(f"Low disk space: {available_gb:.2f} GB")
            recommendations = self.disk_manager.get_cleanup_recommendations()
            for rec in recommendations:
                logger.warning(f"  - {rec['action']} ({rec['size_gb']:.1f} GB)")
        
        # Use default models if not specified
        if not model_ids:
            model_ids = self.config.get('models.default_models', ['codellama:7b'])
        
        # Check model availability
        available_models, space_info = self.disk_manager.check_model_availability(
            self.model_registry, model_ids
        )
        
        if not available_models:
            logger.error("No suitable models available")
            return None
        
        logger.info(f"Using models: {available_models}")
        
        # Load dataset
        if not dataset_id:
            problems = self.dataset_loader.load_dataset()
            dataset_id = self.dataset_loader.dataset_id
        
        # Create evaluation
        evaluation = self.evaluation_manager.create_evaluation(
            user_id='system',
            model_ids=available_models,
            dataset_id=dataset_id,
            config={
                'num_samples': num_samples or self.config.get('evaluation.num_samples_per_task', 5),
                'strategies': self.config.get('evaluation.prompt_strategies', ['zero_shot']),
                'timeout': self.config.get('evaluation.timeout_seconds', 30)
            }
        )
        
        # Run evaluation
        def progress_callback(eval_id, stage, message, data):
            logger.info(f"[{stage}] {message}")
            if data and 'progress' in data:
                print(f"Progress: {data['progress']:.1f}%")
        
        evaluation = await self.evaluation_manager.run_evaluation(
            evaluation.evaluation_id,
            progress_callback
        )
        
        # Aggregate results
        results = self.evaluation_manager.get_results(evaluation.evaluation_id)
        self.result_aggregator.add_results(evaluation.evaluation_id, results)
        
        # Generate report
        report = self.report_generator.generate_summary_report(
            evaluation,
            results,
            format='html'
        )
        
        logger.info(f"Evaluation complete. Report saved to: {report.file_path}")
        
        return evaluation

    def start_dashboard(self, host: str = None, port: int = None):
        """Start the dashboard server"""
        from src.dashboard.app import create_app
        
        app = create_app()
        app.run(
            host=host or self.config.get('dashboard.host', '0.0.0.0'),
            port=port or self.config.get('dashboard.port', 5000),
            debug=self.config.get('dashboard.debug', False)
        )

    def cleanup(self):
        """Clean up resources"""
        logger.info("Cleaning up...")
        
        # Stop resource monitoring
        self.resource_manager.stop_monitoring()
        
        # Close model connections
        asyncio.run(self.model_registry.close_all())
        
        # Clean up sandbox
        self.sandbox.cleanup()
        
        logger.info("Cleanup complete")


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='AI_ModelEval - Evaluate AI Code Models')
    parser.add_argument('--config', type=str, default='config/settings.yaml',
                       help='Configuration file path')
    parser.add_argument('--mode', type=str, choices=['eval', 'dashboard', 'benchmark'],
                       default='dashboard', help='Operation mode')
    parser.add_argument('--models', type=str, nargs='+',
                       help='Models to evaluate')
    parser.add_argument('--samples', type=int, default=5,
                       help='Number of samples per task')
    parser.add_argument('--host', type=str, default='0.0.0.0',
                       help='Dashboard host')
    parser.add_argument('--port', type=int, default=5000,
                       help='Dashboard port')
    
    args = parser.parse_args()
    
    # Create application
    app = AI_ModelEval(config_path=args.config)
    
    try:
        if args.mode == 'eval':
            # Run evaluation
            asyncio.run(app.run_evaluation(
                model_ids=args.models,
                num_samples=args.samples
            ))
            
        elif args.mode == 'dashboard':
            # Start dashboard
            logger.info(f"Starting dashboard on {args.host}:{args.port}")
            app.start_dashboard(host=args.host, port=args.port)
            
        elif args.mode == 'benchmark':
            # Run benchmark
            logger.info("Benchmark mode not yet implemented")
            
    except KeyboardInterrupt:
        logger.info("Interrupted by user")
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        sys.exit(1)
    finally:
        app.cleanup()


if __name__ == '__main__':
    main()

--------------------------------------------------------------------------------
FILE 9/71: PROJECT_STATUS.md
--------------------------------------------------------------------------------

# Project Status and Checklist

Generated: 2026-02-15
Project: AI Model Evaluation Framework

## âœ… Completed Components

### Core Source Code (src/)

- âœ… `__init__.py` - Package initialization with exports
- âœ… `version.py` - Version management utilities
- âœ… `cli.py` - Command-line interface

#### Entities

- âœ… `__init__.py` with imports
- âœ… `user.py` - User entity with authentication
- âœ… `evaluation.py` - Evaluation entity
- âœ… `problem.py` - Problem entity
- âœ… `evaluation_result.py` - Result entity
- âœ… `metric.py` - Metric entity
- âœ… `error.py` - Error entity
- âœ… `report.py` - Report entity
- âœ… `benchmark.py` - Benchmark entity

#### Managers

- âœ… `__init__.py` with imports
- âœ… `evaluation_manager.py` - Orchestrates evaluations (327 lines)
- âœ… `result_aggregator.py` - Aggregates and analyzes results

#### Adapters

- âœ… `__init__.py` with imports
- âœ… `model_adapter.py` - Abstract base class
- âœ… `ollama_adapter.py` - Ollama implementation
- âœ… `huggingface_adapter.py` - HuggingFace implementation
- âœ… `registry.py` - Model registry system

#### Loaders

- âœ… `__init__.py` with imports
- âœ… `dataset_loader.py` - Abstract base class
- âœ… `humaneval_loader.py` - HumanEval dataset loader

#### Executors

- âœ… `__init__.py` with imports
- âœ… `sandbox_executor.py` - Safe code execution
- âœ… `resource_manager.py` - Resource limits management

#### Calculators

- âœ… `__init__.py` with imports
- âœ… `metric_calculator.py` - Base calculator
- âœ… `functional_metrics.py` - Pass@k metrics
- âœ… `quality_metrics.py` - Code quality metrics
- âœ… `semantic_metrics.py` - CodeBLEU and semantic metrics

#### Analyzers

- âœ… `__init__.py` with imports
- âœ… `error_analyzer.py` - Error analysis (423 lines)
- âœ… `pattern_detector.py` - Pattern detection
- âœ… `fix_suggester.py` - Fix suggestions

#### Generators

- âœ… `__init__.py` with imports
- âœ… `report_generator.py` - Report generation
- âœ… `exporters.py` - CSV, JSON, PDF, HTML export classes (423 lines)
- âœ… `templates/report_template.html` - HTML report template
- âœ… `templates/dashboard_template.html` - Dashboard template

#### Prompts

- âœ… `__init__.py` with imports
- âœ… `prompt_engine.py` - Prompt management
- âœ… `strategies.py` - Zero-shot, Few-shot, Chain-of-thought (153 lines)

#### Dashboard

- âœ… `__init__.py` with imports
- âœ… `app.py` - Flask/FastAPI application
- âœ… `routes.py` - API routes
- âœ… `static/` - Static files directory
- âœ… `templates/` - Template files directory

#### Utils

- âœ… `__init__.py` with imports
- âœ… `disk_space_manager.py` - Disk space utilities
- âœ… `debug_timer.py` - Performance timing
- âœ… `validators.py` - Input validation functions

#### Config

- âœ… `__init__.py` - Configuration management class with singleton pattern
- âœ… `settings.yaml` - Configuration file with sensible defaults

### Test Suite (tests/)

- âœ… `__init__.py` - Test package with discovery utilities
- âœ… `test_entities/__init__.py` - Entity tests (90+ lines)
- âœ… `test_managers/__init__.py` - Manager tests (80+ lines)
- âœ… `test_adapters/__init__.py` - Adapter tests (90+ lines)
- âœ… `test_executors/__init__.py` - Executor tests (120+ lines)

### Configuration Files

- âœ… `setup.py` - Package setup and distribution
- âœ… `requirements.txt` - Core dependencies
- âœ… `requirements-dev.txt` - Development dependencies
- âœ… `pytest.ini` - Pytest configuration
- âœ… `.gitignore` - Git ignore patterns
- âœ… `.env.example` - Environment variables template
- âœ… `conftest.py` - Pytest fixtures and configuration

### Documentation

- âœ… `README.md` - Main project documentation
- âœ… `API.md` - Comprehensive API documentation
- âœ… `CONTRIBUTING.md` - Contribution guidelines
- âœ… `LICENSE` - MIT License
- âœ… `CHANGELOG.md` - Version history
- âœ… `DOCKER.md` - Docker and containerization guide
- âœ… `MANIFEST.in` - Package manifest

### Utility Files

- âœ… `Makefile` - Development commands and automation
- âœ… `main.py` - Main entry point (263 lines)
- âœ… `examples.py` - Usage examples and demonstrations
- âœ… `__init__.py` - Root package initialization

### Data Directories

- âœ… `data/` - With .gitkeep placeholder
- âœ… `results/` - With .gitkeep placeholder

## ðŸ“Š Project Statistics

### File Counts

- **Total Python files**: 50+
- **Test files**: 4 test classes
- **Documentation files**: 7
- **Configuration files**: 8
- **Total lines of code**: 2000+

### Code Organization

- **Core modules**: 11 (entities, managers, adapters, loaders, executors, calculators, analyzers, generators, prompts, dashboard, utils)
- **Test suites**: 4 main test classes
- **Configuration systems**: 1 (Singleton pattern)
- **CLI support**: Yes (cli.py module)

## ðŸŽ¯ Key Features Implemented

### Architecture

- âœ… CRC Card based design
- âœ… Adapter pattern for extensibility
- âœ… Abstract base classes for plugins
- âœ… Singleton configuration management
- âœ… Module-based organization

### Functionality

- âœ… Multi-model support framework
- âœ… Dataset loading abstraction
- âœ… Code execution sandboxing
- âœ… Metric calculation pipeline
- âœ… Error analysis and detection
- âœ… Report generation (multiple formats)
- âœ… Web dashboard framework
- âœ… CLI interface

### Testing

- âœ… Unit test suite for all entities
- âœ… Manager tests
- âœ… Adapter tests
- âœ… Executor tests
- âœ… Pytest fixtures
- âœ… Coverage configuration

### Development Tools

- âœ… Makefile with common commands
- âœ… Virtual environment support
- âœ… Git ignore patterns
- âœ… Environment template
- âœ… Package configuration
- âœ… Version management

## ðŸš€ Getting Started

### Installation

```bash
pip install -r requirements.txt
```

### Run Tests

```bash
pytest tests/
```

### Start Dashboard

```bash
python main.py dashboard
```

### Run Examples

```bash
python examples.py
```

### View Make Commands

```bash
make help
```

## ðŸ“‹ Next Steps (For User Implementation)

1. **Implement model adapters** for specific AI models
2. **Add dataset loaders** for additional benchmarks
3. **Configure execution environment** in settings.yaml
4. **Deploy dashboard** to server
5. **Connect to actual models** and datasets
6. **Customize metrics** for specific use cases
7. **Add authentication** for multi-user scenarios
8. **Set up database** for result storage

## ðŸ” Verification Checklist

- âœ… All directories created
- âœ… All source files created with structure
- âœ… All test files created with test cases
- âœ… Configuration files complete
- âœ… Documentation comprehensive
- âœ… Development tools included
- âœ… Package setup ready
- âœ… Import statements working
- âœ… Module structure sound
- âœ… Examples provided

## ðŸ“ Notes

- Configuration uses singleton pattern for global access
- All abstract base classes have proper ABC implementation
- Test files include comprehensive test cases
- Documentation covers all major modules
- Makefile provides quick access to common tasks
- Project is ready for development and customization

---

**Project Status**: âœ… COMPLETE - Ready for Development
**Last Updated**: 2026-02-15

--------------------------------------------------------------------------------
FILE 10/71: pytest.ini
--------------------------------------------------------------------------------

[pytest]
# Pytest configuration
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short --strict-markers
markers =
    unit: Unit tests
    integration: Integration tests
    slow: Slow running tests
    requires_gpu: Tests that require GPU

# Coverage options
[coverage:run]
branch = True
source = src

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod

--------------------------------------------------------------------------------
FILE 11/71: README.md
--------------------------------------------------------------------------------

# AI_ModelEval

A comprehensive framework for evaluating AI code generation models. Built according to SRS specifications with full CRC card implementation.

## Features

- **Multi-model Evaluation**: Test multiple AI models (Ollama, HuggingFace, OpenAI) side-by-side
- **Comprehensive Metrics**: Pass@k, CodeBLEU, cyclomatic complexity, maintainability index
- **Secure Execution**: Docker-based sandbox for safe code execution
- **Error Analysis**: Pattern detection, classification, and fix suggestions
- **Interactive Dashboard**: Real-time visualization of results
- **Report Generation**: Export results in HTML, PDF, CSV, JSON formats
- **Resource Management**: Automatic model selection based on available disk space

## Installation

```bash
# Clone repository
git clone https://github.com/yourusername/AI_ModelEval.git
cd AI_ModelEval

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure settings
cp config/settings.yaml.example config/settings.yaml
# Edit settings.yaml with your configuration
```

## Usage

### Start Dashboard

```bash
python main.py --mode dashboard --host 0.0.0.0 --port 5000
```

### Run Evaluation

```bash
python main.py --mode eval --models codellama:7b starcoder:1b --samples 5
```

### Generate Benchmark

```bash
python main.py --mode benchmark
```

## Project Structure

```
AI_ModelEval/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ entities/          # Core entity classes (CRC cards)
â”‚   â”œâ”€â”€ managers/          # Manager classes
â”‚   â”œâ”€â”€ adapters/          # Model adapters
â”‚   â”œâ”€â”€ loaders/           # Dataset loaders
â”‚   â”œâ”€â”€ executors/         # Code executors
â”‚   â”œâ”€â”€ calculators/       # Metric calculators
â”‚   â”œâ”€â”€ analyzers/         # Error analyzers
â”‚   â”œâ”€â”€ generators/        # Report generators
â”‚   â”œâ”€â”€ prompts/           # Prompt strategies
â”‚   â”œâ”€â”€ dashboard/         # Web dashboard
â”‚   â””â”€â”€ utils/             # Utilities
â”œâ”€â”€ config/                # Configuration files
â”œâ”€â”€ data/                  # Dataset storage
â”œâ”€â”€ results/               # Evaluation results
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ main.py                # Entry point
â””â”€â”€ requirements.txt       # Dependencies
```

## Architecture

The system follows the CRC card specifications from the SRS document:

- **User**: Authentication, sessions, permissions
- **Evaluation**: Lifecycle management, progress tracking
- **ModelAdapter**: Unified interface for AI models
- **DatasetLoader**: Dataset loading and parsing
- **SandboxExecutor**: Secure code execution
- **MetricCalculator**: Multi-dimensional metrics
- **ErrorAnalyzer**: Error classification and suggestions
- **ReportGenerator**: Multi-format report generation

## Configuration

Edit `config/settings.yaml` to configure:

- Paths for data and results
- Model endpoints and API keys
- Evaluation parameters
- Dashboard settings
- Logging levels

## API Documentation

The dashboard provides REST API endpoints:

- `GET /api/health` - System health check
- `GET /api/evaluations` - List evaluations
- `POST /api/evaluations` - Create evaluation
- `GET /api/models` - List available models
- `POST /api/reports` - Generate report
- `GET /api/charts/*` - Chart data endpoints

## Testing

```bash
# Run unit tests
pytest tests/

# Run with coverage
pytest --cov=src tests/
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

MIT License - see LICENSE file for details

## Acknowledgments

- Based on SRS specifications for AI_ModelEval
- Implements CRC cards from software design document
- Uses HumanEval dataset for evaluation

--------------------------------------------------------------------------------
FILE 12/71: requirements-dev.txt
--------------------------------------------------------------------------------

# Development dependencies
pytest>=6.2.0
pytest-cov>=2.12.0
pytest-asyncio>=0.18.0
black>=21.5b0
pylint>=2.9.0
flake8>=3.9.0
mypy>=0.910
isort>=5.9.0
sphinx>=4.0.0
sphinx-rtd-theme>=1.0.0

--------------------------------------------------------------------------------
FILE 13/71: requirements.txt
--------------------------------------------------------------------------------

# Core dependencies
pandas>=1.5.0
numpy>=1.24.0
pyyaml>=6.0
tqdm>=4.65.0

# Code analysis
radon>=5.1.0
pylint>=2.17.0
bandit>=1.7.5
cognitive-complexity>=1.3.0

# AI model integration
ollama>=0.1.0
huggingface-hub>=0.16.0
transformers>=4.30.0
torch>=2.0.0
aiohttp>=3.8.0

# Execution sandbox
docker>=6.1.0
psutil>=5.9.0

# Web dashboard
flask>=2.3.0
flask-cors>=4.0.0
plotly>=5.14.0
matplotlib>=3.7.0

# Reporting
reportlab>=4.0.0  # For PDF generation
jinja2>=3.1.0     # For HTML templates

# Database
sqlalchemy>=2.0.0
alembic>=1.11.0

# Utilities
gitpython>=3.1.0
python-dateutil>=2.8.0
tenacity>=8.2.0

--------------------------------------------------------------------------------
FILE 14/71: setup.py
--------------------------------------------------------------------------------

"""Setup configuration for AI Model Evaluation Framework."""

from setuptools import setup, find_packages

with open('README.md', 'r', encoding='utf-8') as fh:
    long_description = fh.read()

with open('requirements.txt', 'r', encoding='utf-8') as fh:
    requirements = [line.strip() for line in fh if line.strip() and not line.startswith('#')]

setup(
    name='ai-model-eval',
    version='1.0.0',
    author='AI Evaluation Team',
    author_email='team@example.com',
    description='A comprehensive framework for evaluating AI code generation models',
    long_description=long_description,
    long_description_content_type='text/markdown',
    url='https://github.com/yourusername/AI_ModelEval',
    packages=find_packages(),
    classifiers=[
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.8',
        'Programming Language :: Python :: 3.9',
        'Programming Language :: Python :: 3.10',
        'Programming Language :: Python :: 3.11',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
        'Development Status :: 3 - Alpha',
        'Intended Audience :: Developers',
        'Topic :: Scientific/Engineering :: Artificial Intelligence',
    ],
    python_requires='>=3.8',
    install_requires=requirements,
    entry_points={
        'console_scripts': [
            'ai-model-eval=src.cli:main',
        ],
    },
    include_package_data=True,
    zip_safe=False,
)

--------------------------------------------------------------------------------
FILE 15/71: src\__init__.py
--------------------------------------------------------------------------------

"""AI Model Evaluation Framework.

A comprehensive framework for evaluating AI models on various benchmarks and datasets.
"""

__version__ = '1.0.0'
__author__ = 'AI Evaluation Team'

from . import entities
from . import managers
from . import adapters
from . import loaders
from . import executors
from . import calculators
from . import analyzers
from . import generators
from . import prompts
from . import dashboard
from . import utils
from . import config

__all__ = [
    'entities',
    'managers',
    'adapters',
    'loaders',
    'executors',
    'calculators',
    'analyzers',
    'generators',
    'prompts',
    'dashboard',
    'utils',
    'config',
]

--------------------------------------------------------------------------------
FILE 16/71: src\adapters\__init__.py
--------------------------------------------------------------------------------

"""Adapter pattern implementations for model adapters."""

from .model_adapter import ModelAdapter
from .ollama_adapter import OllamaAdapter
from .huggingface_adapter import HuggingFaceAdapter
from .registry import ModelRegistry

__all__ = [
    'ModelAdapter',
    'OllamaAdapter',
    'HuggingFaceAdapter',
    'ModelRegistry'
]

--------------------------------------------------------------------------------
FILE 17/71: src\adapters\huggingface_adapter.py
--------------------------------------------------------------------------------


from typing import List, Dict, Any, Optional, AsyncGenerator
import asyncio
import aiohttp
import json
import logging

from .model_adapter import ModelAdapter

logger = logging.getLogger(__name__)


class HuggingFaceAdapter(ModelAdapter):
    """Adapter for HuggingFace Inference API"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.api_url = f"https://api-inference.huggingface.co/models/{self.model_name}"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self.session: Optional[aiohttp.ClientSession] = None
        self.max_retries = config.get('max_retries', 3)
        self.retry_delay = config.get('retry_delay', 1)

    async def _ensure_session(self) -> aiohttp.ClientSession:
        """Ensure aiohttp session exists"""
        if not self.session or self.session.closed:
            self.session = aiohttp.ClientSession(headers=self.headers)
        return self.session

    async def generate_code(
        self,
        prompt: str,
        config: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generate code using HuggingFace"""
        config = config or {}
        
        # Check cache
        cache_key = self._get_cache_key(prompt, config)
        cached = self._check_cache(cache_key)
        if cached:
            return cached
        
        # Check rate limit
        await self._check_rate_limit()
        
        # Prepare request
        payload = {
            'inputs': prompt,
            'parameters': {
                'temperature': config.get('temperature', self.temperature),
                'top_p': config.get('top_p', self.top_p),
                'max_new_tokens': config.get('max_tokens', self.max_tokens),
                'do_sample': True,
                'return_full_text': False
            },
            'options': {
                'wait_for_model': True,
                'use_cache': self.cache_enabled
            }
        }
        
        for attempt in range(self.max_retries):
            try:
                session = await self._ensure_session()
                async with session.post(self.api_url, json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        
                        # Handle different response formats
                        if isinstance(result, list):
                            generated_text = result[0].get('generated_text', '')
                        else:
                            generated_text = result.get('generated_text', '')
                        
                        # Cache response
                        self._update_cache(cache_key, generated_text)
                        
                        return generated_text
                        
                    elif response.status == 503:
                        # Model loading, retry
                        if attempt < self.max_retries - 1:
                            wait_time = self.retry_delay * (2 ** attempt)
                            logger.info(f"Model loading, retrying in {wait_time}s")
                            await asyncio.sleep(wait_time)
                            continue
                    
                    error_text = await response.text()
                    logger.error(f"HuggingFace API error: {response.status} - {error_text}")
                    return f"# Error: Failed to generate code (HTTP {response.status})"
                    
            except Exception as e:
                logger.error(f"HuggingFace generation failed (attempt {attempt + 1}): {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))
                else:
                    return f"# Error: {str(e)}"
        
        return "# Error: Max retries exceeded"

    async def generate_stream(
        self,
        prompt: str,
        config: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[str, None]:
        """Generate code as a stream (not supported by HF Inference API)"""
        # HF Inference API doesn't support streaming, so we simulate it
        result = await self.generate_code(prompt, config)
        
        # Split into words for simulation
        words = result.split()
        for word in words:
            yield word + " "
            await asyncio.sleep(0.05)

    async def get_capabilities(self) -> Dict[str, Any]:
        """Get HuggingFace model capabilities"""
        try:
            # Try to get model info from HuggingFace API
            model_info_url = f"https://huggingface.co/api/models/{self.model_name}"
            
            session = await self._ensure_session()
            async with session.get(model_info_url) as response:
                if response.status == 200:
                    info = await response.json()
                    return {
                        'tags': info.get('tags', []),
                        'pipeline_tag': info.get('pipeline_tag', 'text-generation'),
                        'likes': info.get('likes', 0),
                        'downloads': info.get('downloads', 0),
                        'library_name': info.get('library_name', 'transformers'),
                        'languages': self._extract_languages(info.get('tags', []))
                    }
        except Exception as e:
            logger.error(f"Failed to get model capabilities: {e}")
        
        # Default capabilities
        return {
            'pipeline_tag': 'text-generation',
            'languages': ['python', 'javascript', 'java', 'c', 'cpp', 'go', 'rust'],
            'supports_code': True
        }

    def _extract_languages(self, tags: List[str]) -> List[str]:
        """Extract programming languages from tags"""
        languages = []
        lang_keywords = ['python', 'javascript', 'java', 'cpp', 'c', 'go', 'rust', 'php']
        
        for tag in tags:
            tag_lower = tag.lower()
            for lang in lang_keywords:
                if lang in tag_lower:
                    languages.append(lang)
        
        return languages or ['python']  # Default to Python

    async def close(self):
        """Close the session"""
        if self.session and not self.session.closed:
            await self.session.close()

--------------------------------------------------------------------------------
FILE 18/71: src\adapters\model_adapter.py
--------------------------------------------------------------------------------


from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, AsyncGenerator
import asyncio
from datetime import datetime, timedelta
import hashlib
import json
import logging

logger = logging.getLogger(__name__)


class ModelAdapter(ABC):
    """Abstract base class for all model adapters"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.adapter_id = self._generate_adapter_id()
        self.provider = self.__class__.__name__.replace('Adapter', '').lower()
        self.model_name = config.get('model_name', 'default')
        self.api_endpoint = config.get('api_endpoint')
        self.api_key = config.get('api_key')
        self.max_tokens = config.get('max_tokens', 512)
        self.temperature = config.get('temperature', 0.7)
        self.top_p = config.get('top_p', 0.9)
        self.cache_enabled = config.get('cache_enabled', True)
        self.cache: Dict[str, tuple] = {}  # key -> (response, timestamp)
        self.cache_ttl = config.get('cache_ttl', 3600)  # 1 hour default
        self.request_count = 0
        self.last_request_time = datetime.now()
        self.rate_limit = config.get('rate_limit', 60)  # requests per minute
        self.rate_limit_window = config.get('rate_limit_window', 60)  # seconds
        
        logger.info(f"Initialized {self.provider} adapter for model {self.model_name}")

    def _generate_adapter_id(self) -> str:
        """Generate a unique adapter ID"""
        import secrets
        return f"adapter_{secrets.token_hex(8)}"

    def _get_cache_key(self, prompt: str, config: Dict) -> str:
        """Generate cache key from prompt and config"""
        key_data = {
            'prompt': prompt,
            'model': self.model_name,
            'temperature': config.get('temperature', self.temperature),
            'max_tokens': config.get('max_tokens', self.max_tokens),
            'top_p': config.get('top_p', self.top_p)
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.sha256(key_str.encode()).hexdigest()

    def _check_cache(self, cache_key: str) -> Optional[str]:
        """Check if response is in cache"""
        if not self.cache_enabled:
            return None
        
        if cache_key in self.cache:
            response, timestamp = self.cache[cache_key]
            if datetime.now() - timestamp < timedelta(seconds=self.cache_ttl):
                logger.debug(f"Cache hit for key {cache_key[:8]}")
                return response
            else:
                # Remove expired entry
                del self.cache[cache_key]
        
        return None

    def _update_cache(self, cache_key: str, response: str) -> None:
        """Update cache with response"""
        if self.cache_enabled:
            self.cache[cache_key] = (response, datetime.now())
            logger.debug(f"Cached response for key {cache_key[:8]}")

    async def _check_rate_limit(self) -> None:
        """Check and enforce rate limiting"""
        now = datetime.now()
        time_diff = (now - self.last_request_time).total_seconds()
        
        if time_diff < self.rate_limit_window:
            if self.request_count >= self.rate_limit:
                wait_time = self.rate_limit_window - time_diff
                logger.warning(f"Rate limit reached, waiting {wait_time:.2f}s")
                await asyncio.sleep(wait_time)
                self.request_count = 0
                self.last_request_time = datetime.now()
        else:
            self.request_count = 0
            self.last_request_time = now
        
        self.request_count += 1

    @abstractmethod
    async def generate_code(
        self,
        prompt: str,
        config: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generate code from prompt"""
        pass

    @abstractmethod
    async def generate_stream(
        self,
        prompt: str,
        config: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[str, None]:
        """Generate code as a stream"""
        pass
        yield ""  # Dummy yield for generator

    @abstractmethod
    async def get_capabilities(self) -> Dict[str, Any]:
        """Get model capabilities"""
        pass

    async def test_connection(self) -> bool:
        """Test API connection"""
        try:
            response = await self.generate_code(
                "def hello():\n    return 'world'",
                {'max_tokens': 10}
            )
            return bool(response and len(response.strip()) > 0)
        except Exception as e:
            logger.error(f"Connection test failed: {e}")
            return False

    def estimate_cost(self, tokens: int) -> float:
        """Estimate API cost for token count"""
        # Default implementation - override in specific adapters
        rates = {
            'ollama': 0.0,  # Free local inference
            'huggingface': 0.0,  # Free tier may apply
            'openai': 0.002,  # $0.002 per 1K tokens
            'anthropic': 0.00163,  # $0.00163 per 1K tokens
            'gemini': 0.0005  # $0.0005 per 1K tokens
        }
        rate = rates.get(self.provider, 0.0)
        return (tokens / 1000) * rate

    def get_model_info(self) -> Dict[str, Any]:
        """Get model information"""
        return {
            'adapter_id': self.adapter_id,
            'provider': self.provider,
            'model_name': self.model_name,
            'max_tokens': self.max_tokens,
            'temperature': self.temperature,
            'capabilities': asyncio.run(self.get_capabilities()),
            'rate_limit': self.rate_limit,
            'cache_enabled': self.cache_enabled
        }

    def validate_response(self, response: str) -> bool:
        """Validate model response"""
        if not response or not response.strip():
            return False
        
        # Check for error indicators
        error_indicators = ['error:', 'exception:', 'failed:', 'timeout']
        response_lower = response.lower()
        
        for indicator in error_indicators:
            if indicator in response_lower and len(response) < 100:
                return False
        
        return True

    async def batch_generate(
        self,
        prompts: List[str],
        config: Optional[Dict[str, Any]] = None
    ) -> List[str]:
        """Generate multiple codes in batch"""
        tasks = [self.generate_code(prompt, config) for prompt in prompts]
        return await asyncio.gather(*tasks)

    def clear_cache(self) -> None:
        """Clear the response cache"""
        self.cache.clear()
        logger.info("Cache cleared")

    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        return {
            'size': len(self.cache),
            'enabled': self.cache_enabled,
            'ttl': self.cache_ttl,
            'keys': list(self.cache.keys())[:10]  # First 10 keys for debugging
        }

--------------------------------------------------------------------------------
FILE 19/71: src\adapters\ollama_adapter.py
--------------------------------------------------------------------------------


from typing import List, Dict, Any, Optional, AsyncGenerator
import asyncio
import aiohttp
import json
import logging

from .model_adapter import ModelAdapter

logger = logging.getLogger(__name__)


class OllamaAdapter(ModelAdapter):
    """Adapter for Ollama local models"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.base_url = config.get('base_url', 'http://localhost:11434')
        self.session: Optional[aiohttp.ClientSession] = None

    async def _ensure_session(self) -> aiohttp.ClientSession:
        """Ensure aiohttp session exists"""
        if not self.session or self.session.closed:
            self.session = aiohttp.ClientSession()
        return self.session

    async def generate_code(
        self,
        prompt: str,
        config: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generate code using Ollama"""
        config = config or {}
        
        # Check cache
        cache_key = self._get_cache_key(prompt, config)
        cached = self._check_cache(cache_key)
        if cached:
            return cached
        
        # Check rate limit
        await self._check_rate_limit()
        
        # Prepare request
        url = f"{self.base_url}/api/generate"
        payload = {
            'model': self.model_name,
            'prompt': prompt,
            'stream': False,
            'options': {
                'temperature': config.get('temperature', self.temperature),
                'top_p': config.get('top_p', self.top_p),
                'max_tokens': config.get('max_tokens', self.max_tokens),
                'stop': config.get('stop', None)
            }
        }
        
        try:
            session = await self._ensure_session()
            async with session.post(url, json=payload) as response:
                if response.status == 200:
                    result = await response.json()
                    generated_text = result.get('response', '')
                    
                    # Cache response
                    self._update_cache(cache_key, generated_text)
                    
                    return generated_text
                else:
                    error_text = await response.text()
                    logger.error(f"Ollama API error: {response.status} - {error_text}")
                    return f"# Error: Failed to generate code (HTTP {response.status})"
                    
        except aiohttp.ClientConnectorError:
            logger.error("Failed to connect to Ollama. Is it running?")
            return "# Error: Cannot connect to Ollama. Please ensure Ollama is running."
        except Exception as e:
            logger.error(f"Ollama generation failed: {e}")
            return f"# Error: {str(e)}"

    async def generate_stream(
        self,
        prompt: str,
        config: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[str, None]:
        """Generate code as a stream"""
        config = config or {}
        
        url = f"{self.base_url}/api/generate"
        payload = {
            'model': self.model_name,
            'prompt': prompt,
            'stream': True,
            'options': {
                'temperature': config.get('temperature', self.temperature),
                'top_p': config.get('top_p', self.top_p),
                'max_tokens': config.get('max_tokens', self.max_tokens)
            }
        }
        
        try:
            session = await self._ensure_session()
            async with session.post(url, json=payload) as response:
                if response.status == 200:
                    async for line in response.content:
                        if line:
                            try:
                                chunk = json.loads(line)
                                if 'response' in chunk:
                                    yield chunk['response']
                                if chunk.get('done', False):
                                    break
                            except json.JSONDecodeError:
                                continue
                else:
                    yield f"# Error: HTTP {response.status}"
                    
        except Exception as e:
            logger.error(f"Ollama stream generation failed: {e}")
            yield f"# Error: {str(e)}"

    async def get_capabilities(self) -> Dict[str, Any]:
        """Get Ollama model capabilities"""
        try:
            url = f"{self.base_url}/api/show"
            payload = {'name': self.model_name}
            
            session = await self._ensure_session()
            async with session.post(url, json=payload) as response:
                if response.status == 200:
                    info = await response.json()
                    return {
                        'context_length': info.get('context_length', 2048),
                        'parameters': info.get('parameters', 'unknown'),
                        'quantization': info.get('quantization', 'unknown'),
                        'families': info.get('families', []),
                        'template': info.get('template', ''),
                        'license': info.get('license', 'unknown')
                    }
        except Exception as e:
            logger.error(f"Failed to get model capabilities: {e}")
        
        # Default capabilities
        return {
            'context_length': 2048,
            'parameters': 'unknown',
            'supports_code': True,
            'languages': ['python', 'javascript', 'java', 'c', 'cpp', 'go', 'rust']
        }

    async def list_models(self) -> List[Dict[str, Any]]:
        """List available models"""
        try:
            url = f"{self.base_url}/api/tags"
            
            session = await self._ensure_session()
            async with session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get('models', [])
        except Exception as e:
            logger.error(f"Failed to list models: {e}")
        
        return []

    async def pull_model(self, model_name: str) -> bool:
        """Pull a model"""
        try:
            url = f"{self.base_url}/api/pull"
            payload = {'name': model_name}
            
            session = await self._ensure_session()
            async with session.post(url, json=payload) as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"Failed to pull model {model_name}: {e}")
            return False

    async def close(self):
        """Close the session"""
        if self.session and not self.session.closed:
            await self.session.close()

--------------------------------------------------------------------------------
FILE 20/71: src\adapters\registry.py
--------------------------------------------------------------------------------


from typing import Dict, List, Any, Optional, Type
import logging

from .model_adapter import ModelAdapter
from .ollama_adapter import OllamaAdapter
from .huggingface_adapter import HuggingFaceAdapter

logger = logging.getLogger(__name__)


class ModelRegistry:
    """Registry for managing model adapters"""
    
    def __init__(self):
        self.adapters: Dict[str, ModelAdapter] = {}
        self.adapter_classes: Dict[str, Type[ModelAdapter]] = {
            'ollama': OllamaAdapter,
            'huggingface': HuggingFaceAdapter
        }
        self.model_configs: Dict[str, Dict[str, Any]] = {}

    def register_adapter_class(
        self,
        provider: str,
        adapter_class: Type[ModelAdapter]
    ) -> None:
        """Register a new adapter class"""
        self.adapter_classes[provider.lower()] = adapter_class
        logger.info(f"Registered adapter class for provider: {provider}")

    def register_model(
        self,
        model_id: str,
        provider: str,
        config: Dict[str, Any]
    ) -> None:
        """Register a model with configuration"""
        self.model_configs[model_id] = {
            'provider': provider.lower(),
            'config': config
        }
        logger.info(f"Registered model: {model_id} with provider {provider}")

    def get_model(self, model_id: str) -> Optional[ModelAdapter]:
        """Get or create a model adapter"""
        # Check if already created
        if model_id in self.adapters:
            return self.adapters[model_id]
        
        # Get model config
        if model_id not in self.model_configs:
            logger.error(f"Model {model_id} not found in registry")
            return None
        
        config_info = self.model_configs[model_id]
        provider = config_info['provider']
        config = config_info['config'].copy()
        config['model_name'] = model_id
        
        # Create adapter
        if provider in self.adapter_classes:
            adapter_class = self.adapter_classes[provider]
            try:
                adapter = adapter_class(config)
                self.adapters[model_id] = adapter
                logger.info(f"Created adapter for model: {model_id}")
                return adapter
            except Exception as e:
                logger.error(f"Failed to create adapter for {model_id}: {e}")
                return None
        else:
            logger.error(f"Unknown provider: {provider}")
            return None

    def list_models(self) -> List[Dict[str, Any]]:
        """List all registered models"""
        models = []
        for model_id, config_info in self.model_configs.items():
            models.append({
                'model_id': model_id,
                'provider': config_info['provider'],
                'config': config_info['config'],
                'active': model_id in self.adapters
            })
        return models

    def get_active_models(self) -> List[str]:
        """Get list of active (initialized) models"""
        return list(self.adapters.keys())

    def remove_model(self, model_id: str) -> bool:
        """Remove a model from registry"""
        if model_id in self.adapters:
            # Clean up adapter
            adapter = self.adapters[model_id]
            if hasattr(adapter, 'close'):
                try:
                    import asyncio
                    asyncio.create_task(adapter.close())
                except:
                    pass
            
            del self.adapters[model_id]
            
        if model_id in self.model_configs:
            del self.model_configs[model_id]
            
        logger.info(f"Removed model: {model_id}")
        return True

    def clear_cache(self, model_id: Optional[str] = None) -> None:
        """Clear cache for specific model or all models"""
        if model_id:
            if model_id in self.adapters:
                self.adapters[model_id].clear_cache()
        else:
            for adapter in self.adapters.values():
                adapter.clear_cache()
        
        logger.info("Cache cleared")

    async def close_all(self):
        """Close all adapter sessions"""
        for model_id, adapter in self.adapters.items():
            if hasattr(adapter, 'close'):
                try:
                    await adapter.close()
                except Exception as e:
                    logger.error(f"Error closing adapter for {model_id}: {e}")
        
        self.adapters.clear()
        logger.info("All adapters closed")

    def load_from_config(self, config: Dict[str, Any]) -> None:
        """Load models from configuration"""
        models_config = config.get('models', {})
        
        # Load custom models
        custom_models = models_config.get('custom_models', [])
        for model_config in custom_models:
            model_id = model_config['name']
            provider = model_config['adapter']
            config = model_config.get('config', {})
            self.register_model(model_id, provider, config)
        
        # Load default models
        default_models = models_config.get('default_models', [])
        for model_id in default_models:
            if model_id not in self.model_configs:
                # Assume Ollama for default models
                self.register_model(
                    model_id,
                    'ollama',
                    {'base_url': models_config.get('ollama_base_url', 'http://localhost:11434')}
                )

--------------------------------------------------------------------------------
FILE 21/71: src\analyzers\__init__.py
--------------------------------------------------------------------------------

"""Analyzer classes for analyzing errors and patterns."""

from .error_analyzer import ErrorAnalyzer
from .pattern_detector import PatternDetector
from .fix_suggester import FixSuggester

__all__ = ['ErrorAnalyzer', 'PatternDetector', 'FixSuggester']

--------------------------------------------------------------------------------
FILE 22/71: src\analyzers\error_analyzer.py
--------------------------------------------------------------------------------


from typing import List, Dict, Any, Optional, Set, Tuple
from collections import defaultdict, Counter
import re
import ast
import logging
from datetime import datetime, timedelta

from ..entities import Error, EvaluationResult

logger = logging.getLogger(__name__)


class ErrorAnalyzer:
    """Analyzes errors from code execution"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.analyzer_id = self._generate_id()
        self.error_patterns: Dict[str, Dict[str, Any]] = {}
        self.classification_rules: Dict[str, List[str]] = self._init_classification_rules()
        self.fix_suggestions: Dict[str, List[str]] = self._init_fix_suggestions()
        self.error_history: List[Dict[str, Any]] = []
        
        logger.info(f"Initialized ErrorAnalyzer with {len(self.classification_rules)} rules")

    def _generate_id(self) -> str:
        """Generate a unique analyzer ID"""
        import secrets
        return f"analyzer_{secrets.token_hex(8)}"

    def _init_classification_rules(self) -> Dict[str, List[str]]:
        """Initialize error classification rules"""
        return {
            'syntax_error': [
                'invalid syntax',
                'unexpected EOF',
                'expected',
                'missing',
                'unmatched',
                'IndentationError',
                'TabError'
            ],
            'name_error': [
                'is not defined',
                'NameError',
                'undefined name'
            ],
            'type_error': [
                'TypeError',
                'unsupported operand type',
                'must be',
                'cannot be'
            ],
            'value_error': [
                'ValueError',
                'invalid literal',
                'could not convert'
            ],
            'attribute_error': [
                'AttributeError',
                'has no attribute'
            ],
            'index_error': [
                'IndexError',
                'list index out of range'
            ],
            'key_error': [
                'KeyError'
            ],
            'zero_division_error': [
                'division by zero',
                'ZeroDivisionError'
            ],
            'import_error': [
                'ImportError',
                'ModuleNotFoundError',
                'No module named'
            ],
            'runtime_error': [
                'RuntimeError',
                'maximum recursion depth exceeded',
                'TimeoutError'
            ],
            'assertion_error': [
                'AssertionError',
                'assert'
            ],
            'memory_error': [
                'MemoryError',
                'out of memory',
                'cannot allocate'
            ],
            'timeout_error': [
                'Timeout',
                'timed out',
                'execution time exceeded'
            ]
        }

    def _init_fix_suggestions(self) -> Dict[str, List[str]]:
        """Initialize fix suggestions for common errors"""
        return {
            'syntax_error': [
                "Check for missing parentheses, brackets, or quotes",
                "Ensure proper indentation (use spaces consistently)",
                "Verify that all strings are properly closed",
                "Check for missing colons after if/for/while statements"
            ],
            'name_error': [
                "Check if the variable/function is defined before use",
                "Verify spelling of variable/function names",
                "Ensure proper import of required modules",
                "Check scope of variable (local vs global)"
            ],
            'type_error': [
                "Verify that operands have compatible types",
                "Check if variable is None before using it",
                "Ensure function arguments are of correct type",
                "Convert types explicitly if needed (str(), int(), etc.)"
            ],
            'value_error': [
                "Check if input values are within expected range",
                "Verify that strings can be converted to numbers",
                "Ensure list indices are integers, not floats",
                "Check for empty sequences in operations"
            ],
            'attribute_error': [
                "Verify that the object has the required attribute/method",
                "Check if you're using the correct object type",
                "Ensure module imports are correct",
                "Check for typos in attribute names"
            ],
            'index_error': [
                "Verify that list indices are within bounds",
                "Check if list is empty before accessing elements",
                "Use len() to check list size before indexing",
                "Consider using try-except for safe access"
            ],
            'key_error': [
                "Check if dictionary key exists before accessing",
                "Use dict.get() for safe access with default value",
                "Verify dictionary contents",
                "Consider using defaultdict for missing keys"
            ],
            'zero_division_error': [
                "Check if denominator is zero before division",
                "Add a condition to handle zero case",
                "Use try-except to catch division by zero",
                "Ensure values are not zero in calculations"
            ],
            'import_error': [
                "Install required module using pip",
                "Check module name spelling",
                "Verify virtual environment is activated",
                "Check if module is in Python path"
            ],
            'memory_error': [
                "Reduce data size or process in chunks",
                "Use generators instead of lists for large data",
                "Free unused memory with del and gc.collect()",
                "Consider using more efficient algorithms"
            ],
            'timeout_error': [
                "Optimize code for better performance",
                "Reduce input size or complexity",
                "Use caching for repeated calculations",
                "Consider iterative instead of recursive solutions"
            ]
        }

    def analyze_error(self, error: Error) -> Dict[str, Any]:
        """Analyze a single error"""
        analysis = {
            'error_id': error.error_id,
            'error_type': error.error_type,
            'error_message': error.error_message,
            'classification': self.classify_error(error),
            'suggestions': self.suggest_fix(error),
            'pattern': self.find_similar_errors(error),
            'severity': self.calculate_severity(error),
            'frequency': error.frequency
        }
        
        return analysis

    def classify_error(self, error: Error) -> str:
        """Classify error type"""
        error_msg = error.error_message.lower()
        
        for error_type, patterns in self.classification_rules.items():
            for pattern in patterns:
                if pattern.lower() in error_msg:
                    return error_type
        
        # Default to original error type if no match
        return error.error_type

    def suggest_fix(self, error: Error) -> List[str]:
        """Suggest fixes for error"""
        error_type = self.classify_error(error)
        
        # Get general suggestions for this error type
        suggestions = self.fix_suggestions.get(error_type, [
            "Review the error message and check the code around the reported line",
            "Use print statements to debug variable values",
            "Consider using a debugger to step through the code"
        ])
        
        # Add specific suggestions based on error message
        error_msg = error.error_message.lower()
        
        if 'indentation' in error_msg:
            suggestions.insert(0, "Check that all code blocks are properly indented with consistent spaces/tabs")
        
        if 'expected an indented block' in error_msg:
            suggestions.insert(0, "Add proper indentation after function definitions, if/else, for/while loops")
        
        if 'unexpected indent' in error_msg:
            suggestions.insert(0, "Remove extra indentation or check for mixed tabs and spaces")
        
        if 'is not defined' in error_msg:
            # Extract variable name
            import re
            match = re.search(r"'(\w+)' is not defined", error_msg)
            if match:
                var_name = match.group(1)
                suggestions.insert(0, f"Check if '{var_name}' is defined before use, or if there's a typo")
        
        return suggestions

    def find_similar_errors(
        self,
        error: Error,
        max_results: int = 5
    ) -> List[Dict[str, Any]]:
        """Find similar errors from history"""
        if not self.error_history:
            return []
        
        similar = []
        error_signature = self._get_error_signature(error)
        
        for hist_error in self.error_history[-100:]:  # Check last 100 errors
            hist_signature = self._get_error_signature(hist_error)
            
            if error_signature == hist_signature:
                similar.append(hist_error)
                if len(similar) >= max_results:
                    break
        
        return similar

    def _get_error_signature(self, error: Dict[str, Any]) -> str:
        """Get error signature for matching"""
        # Normalize error message by removing variable parts
        msg = error.get('error_message', '')
        
        # Remove line numbers
        msg = re.sub(r'line \d+', 'line N', msg)
        
        # Remove quoted strings
        msg = re.sub(r'"[^"]*"', '"..."', msg)
        msg = re.sub(r"'[^']*'", "'...'", msg)
        
        # Remove numbers
        msg = re.sub(r'\b\d+\b', 'N', msg)
        
        return f"{error.get('error_type', 'unknown')}:{msg[:100]}"

    def calculate_severity(self, error: Error) -> str:
        """Calculate error severity"""
        error_type = error.error_type
        error_msg = error.error_message.lower()
        
        # Critical errors
        if error_type in ['memory_error', 'timeout_error']:
            return 'critical'
        
        if 'out of memory' in error_msg or 'cannot allocate' in error_msg:
            return 'critical'
        
        # High severity
        if error_type in ['runtime_error', 'zero_division_error']:
            return 'high'
        
        if 'maximum recursion depth exceeded' in error_msg:
            return 'high'
        
        # Medium severity
        if error_type in ['syntax_error', 'type_error', 'name_error', 'attribute_error']:
            return 'medium'
        
        # Low severity
        if error_type in ['assertion_error', 'value_error', 'index_error', 'key_error']:
            return 'low'
        
        return 'unknown'

    def analyze_errors(self, errors: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze multiple errors"""
        if not errors:
            return {
                'total_errors': 0,
                'unique_errors': 0,
                'by_type': {},
                'by_severity': {},
                'most_common': [],
                'patterns': {}
            }
        
        # Add to history
        self.error_history.extend(errors)
        
        # Group by type
        by_type = defaultdict(int)
        by_severity = defaultdict(int)
        error_messages = []
        
        for error in errors:
            error_type = error.get('error_type', 'unknown')
            severity = self.calculate_severity(Error.from_dict(error))
            
            by_type[error_type] += 1
            by_severity[severity] += 1
            error_messages.append(error.get('error_message', ''))
        
        # Find most common error messages
        msg_counter = Counter(error_messages)
        most_common = [
            {'message': msg, 'count': count}
            for msg, count in msg_counter.most_common(5)
        ]
        
        # Detect patterns
        patterns = self.detect_patterns(errors)
        
        return {
            'total_errors': len(errors),
            'unique_errors': len(set(e.get('error_message', '') for e in errors)),
            'by_type': dict(by_type),
            'by_severity': dict(by_severity),
            'most_common': most_common,
            'patterns': patterns
        }

    def detect_patterns(self, errors: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Detect error patterns"""
        patterns = {}
        
        # Group errors by type
        type_groups = defaultdict(list)
        for error in errors:
            error_type = error.get('error_type', 'unknown')
            type_groups[error_type].append(error)
        
        for error_type, type_errors in type_groups.items():
            if len(type_errors) < 3:
                continue
            
            # Look for common patterns in messages
            messages = [e.get('error_message', '') for e in type_errors]
            
            # Find common substrings
            common = self._find_common_substrings(messages)
            
            if common:
                patterns[error_type] = {
                    'count': len(type_errors),
                    'common_patterns': common,
                    'suggestions': self.fix_suggestions.get(error_type, [])
                }
        
        return patterns

    def _find_common_substrings(self, strings: List[str], min_length: int = 10) -> List[str]:
        """Find common substrings in error messages"""
        if len(strings) < 2:
            return []
        
        common = []
        
        # Find longest common substring between first two
        s1, s2 = strings[0], strings[1]
        
        # Simple LCS implementation
        matrix = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]
        longest = 0
        longest_str = ""
        
        for i in range(len(s1)):
            for j in range(len(s2)):
                if s1[i] == s2[j]:
                    matrix[i+1][j+1] = matrix[i][j] + 1
                    if matrix[i+1][j+1] > longest:
                        longest = matrix[i+1][j+1]
                        longest_str = s1[i-longest+1:i+1]
                else:
                    matrix[i+1][j+1] = 0
        
        if len(longest_str) >= min_length:
            common.append(longest_str)
        
        return common

    def generate_error_report(
        self,
        errors: List[Dict[str, Any]],
        format: str = 'summary'
    ) -> Dict[str, Any]:
        """Generate error analysis report"""
        analysis = self.analyze_errors(errors)
        
        if format == 'summary':
            return {
                'total_errors': analysis['total_errors'],
                'error_rate': analysis['total_errors'] / max(len(errors), 1),
                'most_common_type': max(analysis['by_type'].items(), key=lambda x: x[1])[0] if analysis['by_type'] else None,
                'critical_errors': analysis['by_severity'].get('critical', 0),
                'suggestions': self._get_general_suggestions(analysis)
            }
        
        return analysis

    def _get_general_suggestions(self, analysis: Dict[str, Any]) -> List[str]:
        """Get general suggestions based on error analysis"""
        suggestions = []
        
        if analysis['by_severity'].get('critical', 0) > 0:
            suggestions.append("Critical errors detected. Check system resources and timeout settings.")
        
        if analysis['by_type'].get('syntax_error', 0) > analysis['total_errors'] * 0.3:
            suggestions.append("High number of syntax errors. Consider using a linter or IDE with syntax checking.")
        
        if analysis['by_type'].get('name_error', 0) > analysis['total_errors'] * 0.2:
            suggestions.append("Many name errors. Review variable scope and naming conventions.")
        
        if analysis['by_type'].get('type_error', 0) > analysis['total_errors'] * 0.2:
            suggestions.append("Frequent type errors. Add type checking or use type hints.")
        
        return suggestions

--------------------------------------------------------------------------------
FILE 23/71: src\analyzers\fix_suggester.py
--------------------------------------------------------------------------------


from typing import List, Dict, Any, Optional
import ast
import logging
from collections import defaultdict

from ..entities import Error

logger = logging.getLogger(__name__)


class FixSuggester:
    """Suggests fixes for code errors"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.fix_templates: Dict[str, List[Dict[str, Any]]] = self._init_fix_templates()
        self.learning_enabled = config.get('learning_enabled', True)
        self.successful_fixes: List[Dict[str, Any]] = []

    def _init_fix_templates(self) -> Dict[str, List[Dict[str, Any]]]:
        """Initialize fix templates for common errors"""
        return {
            'syntax_error': [
                {
                    'pattern': r'missing (\w+)',
                    'fix': 'Add missing {0}',
                    'example': 'Missing parenthesis â†’ Add closing parenthesis )'
                },
                {
                    'pattern': r'unexpected indent',
                    'fix': 'Remove extra indentation or check for mixed tabs/spaces',
                    'example': 'Use consistent indentation (4 spaces recommended)'
                }
            ],
            'name_error': [
                {
                    'pattern': r"'(\w+)' is not defined",
                    'fix': "Define '{0}' before use or check for typos",
                    'example': "name 'x' is not defined â†’ x = value before using it"
                }
            ],
            'type_error': [
                {
                    'pattern': r"unsupported operand type\(s\) for ([+-/*]): '(\w+)' and '(\w+)'",
                    'fix': "Convert {1} or {2} to compatible types before operation",
                    'example': "unsupported operand type(s) for +: 'int' and 'str' â†’ Convert str to int: int(str_value)"
                },
                {
                    'pattern': r"'(\w+)' object is not callable",
                    'fix': "Remove parentheses or check if {0} is a function",
                    'example': "'str' object is not callable â†’ Don't use str() as a function call"
                }
            ],
            'value_error': [
                {
                    'pattern': r"invalid literal for int\(\) with base 10: '(\w+)'",
                    'fix': "Ensure string '{0}' contains only digits",
                    'example': "invalid literal for int() with base 10: 'abc' â†’ Check input contains numbers only"
                }
            ],
            'attribute_error': [
                {
                    'pattern': r"'(\w+)' object has no attribute '(\w+)'",
                    'fix': "Check if '{1}' is a valid attribute/method for {0} objects",
                    'example': "'list' object has no attribute 'push' â†’ Use append() instead of push()"
                }
            ],
            'index_error': [
                {
                    'pattern': r'list index out of range',
                    'fix': "Check list length before accessing index, or use try-except",
                    'example': "if i < len(my_list): value = my_list[i]"
                }
            ],
            'key_error': [
                {
                    'pattern': r"'(\w+)'",
                    'fix': "Use dict.get('{0}', default_value) for safe access",
                    'example': "value = my_dict.get('key', 0)  # Returns 0 if key doesn't exist"
                }
            ],
            'zero_division_error': [
                {
                    'pattern': r'division by zero',
                    'fix': "Check if denominator is zero before division",
                    'example': "if y != 0: result = x / y else: result = float('inf')"
                }
            ],
            'import_error': [
                {
                    'pattern': r"No module named '(\w+)'",
                    'fix': "Install {0} using: pip install {0}",
                    'example': "pip install requests"
                }
            ],
            'memory_error': [
                {
                    'pattern': r'out of memory',
                    'fix': "Process data in chunks or use generators",
                    'example': "for chunk in pd.read_csv('large.csv', chunksize=10000): process(chunk)"
                }
            ],
            'timeout_error': [
                {
                    'pattern': r'timed out',
                    'fix': "Optimize algorithm or increase timeout limit",
                    'example': "Use caching: from functools import lru_cache"
                }
            ]
        }

    def suggest_fix(self, error: Error) -> List[Dict[str, Any]]:
        """Suggest fixes for an error"""
        suggestions = []
        error_msg = error.error_message
        error_type = error.error_type
        
        # Get templates for this error type
        templates = self.fix_templates.get(error_type, [])
        
        for template in templates:
            import re
            match = re.search(template['pattern'], error_msg, re.IGNORECASE)
            
            if match:
                # Generate fix with captured groups
                fix = template['fix'].format(*match.groups())
                suggestions.append({
                    'fix': fix,
                    'example': template.get('example', ''),
                    'confidence': 'high' if match.groups() else 'medium',
                    'type': 'specific'
                })
        
        # Add general suggestions if no specific matches
        if not suggestions:
            general_fixes = self._get_general_fixes(error_type, error_msg)
            suggestions.extend(general_fixes)
        
        # Add learned fixes from history
        if self.learning_enabled:
            learned = self._get_learned_fixes(error)
            suggestions.extend(learned)
        
        return suggestions

    def _get_general_fixes(
        self,
        error_type: str,
        error_msg: str
    ) -> List[Dict[str, Any]]:
        """Get general fixes for error type"""
        fixes = []
        
        # Error type specific general advice
        advice = {
            'syntax_error': {
                'fix': "Use a linter to catch syntax errors",
                'example': "Run: pylint your_file.py"
            },
            'name_error': {
                'fix': "Check variable scope and imports",
                'example': "from module import function"
            },
            'type_error': {
                'fix': "Add type checking with isinstance()",
                'example': "if isinstance(x, int): process(x)"
            },
            'value_error': {
                'fix': "Validate input before processing",
                'example': "try: value = int(input_string) except ValueError: handle_error()"
            },
            'attribute_error': {
                'fix': "Check object type with dir() or hasattr()",
                'example': "if hasattr(obj, 'method'): obj.method()"
            },
            'index_error': {
                'fix': "Use safe indexing with bounds checking",
                'example': "if 0 <= index < len(my_list): value = my_list[index]"
            },
            'key_error': {
                'fix': "Use defaultdict for missing keys",
                'example': "from collections import defaultdict; d = defaultdict(int)"
            },
            'zero_division_error': {
                'fix': "Add epsilon to denominator",
                'example': "result = x / (y + 1e-10)  # Avoid division by zero"
            },
            'import_error': {
                'fix': "Check requirements.txt for missing dependencies",
                'example': "pip install -r requirements.txt"
            },
            'memory_error': {
                'fix': "Use memory profiling to identify leaks",
                'example': "pip install memory_profiler"
            },
            'timeout_error': {
                'fix': "Implement caching for repeated computations",
                'example': "@lru_cache(maxsize=128) def expensive_function():"
            }
        }
        
        if error_type in advice:
            fixes.append({
                'fix': advice[error_type]['fix'],
                'example': advice[error_type]['example'],
                'confidence': 'medium',
                'type': 'general'
            })
        
        return fixes

    def _get_learned_fixes(self, error: Error) -> List[Dict[str, Any]]:
        """Get fixes learned from successful resolutions"""
        fixes = []
        
        # Look for similar errors that were successfully fixed
        for fix_record in self.successful_fixes[-20:]:  # Check last 20 fixes
            if fix_record.get('error_type') == error.error_type:
                similarity = self._calculate_similarity(
                    error.error_message,
                    fix_record.get('error_message', '')
                )
                
                if similarity > 0.7:  # High similarity
                    fixes.append({
                        'fix': fix_record.get('applied_fix'),
                        'example': fix_record.get('result', ''),
                        'confidence': 'medium' if similarity > 0.8 else 'low',
                        'type': 'learned',
                        'similarity': similarity
                    })
        
        return fixes

    def _calculate_similarity(self, msg1: str, msg2: str) -> float:
        """Calculate similarity between two error messages"""
        from difflib import SequenceMatcher
        return SequenceMatcher(None, msg1.lower(), msg2.lower()).ratio()

    def record_successful_fix(
        self,
        error: Error,
        applied_fix: str,
        result: str
    ) -> None:
        """Record a successful fix for learning"""
        self.successful_fixes.append({
            'error_type': error.error_type,
            'error_message': error.error_message,
            'applied_fix': applied_fix,
            'result': result,
            'timestamp': datetime.now()
        })
        
        # Keep only last 100 fixes
        if len(self.successful_fixes) > 100:
            self.successful_fixes = self.successful_fixes[-100:]

    def suggest_code_improvements(self, code: str) -> List[Dict[str, Any]]:
        """Suggest improvements for code quality"""
        suggestions = []
        
        try:
            tree = ast.parse(code)
            
            # Check for common code smells
            for node in ast.walk(tree):
                # Long functions
                if isinstance(node, ast.FunctionDef):
                    line_count = len(code.split('\n')[node.lineno-1:node.end_lineno])
                    if line_count > 20:
                        suggestions.append({
                            'type': 'code_smell',
                            'suggestion': f"Function '{node.name}' is too long ({line_count} lines)",
                            'fix': "Consider breaking it into smaller functions",
                            'line': node.lineno
                        })
                
                # Deep nesting
                if isinstance(node, ast.If):
                    nesting = self._get_nesting_level(node)
                    if nesting > 3:
                        suggestions.append({
                            'type': 'code_smell',
                            'suggestion': f"Deep nesting detected at line {node.lineno}",
                            'fix': "Consider early returns or guard clauses",
                            'line': node.lineno
                        })
                
                # Repeated code
                # This would require more sophisticated analysis
            
            # Check for missing docstrings
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                    if not ast.get_docstring(node):
                        suggestions.append({
                            'type': 'documentation',
                            'suggestion': f"Missing docstring for {node.name}",
                            'fix': "Add docstring describing purpose and parameters",
                            'line': node.lineno
                        })
            
        except Exception as e:
            logger.debug(f"Code analysis failed: {e}")
        
        return suggestions

    def _get_nesting_level(self, node: ast.AST, level: int = 0) -> int:
        """Get nesting level of AST node"""
        max_level = level
        for child in ast.iter_child_nodes(node):
            if isinstance(child, (ast.If, ast.For, ast.While, ast.Try)):
                max_level = max(max_level, self._get_nesting_level(child, level + 1))
        return max_level

--------------------------------------------------------------------------------
FILE 24/71: src\analyzers\pattern_detector.py
--------------------------------------------------------------------------------


from typing import List, Dict, Any, Optional, Set
from collections import defaultdict, Counter
import re
import hashlib
import logging
from datetime import datetime, timedelta

from ..entities import Error

logger = logging.getLogger(__name__)


class PatternDetector:
    """Detects error patterns in code generation"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.patterns: Dict[str, Dict[str, Any]] = {}
        self.min_pattern_occurrences = config.get('min_pattern_occurrences', 3)
        self.similarity_threshold = config.get('similarity_threshold', 0.8)

    def detect_patterns(self, errors: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Detect patterns in errors"""
        if len(errors) < self.min_pattern_occurrences:
            return []
        
        patterns = []
        
        # Group by error type
        type_groups = defaultdict(list)
        for error in errors:
            error_type = error.get('error_type', 'unknown')
            type_groups[error_type].append(error)
        
        for error_type, type_errors in type_groups.items():
            if len(type_errors) < self.min_pattern_occurrences:
                continue
            
            # Extract patterns from this error type
            type_patterns = self._extract_patterns_from_type(type_errors)
            patterns.extend(type_patterns)
        
        # Store patterns
        for pattern in patterns:
            pattern_id = self._generate_pattern_id(pattern)
            pattern['pattern_id'] = pattern_id
            self.patterns[pattern_id] = pattern
        
        return patterns

    def _extract_patterns_from_type(
        self,
        errors: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Extract patterns from errors of same type"""
        patterns = []
        
        # Extract error messages
        messages = [e.get('error_message', '') for e in errors]
        
        # Find common substrings
        common_substrings = self._find_common_substrings(messages)
        
        for substring in common_substrings:
            # Find errors matching this pattern
            matching_errors = [
                e for e in errors
                if substring in e.get('error_message', '')
            ]
            
            if len(matching_errors) >= self.min_pattern_occurrences:
                # Extract variable parts
                variables = self._extract_variables(
                    substring,
                    [e.get('error_message', '') for e in matching_errors]
                )
                
                pattern = {
                    'pattern': substring,
                    'error_type': errors[0].get('error_type'),
                    'occurrences': len(matching_errors),
                    'first_seen': min(e.get('recorded_at', datetime.now()) for e in matching_errors),
                    'last_seen': max(e.get('recorded_at', datetime.now()) for e in matching_errors),
                    'examples': [e.get('error_message') for e in matching_errors[:3]],
                    'variables': variables,
                    'affected_models': list(set(e.get('model_id') for e in matching_errors if 'model_id' in e)),
                    'severity': self._calculate_pattern_severity(matching_errors)
                }
                patterns.append(pattern)
        
        return patterns

    def _find_common_substrings(
        self,
        strings: List[str],
        min_length: int = 10
    ) -> List[str]:
        """Find common substrings across multiple strings"""
        if len(strings) < 2:
            return []
        
        common = set()
        
        # Build suffix array for first string
        s1 = strings[0]
        suffixes = [(s1[i:], i) for i in range(len(s1))]
        suffixes.sort()
        
        # Compare with other strings
        for other in strings[1:]:
            current_common = set()
            
            for suffix, pos in suffixes:
                # Find common prefix with other string
                prefix_len = 0
                while (pos + prefix_len < len(s1) and
                       prefix_len < len(other) and
                       s1[pos + prefix_len] == other[prefix_len]):
                    prefix_len += 1
                
                if prefix_len >= min_length:
                    current_common.add(s1[pos:pos + prefix_len])
            
            if common:
                common &= current_common
            else:
                common = current_common
            
            if not common:
                break
        
        # Filter and sort results
        result = list(common)
        result.sort(key=len, reverse=True)
        
        return result[:10]  # Return top 10 longest patterns

    def _extract_variables(self, pattern: str, messages: List[str]) -> List[str]:
        """Extract variable parts from messages matching pattern"""
        variables = set()
        
        # Create regex pattern with capture groups for variable parts
        # Replace words that vary with capture groups
        words = pattern.split()
        regex_parts = []
        var_positions = []
        
        for i, word in enumerate(words):
            # Check if this word varies across messages
            word_variations = set()
            for msg in messages:
                msg_words = msg.split()
                if i < len(msg_words):
                    word_variations.add(msg_words[i])
            
            if len(word_variations) > 1:
                # This is a variable
                regex_parts.append(r'(\S+)')
                var_positions.append(i)
            else:
                # This is constant
                regex_parts.append(re.escape(word))
        
        regex_str = r'\s+'.join(regex_parts)
        
        # Extract variables from each message
        for msg in messages:
            match = re.search(regex_str, msg)
            if match:
                for j, pos in enumerate(var_positions):
                    var_value = match.group(j + 1)
                    variables.add(var_value)
        
        return list(variables)

    def _calculate_pattern_severity(self, errors: List[Dict[str, Any]]) -> str:
        """Calculate pattern severity based on errors"""
        # Count error severities
        severities = defaultdict(int)
        for error in errors:
            severities[error.get('severity', 'medium')] += 1
        
        if severities.get('critical', 0) > 0:
            return 'critical'
        elif severities.get('high', 0) > len(errors) * 0.3:
            return 'high'
        elif severities.get('medium', 0) > len(errors) * 0.5:
            return 'medium'
        else:
            return 'low'

    def _generate_pattern_id(self, pattern: Dict[str, Any]) -> str:
        """Generate unique pattern ID"""
        pattern_str = f"{pattern['pattern']}_{pattern['error_type']}"
        hash_obj = hashlib.md5(pattern_str.encode())
        return f"pat_{hash_obj.hexdigest()[:8]}"

    def match_error_to_pattern(self, error: Dict[str, Any]) -> Optional[str]:
        """Match an error to existing patterns"""
        error_msg = error.get('error_message', '')
        
        for pattern_id, pattern in self.patterns.items():
            if pattern['pattern'] in error_msg:
                return pattern_id
        
        return None

    def get_pattern_statistics(self) -> Dict[str, Any]:
        """Get statistics about detected patterns"""
        if not self.patterns:
            return {}
        
        stats = {
            'total_patterns': len(self.patterns),
            'by_severity': defaultdict(int),
            'by_type': defaultdict(int),
            'most_frequent': [],
            'active_patterns': 0
        }
        
        now = datetime.now()
        week_ago = now - timedelta(days=7)
        
        for pattern in self.patterns.values():
            stats['by_severity'][pattern.get('severity', 'unknown')] += 1
            stats['by_type'][pattern.get('error_type', 'unknown')] += 1
            
            # Check if pattern is still active (seen in last week)
            if pattern.get('last_seen', now) > week_ago:
                stats['active_patterns'] += 1
            
            # Track most frequent
            stats['most_frequent'].append({
                'pattern': pattern['pattern'][:50] + '...' if len(pattern['pattern']) > 50 else pattern['pattern'],
                'occurrences': pattern['occurrences'],
                'severity': pattern.get('severity')
            })
        
        # Sort most frequent
        stats['most_frequent'].sort(key=lambda x: x['occurrences'], reverse=True)
        stats['most_frequent'] = stats['most_frequent'][:10]
        
        return dict(stats)

--------------------------------------------------------------------------------
FILE 25/71: src\calculators\__init__.py
--------------------------------------------------------------------------------

"""Calculator classes for computing metrics."""

from .metric_calculator import MetricCalculator
from .functional_metrics import FunctionalMetricsCalculator
from .quality_metrics import QualityMetricsCalculator
from .semantic_metrics import SemanticMetricsCalculator

__all__ = [
    'MetricCalculator',
    'FunctionalMetricsCalculator',
    'QualityMetricsCalculator',
    'SemanticMetricsCalculator'
]

--------------------------------------------------------------------------------
FILE 26/71: src\calculators\functional_metrics.py
--------------------------------------------------------------------------------


from typing import List, Dict, Any, Optional
import numpy as np
from collections import defaultdict
import logging

from .metric_calculator import MetricCalculator
from ..entities import EvaluationResult, Metric

logger = logging.getLogger(__name__)


class FunctionalMetricsCalculator(MetricCalculator):
    """Calculator for functional correctness metrics"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self.supported_metrics = ['pass_rate', 'pass@1', 'pass@5', 'pass@10', 'error_rate']
        
        # Set normalization rules
        self.set_normalization_rule('pass_rate', 0, 1, higher_is_better=True)
        self.set_normalization_rule('pass@1', 0, 1, higher_is_better=True)
        self.set_normalization_rule('pass@5', 0, 1, higher_is_better=True)
        self.set_normalization_rule('pass@10', 0, 1, higher_is_better=True)
        self.set_normalization_rule('error_rate', 0, 1, higher_is_better=False)
        
        # Set default thresholds
        self.set_threshold('pass_rate', 0.7)
        self.set_threshold('pass@1', 0.5)
        
        # Set default weights
        self.set_weight('pass@1', 0.4)
        self.set_weight('pass@5', 0.3)
        self.set_weight('pass_rate', 0.2)
        self.set_weight('error_rate', 0.1)

    def calculate(self, results: List[EvaluationResult]) -> Dict[str, float]:
        """Calculate functional metrics"""
        if not results:
            return {}
        
        metrics = {}
        
        # Overall pass rate
        passed = sum(1 for r in results if r.passed)
        total = len(results)
        metrics['pass_rate'] = passed / total if total > 0 else 0
        
        # Error rate
        errors = sum(len(r.errors) for r in results)
        metrics['error_rate'] = errors / total if total > 0 else 0
        
        # Group by problem and model for Pass@k
        problem_model_groups = defaultdict(list)
        for result in results:
            key = (result.problem_id, result.model_id)
            problem_model_groups[key].append(result)
        
        # Calculate Pass@k for different k values
        for k in [1, 5, 10]:
            pass_at_k = self._calculate_pass_at_k(problem_model_groups, k)
            metrics[f'pass@{k}'] = pass_at_k
        
        return metrics

    def _calculate_pass_at_k(
        self,
        groups: Dict[tuple, List[EvaluationResult]],
        k: int
    ) -> float:
        """Calculate unbiased Pass@k estimator"""
        pass_rates = []
        
        for (problem_id, model_id), results in groups.items():
            n = len(results)
            if n < k:
                continue
            
            # Count passing samples
            c = sum(1 for r in results if r.passed)
            
            # Unbiased estimator from the Codex paper
            if n - c < k:
                pass_rate = 1.0
            else:
                # Calculate 1 - comb(n-c, k) / comb(n, k)
                # Using product formula for numerical stability
                pass_rate = 1.0
                for i in range(k):
                    pass_rate *= (n - c - i) / (n - i)
                pass_rate = 1.0 - pass_rate
            
            pass_rates.append(pass_rate)
        
        return np.mean(pass_rates) if pass_rates else 0.0

    def calculate_per_problem_stats(
        self,
        results: List[EvaluationResult]
    ) -> Dict[str, Dict[str, float]]:
        """Calculate per-problem statistics"""
        problem_stats = defaultdict(lambda: {
            'total_samples': 0,
            'passed_samples': 0,
            'pass_rate': 0,
            'avg_execution_time': 0,
            'total_tests': 0,
            'passed_tests': 0
        })
        
        for result in results:
            stats = problem_stats[result.problem_id]
            stats['total_samples'] += 1
            if result.passed:
                stats['passed_samples'] += 1
            
            if result.execution_time_ms:
                stats['avg_execution_time'] += result.execution_time_ms
            
            if result.test_results:
                stats['total_tests'] += len(result.test_results)
                stats['passed_tests'] += sum(1 for t in result.test_results if t.get('passed', False))
        
        # Calculate averages
        for problem_id, stats in problem_stats.items():
            if stats['total_samples'] > 0:
                stats['pass_rate'] = stats['passed_samples'] / stats['total_samples']
                stats['avg_execution_time'] /= stats['total_samples']
            
            if stats['total_tests'] > 0:
                stats['test_pass_rate'] = stats['passed_tests'] / stats['total_tests']
        
        return dict(problem_stats)

    def calculate_by_model(
        self,
        results: List[EvaluationResult]
    ) -> Dict[str, Dict[str, float]]:
        """Calculate metrics grouped by model"""
        model_results = defaultdict(list)
        for result in results:
            model_results[result.model_id].append(result)
        
        model_metrics = {}
        for model_id, model_results_list in model_results.items():
            model_metrics[model_id] = self.calculate(model_results_list)
        
        return model_metrics

--------------------------------------------------------------------------------
FILE 27/71: src\calculators\metric_calculator.py
--------------------------------------------------------------------------------


from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
import logging
from datetime import datetime

from ..entities import EvaluationResult, Metric

logger = logging.getLogger(__name__)


class MetricCalculator(ABC):
    """Base class for all metric calculators"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.calculator_id = self._generate_id()
        self.supported_metrics: List[str] = []
        self.normalization_rules: Dict[str, Dict[str, float]] = {}
        self.thresholds: Dict[str, float] = {}
        self.weights: Dict[str, float] = {}
        
        logger.info(f"Initialized {self.__class__.__name__}")

    def _generate_id(self) -> str:
        """Generate a unique calculator ID"""
        import secrets
        return f"calc_{secrets.token_hex(8)}"

    @abstractmethod
    def calculate(self, results: List[EvaluationResult]) -> Dict[str, float]:
        """Calculate metrics from results"""
        pass

    def calculate_for_result(self, result: EvaluationResult) -> Dict[str, float]:
        """Calculate metrics for a single result"""
        return self.calculate([result])

    def normalize(
        self,
        metric_name: str,
        value: float,
        target_range: Tuple[float, float] = (0, 1)
    ) -> float:
        """Normalize a metric value"""
        if metric_name not in self.normalization_rules:
            return value
        
        rules = self.normalization_rules[metric_name]
        min_val = rules.get('min', 0)
        max_val = rules.get('max', 1)
        higher_is_better = rules.get('higher_is_better', True)
        
        # Min-max normalization
        if max_val == min_val:
            normalized = target_range[0]
        else:
            normalized = (value - min_val) / (max_val - min_val)
            normalized = normalized * (target_range[1] - target_range[0]) + target_range[0]
        
        # Invert if lower is better
        if not higher_is_better:
            normalized = target_range[1] - normalized + target_range[0]
        
        return max(target_range[0], min(target_range[1], normalized))

    def compare_metrics(
        self,
        metrics1: Dict[str, float],
        metrics2: Dict[str, float]
    ) -> Dict[str, Any]:
        """Compare two sets of metrics"""
        comparison = {
            'differences': {},
            'better_in': [],
            'worse_in': [],
            'tie_in': [],
            'aggregate_score_diff': 0
        }
        
        all_metrics = set(metrics1.keys()) | set(metrics2.keys())
        
        score1 = 0
        score2 = 0
        
        for metric in all_metrics:
            val1 = metrics1.get(metric, 0)
            val2 = metrics2.get(metric, 0)
            
            diff = val1 - val2
            comparison['differences'][metric] = diff
            
            higher_is_better = self.normalization_rules.get(metric, {}).get('higher_is_better', True)
            
            if abs(diff) < 1e-6:
                comparison['tie_in'].append(metric)
            elif (diff > 0 and higher_is_better) or (diff < 0 and not higher_is_better):
                comparison['better_in'].append(metric)
            else:
                comparison['worse_in'].append(metric)
            
            # Aggregate score
            weight = self.weights.get(metric, 1.0)
            score1 += self.normalize(metric, val1) * weight
            score2 += self.normalize(metric, val2) * weight
        
        comparison['aggregate_score_diff'] = score1 - score2
        
        return comparison

    def calculate_aggregate_score(
        self,
        metrics: Dict[str, float],
        custom_weights: Optional[Dict[str, float]] = None
    ) -> float:
        """Calculate aggregate score from metrics"""
        weights = custom_weights or self.weights
        total_weight = sum(weights.values())
        
        if total_weight == 0:
            return 0.0
        
        score = 0.0
        for metric, value in metrics.items():
            if metric in weights:
                normalized = self.normalize(metric, value)
                score += normalized * weights[metric]
        
        return score / total_weight

    def set_threshold(self, metric_name: str, threshold: float) -> None:
        """Set threshold for a metric"""
        self.thresholds[metric_name] = threshold

    def set_weight(self, metric_name: str, weight: float) -> None:
        """Set weight for a metric"""
        self.weights[metric_name] = weight

    def set_normalization_rule(
        self,
        metric_name: str,
        min_val: float,
        max_val: float,
        higher_is_better: bool = True
    ) -> None:
        """Set normalization rule for a metric"""
        self.normalization_rules[metric_name] = {
            'min': min_val,
            'max': max_val,
            'higher_is_better': higher_is_better
        }

    def get_metric_explanation(self, metric_name: str) -> str:
        """Get explanation for a metric"""
        explanations = {
            'pass_rate': 'Percentage of test cases that passed',
            'pass@1': 'Probability that the first generated solution passes all tests',
            'pass@5': 'Probability that at least one of five generated solutions passes all tests',
            'execution_time': 'Average execution time in milliseconds',
            'memory_usage': 'Peak memory usage in KB',
            'codebleu': 'CodeBERT-based semantic similarity score (0-1)',
            'cyclomatic_complexity': 'Measure of code complexity based on control flow',
            'maintainability_index': 'Measure of how maintainable the code is (0-100)',
            'cognitive_complexity': 'Measure of how difficult code is to understand',
            'error_count': 'Number of errors encountered during execution',
            'syntax_errors': 'Number of syntax errors in generated code',
            'runtime_errors': 'Number of runtime errors during execution'
        }
        return explanations.get(metric_name, f'No explanation available for {metric_name}')

--------------------------------------------------------------------------------
FILE 28/71: src\calculators\quality_metrics.py
--------------------------------------------------------------------------------


from typing import List, Dict, Any, Optional
import ast
import radon
from radon.raw import analyze
from radon.complexity import cc_visit
from radon.metrics import mi_visit
import logging

from .metric_calculator import MetricCalculator
from ..entities import EvaluationResult, Metric

logger = logging.getLogger(__name__)


class QualityMetricsCalculator(MetricCalculator):
    """Calculator for code quality metrics"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self.supported_metrics = [
            'loc', 'lloc', 'comments', 'cyclomatic_complexity',
            'maintainability_index', 'cognitive_complexity'
        ]
        
        # Set normalization rules
        self.set_normalization_rule('loc', 0, 100, higher_is_better=False)
        self.set_normalization_rule('cyclomatic_complexity', 0, 20, higher_is_better=False)
        self.set_normalization_rule('maintainability_index', 0, 100, higher_is_better=True)
        self.set_normalization_rule('cognitive_complexity', 0, 50, higher_is_better=False)
        
        # Set default thresholds
        self.set_threshold('maintainability_index', 60)
        self.set_threshold('cyclomatic_complexity', 10)
        
        # Set default weights
        self.set_weight('maintainability_index', 0.4)
        self.set_weight('cyclomatic_complexity', 0.3)
        self.set_weight('cognitive_complexity', 0.2)
        self.set_weight('loc', 0.1)

    def calculate(self, results: List[EvaluationResult]) -> Dict[str, float]:
        """Calculate quality metrics"""
        if not results:
            return {}
        
        metrics = {}
        
        # Collect all code
        all_code = [r.generated_code for r in results if r.generated_code]
        
        if not all_code:
            return {}
        
        # Calculate average metrics across all code
        total_loc = 0
        total_lloc = 0
        total_comments = 0
        total_complexity = 0
        total_maintainability = 0
        total_cognitive = 0
        count = 0
        
        for code in all_code:
            try:
                # Radon metrics
                raw = analyze(code)
                total_loc += raw.loc
                total_lloc += raw.lloc
                total_comments += raw.comments
                
                # Cyclomatic complexity
                cc_metrics = cc_visit(code)
                if cc_metrics:
                    avg_cc = sum(c.complexity for c in cc_metrics) / len(cc_metrics)
                    total_complexity += avg_cc
                
                # Maintainability index
                mi = mi_visit(code, multi=True)
                total_maintainability += mi
                
                # Cognitive complexity
                cognitive = self._calculate_cognitive_complexity(code)
                total_cognitive += cognitive
                
                count += 1
                
            except Exception as e:
                logger.debug(f"Failed to analyze code quality: {e}")
        
        if count > 0:
            metrics['loc'] = total_loc / count
            metrics['lloc'] = total_lloc / count
            metrics['comments'] = total_comments / count
            metrics['cyclomatic_complexity'] = total_complexity / count
            metrics['maintainability_index'] = total_maintainability / count
            metrics['cognitive_complexity'] = total_cognitive / count
        
        return metrics

    def _calculate_cognitive_complexity(self, code: str) -> float:
        """Calculate cognitive complexity"""
        try:
            score = 0
            lines = code.split('\n')
            nesting_level = 0
            
            control_keywords = [
                'if ', 'elif ', 'else:', 'for ', 'while ',
                'except ', 'try:', 'with ', 'case ', 'default:'
            ]
            
            for line in lines:
                stripped = line.strip()
                
                # Skip comments and empty lines
                if stripped.startswith('#') or not stripped:
                    continue
                
                # Check for control flow keywords
                for keyword in control_keywords:
                    if keyword in stripped:
                        score += 1 + nesting_level
                        break
                
                # Track nesting level
                if stripped.endswith(':'):
                    nesting_level += 1
                elif nesting_level > 0 and len(stripped) > 0 and len(stripped) - len(stripped.lstrip()) == 0:
                    nesting_level = max(0, nesting_level - 1)
            
            return max(0, score)
            
        except Exception:
            return 0

    def calculate_per_file(self, code: str) -> Dict[str, float]:
        """Calculate quality metrics for a single file"""
        metrics = {}
        
        try:
            # Radon metrics
            raw = analyze(code)
            metrics['loc'] = raw.loc
            metrics['lloc'] = raw.lloc
            metrics['comments'] = raw.comments
            
            # Cyclomatic complexity
            cc_metrics = cc_visit(code)
            if cc_metrics:
                metrics['cyclomatic_complexity'] = sum(c.complexity for c in cc_metrics) / len(cc_metrics)
                metrics['total_complexity'] = sum(c.complexity for c in cc_metrics)
                metrics['function_count'] = len(cc_metrics)
            else:
                metrics['cyclomatic_complexity'] = 0
                metrics['function_count'] = 0
            
            # Maintainability index
            metrics['maintainability_index'] = mi_visit(code, multi=True)
            
            # Cognitive complexity
            metrics['cognitive_complexity'] = self._calculate_cognitive_complexity(code)
            
        except Exception as e:
            logger.error(f"Failed to calculate quality metrics: {e}")
        
        return metrics

    def grade_quality(self, metrics: Dict[str, float]) -> str:
        """Grade code quality based on metrics"""
        if 'maintainability_index' in metrics:
            mi = metrics['maintainability_index']
            if mi >= 80:
                return 'Excellent'
            elif mi >= 60:
                return 'Good'
            elif mi >= 40:
                return 'Fair'
            else:
                return 'Poor'
        
        if 'cyclomatic_complexity' in metrics:
            cc = metrics['cyclomatic_complexity']
            if cc <= 5:
                return 'Excellent'
            elif cc <= 10:
                return 'Good'
            elif cc <= 15:
                return 'Fair'
            else:
                return 'Poor'
        
        return 'Unknown'

--------------------------------------------------------------------------------
FILE 29/71: src\calculators\semantic_metrics.py
--------------------------------------------------------------------------------


from typing import List, Dict, Any, Optional, Tuple
import ast
import difflib
import logging
import numpy as np

from .metric_calculator import MetricCalculator
from ..entities import EvaluationResult, Metric

logger = logging.getLogger(__name__)


class SemanticMetricsCalculator(MetricCalculator):
    """Calculator for semantic similarity metrics"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self.supported_metrics = ['codebleu', 'ast_similarity', 'dataflow_match']
        
        # Set normalization rules
        self.set_normalization_rule('codebleu', 0, 1, higher_is_better=True)
        self.set_normalization_rule('ast_similarity', 0, 1, higher_is_better=True)
        self.set_normalization_rule('dataflow_match', 0, 1, higher_is_better=True)
        
        # Set default thresholds
        self.set_threshold('codebleu', 0.6)
        
        # Set default weights
        self.set_weight('codebleu', 0.5)
        self.set_weight('ast_similarity', 0.3)
        self.set_weight('dataflow_match', 0.2)

    def calculate(self, results: List[EvaluationResult]) -> Dict[str, float]:
        """Calculate semantic metrics"""
        if not results:
            return {}
        
        metrics = {}
        
        total_codebleu = 0
        total_ast_sim = 0
        total_dataflow = 0
        count = 0
        
        for result in results:
            if not result.generated_code or not result.metadata.get('reference_code'):
                continue
            
            reference = result.metadata['reference_code']
            generated = result.generated_code
            
            # Calculate CodeBLEU (simplified version)
            codebleu = self._calculate_codebleu(generated, reference)
            total_codebleu += codebleu
            
            # Calculate AST similarity
            ast_sim = self._calculate_ast_similarity(generated, reference)
            total_ast_sim += ast_sim
            
            # Calculate dataflow match
            dataflow = self._calculate_dataflow_match(generated, reference)
            total_dataflow += dataflow
            
            count += 1
        
        if count > 0:
            metrics['codebleu'] = total_codebleu / count
            metrics['ast_similarity'] = total_ast_sim / count
            metrics['dataflow_match'] = total_dataflow / count
        
        return metrics

    def _calculate_codebleu(self, generated: str, reference: str, n_gram: int = 4) -> float:
        """Calculate simplified CodeBLEU score"""
        # This is a simplified version. Full CodeBLEU requires parsing and AST matching
        
        # Tokenize
        gen_tokens = self._tokenize_code(generated)
        ref_tokens = self._tokenize_code(reference)
        
        if not gen_tokens or not ref_tokens:
            return 0.0
        
        # Calculate n-gram precision
        precisions = []
        for n in range(1, min(n_gram + 1, 5)):
            gen_ngrams = self._get_ngrams(gen_tokens, n)
            ref_ngrams = self._get_ngrams(ref_tokens, n)
            
            if not ref_ngrams:
                continue
            
            matches = sum(1 for ng in gen_ngrams if ng in ref_ngrams)
            precision = matches / len(gen_ngrams) if gen_ngrams else 0
            precisions.append(precision)
        
        if not precisions:
            return 0.0
        
        # Geometric mean of precisions
        codebleu = np.exp(np.mean(np.log(precisions + 1e-10)))
        
        # Brevity penalty
        gen_len = len(gen_tokens)
        ref_len = len(ref_tokens)
        
        if gen_len > ref_len:
            bp = 1.0
        else:
            bp = np.exp(1 - ref_len / gen_len) if gen_len > 0 else 0
        
        return codebleu * bp

    def _tokenize_code(self, code: str) -> List[str]:
        """Simple code tokenization"""
        import re
        # Split on whitespace and punctuation
        tokens = re.findall(r'\w+|[^\w\s]', code)
        return [t for t in tokens if t.strip()]

    def _get_ngrams(self, tokens: List[str], n: int) -> List[Tuple[str, ...]]:
        """Get n-grams from tokens"""
        if len(tokens) < n:
            return []
        return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]

    def _calculate_ast_similarity(self, generated: str, reference: str) -> float:
        """Calculate AST similarity"""
        try:
            gen_ast = self._get_ast_structure(generated)
            ref_ast = self._get_ast_structure(reference)
            
            if not gen_ast or not ref_ast:
                return 0.0
            
            # Compare AST structures using sequence matching
            gen_seq = self._ast_to_sequence(gen_ast)
            ref_seq = self._ast_to_sequence(ref_ast)
            
            matcher = difflib.SequenceMatcher(None, gen_seq, ref_seq)
            return matcher.ratio()
            
        except Exception as e:
            logger.debug(f"AST similarity calculation failed: {e}")
            return 0.0

    def _get_ast_structure(self, code: str) -> Optional[ast.AST]:
        """Parse code to AST"""
        try:
            return ast.parse(code)
        except SyntaxError:
            return None

    def _ast_to_sequence(self, tree: ast.AST) -> List[str]:
        """Convert AST to sequence of node types"""
        sequence = []
        for node in ast.walk(tree):
            sequence.append(type(node).__name__)
        return sequence

    def _calculate_dataflow_match(self, generated: str, reference: str) -> float:
        """Calculate dataflow pattern similarity"""
        try:
            gen_flow = self._extract_dataflow(generated)
            ref_flow = self._extract_dataflow(reference)
            
            if not gen_flow or not ref_flow:
                return 0.0
            
            # Compare dataflow patterns
            all_vars = set(gen_flow.keys()) | set(ref_flow.keys())
            if not all_vars:
                return 1.0
            
            matches = 0
            for var in all_vars:
                gen_patterns = gen_flow.get(var, [])
                ref_patterns = ref_flow.get(var, [])
                
                # Compare operation sequences
                matcher = difflib.SequenceMatcher(None, gen_patterns, ref_patterns)
                matches += matcher.ratio()
            
            return matches / len(all_vars)
            
        except Exception as e:
            logger.debug(f"Dataflow match calculation failed: {e}")
            return 0.0

    def _extract_dataflow(self, code: str) -> Dict[str, List[str]]:
        """Extract dataflow patterns from code"""
        flow = {}
        
        try:
            tree = ast.parse(code)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Assign):
                    # Track variable assignments
                    for target in node.targets:
                        if isinstance(target, ast.Name):
                            var_name = target.id
                            if var_name not in flow:
                                flow[var_name] = []
                            flow[var_name].append('assign')
                            
                elif isinstance(node, ast.AugAssign):
                    # Track augmented assignments
                    if isinstance(node.target, ast.Name):
                        var_name = node.target.id
                        if var_name not in flow:
                            flow[var_name] = []
                        flow[var_name].append('aug_assign')
                        
                elif isinstance(node, ast.BinOp):
                    # Track binary operations
                    for child in ast.walk(node):
                        if isinstance(child, ast.Name):
                            var_name = child.id
                            if var_name not in flow:
                                flow[var_name] = []
                            flow[var_name].append('bin_op')
                            
                elif isinstance(node, ast.Call):
                    # Track function calls
                    for child in ast.walk(node):
                        if isinstance(child, ast.Name):
                            var_name = child.id
                            if var_name not in flow:
                                flow[var_name] = []
                            flow[var_name].append('call')
                            
        except Exception:
            pass
        
        return flow

    def calculate_pairwise_similarity(
        self,
        results: List[EvaluationResult]
    ) -> np.ndarray:
        """Calculate pairwise similarity matrix"""
        n = len(results)
        similarity = np.zeros((n, n))
        
        for i in range(n):
            for j in range(i+1, n):
                if not results[i].generated_code or not results[j].generated_code:
                    continue
                
                sim = self._calculate_codebleu(
                    results[i].generated_code,
                    results[j].generated_code
                )
                similarity[i][j] = sim
                similarity[j][i] = sim
        
        return similarity

--------------------------------------------------------------------------------
FILE 30/71: src\cli.py
--------------------------------------------------------------------------------

"""
CLI module for command-line interface.

This module provides command-line interface functionality for the AI Model Evaluation framework.
"""

import argparse
import logging
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))


def setup_logging(log_level: str = 'INFO') -> None:
    """Set up logging configuration."""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )


def create_parser() -> argparse.ArgumentParser:
    """Create and return the argument parser."""
    parser = argparse.ArgumentParser(
        description='AI Model Evaluation Framework',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  # Run evaluation
  python main.py eval --models ollama huggingface --samples 10
  
  # Start dashboard
  python main.py dashboard --host 0.0.0.0 --port 5000
  
  # Generate report
  python main.py report --input results/evaluation.json --format html
        '''
    )
    
    # Global options
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        default='INFO',
        help='Set logging level (default: INFO)'
    )
    
    # Subcommands
    subparsers = parser.add_subparsers(dest='command', help='Commands')
    
    # Eval command
    eval_parser = subparsers.add_parser('eval', help='Run evaluation')
    eval_parser.add_argument(
        '--models',
        nargs='+',
        required=True,
        help='Models to evaluate'
    )
    eval_parser.add_argument(
        '--dataset',
        default='humaneval',
        help='Dataset to use (default: humaneval)'
    )
    eval_parser.add_argument(
        '--samples',
        type=int,
        default=5,
        help='Number of samples (default: 5)'
    )
    eval_parser.add_argument(
        '--timeout',
        type=int,
        default=30,
        help='Timeout in seconds (default: 30)'
    )
    
    # Dashboard command
    dashboard_parser = subparsers.add_parser('dashboard', help='Start web dashboard')
    dashboard_parser.add_argument(
        '--host',
        default='127.0.0.1',
        help='Host to bind to (default: 127.0.0.1)'
    )
    dashboard_parser.add_argument(
        '--port',
        type=int,
        default=5000,
        help='Port to bind to (default: 5000)'
    )
    dashboard_parser.add_argument(
        '--debug',
        action='store_true',
        help='Enable debug mode'
    )
    
    # Report command
    report_parser = subparsers.add_parser('report', help='Generate report')
    report_parser.add_argument(
        '--input',
        required=True,
        help='Input evaluation results file'
    )
    report_parser.add_argument(
        '--format',
        choices=['html', 'pdf', 'csv', 'json'],
        default='html',
        help='Output format (default: html)'
    )
    report_parser.add_argument(
        '--output',
        help='Output file path'
    )
    
    return parser


def main() -> int:
    """Main entry point."""
    parser = create_parser()
    args = parser.parse_args()
    
    setup_logging(args.log_level)
    logger = logging.getLogger(__name__)
    
    if not args.command:
        parser.print_help()
        return 0
    
    try:
        if args.command == 'eval':
            logger.info(f'Starting evaluation with models: {args.models}')
            # Evaluation logic would go here
            
        elif args.command == 'dashboard':
            logger.info(f'Starting dashboard on {args.host}:{args.port}')
            # Dashboard startup logic would go here
            
        elif args.command == 'report':
            logger.info(f'Generating report from {args.input}')
            # Report generation logic would go here
            
        return 0
        
    except Exception as e:
        logger.error(f'Error: {e}', exc_info=True)
        return 1


if __name__ == '__main__':
    sys.exit(main())

--------------------------------------------------------------------------------
FILE 31/71: src\config\__init__.py
--------------------------------------------------------------------------------

"""Configuration module."""
import os
import yaml
from pathlib import Path
from typing import Dict, Any, Optional


class Config:
    """Configuration manager for the application."""
    
    _instance = None
    _config: Dict[str, Any] = {}
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(Config, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        """Initialize configuration from settings.yaml."""
        if not self._config:
            self.load()
    
    def load(self, config_path: Optional[str] = None):
        """Load configuration from YAML file."""
        if config_path is None:
            config_path = Path(__file__).parent / 'settings.yaml'
        
        try:
            with open(config_path, 'r') as f:
                self._config = yaml.safe_load(f) or {}
        except FileNotFoundError:
            self._config = self._get_default_config()
    
    @staticmethod
    def _get_default_config() -> Dict[str, Any]:
        """Get default configuration."""
        return {
            'models': [],
            'datasets': [],
            'evaluation': {
                'timeout': 30,
                'memory_limit': '4GB',
                'num_workers': 4
            },
            'output': {
                'results_dir': 'results',
                'export_formats': ['json', 'csv', 'html', 'pdf']
            },
            'logging': {
                'level': 'INFO',
                'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            }
        }
    
    def get(self, key: str, default: Any = None) -> Any:
        """Get configuration value by key."""
        keys = key.split('.')
        value = self._config
        
        for k in keys:
            if isinstance(value, dict):
                value = value.get(k)
                if value is None:
                    return default
            else:
                return default
        
        return value
    
    def set(self, key: str, value: Any) -> None:
        """Set configuration value by key."""
        keys = key.split('.')
        config = self._config
        
        for k in keys[:-1]:
            if k not in config:
                config[k] = {}
            config = config[k]
        
        config[keys[-1]] = value
    
    def __getitem__(self, key: str) -> Any:
        """Dictionary-style access."""
        return self.get(key)
    
    def __setitem__(self, key: str, value: Any) -> None:
        """Dictionary-style setting."""
        self.set(key, value)


# Global configuration instance
config = Config()

--------------------------------------------------------------------------------
FILE 32/71: src\config\config_manager.py
--------------------------------------------------------------------------------

import os
import yaml
from pathlib import Path
from typing import Any, Dict, Optional


class ConfigManager:
    """Configuration manager for AI_ModelEval"""
    
    def __init__(self, config_path: str = "config/settings.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
        
    def _load_config(self) -> Dict[str, Any]:
        """Load configuration from YAML file"""
        config_file = Path(self.config_path)
        
        if not config_file.exists():
            # Return default config if file doesn't exist
            return self._get_default_config()
        
        with open(config_file, 'r') as f:
            return yaml.safe_load(f)
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Return default configuration"""
        return {
            'paths': {
                'data_dir': 'data',
                'results_dir': 'results',
                'cache_dir': 'cache',
                'logs_dir': 'logs',
                'repo_url': 'https://github.com/openai/human-eval'
            },
            'models': {
                'default_models': ['codellama:7b']
            },
            'evaluation': {
                'timeout_seconds': 30,
                'max_memory_mb': 512,
                'num_samples_per_task': 5,
                'prompt_strategies': ['zero_shot'],
                'resource_limits': {
                    'max_concurrent': 4
                }
            },
            'dashboard': {
                'host': '0.0.0.0',
                'port': 5000,
                'debug': False
            }
        }
    
    def get(self, key: str, default: Any = None) -> Any:
        """Get config value using dot notation (e.g., 'paths.data_dir')"""
        keys = key.split('.')
        value = self.config
        
        try:
            for k in keys:
                value = value[k]
            return value
        except (KeyError, TypeError):
            return default
    
    def ensure_dirs(self):
        """Create necessary directories if they don't exist"""
        paths = self.get('paths', {})
        for key, path in paths.items():
            if isinstance(path, str) and not path.startswith(('http', 'https')):
                Path(path).mkdir(parents=True, exist_ok=True)

--------------------------------------------------------------------------------
FILE 33/71: src\config\settings.yaml
--------------------------------------------------------------------------------

# AI_ModelEval Configuration File

# Paths configuration
paths:
  data_dir: "data"
  results_dir: "results"
  cache_dir: "cache"
  log_dir: "logs"
  repo_url: "https://github.com/openai/human-eval.git"

# Models configuration
models:
  ollama_base_url: "http://localhost:11434"
  hf_api_key: ""  # Add your HuggingFace token if needed
  default_models:
    - "codellama:7b"
    - "codellama:13b"
    - "starcoder:1b"
    - "tinyllama:1.1b"
  
  custom_models:
    - name: "codellama:34b"
      adapter: "ollama"
      config:
        base_url: "http://localhost:11434"
        max_tokens: 1024
        temperature: 0.7
    
    - name: "gpt-3.5-turbo"
      adapter: "openai"
      config:
        api_key: "${OPENAI_API_KEY}"
        max_tokens: 1024
        temperature: 0.7

# Evaluation configuration
evaluation:
  num_samples_per_task: 5
  timeout_seconds: 30
  max_memory_mb: 512
  prompt_strategies:
    - "zero_shot"
    - "few_shot"
    - "chain_of_thought"
  
  resource_limits:
    max_concurrent: 4
    max_cpu_percent: 80
    max_memory_percent: 80

# Metrics configuration
metrics:
  enable_functional: true
  enable_quality: true
  enable_semantic: true
  pass_at_k_values: [1, 5, 10]
  
  weights:
    pass@1: 0.4
    pass@5: 0.3
    codebleu: 0.2
    maintainability: 0.1

# Dashboard configuration
dashboard:
  host: "0.0.0.0"
  port: 5000
  debug: false
  secret_key: "change-this-in-production"
  auto_refresh: true
  refresh_interval: 30

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/app.log"
  max_size_mb: 10
  backup_count: 5

# Cache configuration
cache:
  enabled: true
  ttl_seconds: 3600
  max_size_mb: 1024
  backend: "disk"  # disk, redis, memory

# Security configuration
security:
  enable_authentication: false
  session_timeout_hours: 24
  max_login_attempts: 5
  password_min_length: 8
  require_email_verification: false

--------------------------------------------------------------------------------
FILE 34/71: src\dashboard\__init__.py
--------------------------------------------------------------------------------

"""Dashboard web interface."""

from .app import create_app
from .routes import register_routes

__all__ = ['create_app', 'register_routes']

--------------------------------------------------------------------------------
FILE 35/71: src\dashboard\app.py
--------------------------------------------------------------------------------


from flask import Flask, render_template, jsonify, request, session
from flask_cors import CORS
import logging
from pathlib import Path
import yaml
from datetime import timedelta

from .routes import register_routes
from ..utils.disk_space_manager import DiskSpaceManager
from ..utils.debug_timer import DebugTimer

logger = logging.getLogger(__name__)


def create_app(config_path: str = None):
    """Create and configure the Flask application"""
    app = Flask(__name__,
                template_folder='templates',
                static_folder='static')
    
    # Load configuration
    if config_path and Path(config_path).exists():
        with open(config_path, 'r') as f:
            app.config.update(yaml.safe_load(f))
    
    # Configure app
    app.config['SECRET_KEY'] = app.config.get('secret_key', 'dev-secret-key')
    app.config['SESSION_TYPE'] = 'filesystem'
    app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(hours=24)
    
    # Enable CORS
    CORS(app)
    
    # Register routes
    register_routes(app)
    
    # Add template filters
    @app.template_filter('datetime')
    def datetime_filter(value, format='%Y-%m-%d %H:%M:%S'):
        if value:
            return value.strftime(format)
        return ''
    
    @app.template_filter('filesize')
    def filesize_filter(value):
        if value is None:
            return '0 B'
        for unit in ['B', 'KB', 'MB', 'GB']:
            if value < 1024.0:
                return f"{value:.1f} {unit}"
            value /= 1024.0
        return f"{value:.1f} TB"
    
    # Error handlers
    @app.errorhandler(404)
    def not_found_error(error):
        return render_template('error.html', 
                             error_code=404,
                             error_message="Page not found"), 404
    
    @app.errorhandler(500)
    def internal_error(error):
        logger.error(f"Internal server error: {error}")
        return render_template('error.html',
                             error_code=500,
                             error_message="Internal server error"), 500
    
    # Context processors
    @app.context_processor
    def utility_processor():
        return {
            'disk_space': DiskSpaceManager.get_available_space_gb(),
            'app_name': 'AI ModelEval',
            'version': '2.0.0'
        }
    
    logger.info("Dashboard application created successfully")
    return app


def main():
    """Run the dashboard application"""
    app = create_app()
    app.run(
        host=app.config.get('host', '0.0.0.0'),
        port=app.config.get('port', 5000),
        debug=app.config.get('debug', False)
    )


if __name__ == '__main__':
    main()

--------------------------------------------------------------------------------
FILE 36/71: src\dashboard\routes.py
--------------------------------------------------------------------------------

from flask import render_template, jsonify, request, session, send_file, current_app
import logging
from pathlib import Path
import json
import pandas as pd
from datetime import datetime
import plotly.utils
import plotly.graph_objects as go
import plotly.express as px

from ..managers import EvaluationManager, ResultAggregator
from ..entities import Evaluation, EvaluationResult
from ..generators import ReportGenerator
from ..utils.disk_space_manager import DiskSpaceManager

logger = logging.getLogger(__name__)


def register_routes(app):
    """Register all routes with the app"""
    
    @app.route('/')
    def index():
        """Main dashboard page"""
        return render_template('index.html',
                             active_page='dashboard')

    @app.route('/evaluations')
    def evaluations():
        """List all evaluations"""
        evaluations_list = []
        # Safely access evaluation_manager
        if hasattr(app, 'evaluation_manager') and app.evaluation_manager:
            evaluations_list = list(app.evaluation_manager.evaluations.values())
        
        return render_template('evaluations.html',
                             evaluations=evaluations_list,
                             active_page='evaluations')

    @app.route('/evaluation/<evaluation_id>')
    def evaluation_detail(evaluation_id):
        """Show evaluation details"""
        if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager:
            return render_template('error.html',
                                 error_code=500,
                                 error_message="Evaluation manager not initialized"), 500
        
        evaluation = app.evaluation_manager.get_evaluation(evaluation_id)
        if not evaluation:
            return render_template('error.html',
                                 error_code=404,
                                 error_message=f"Evaluation {evaluation_id} not found"), 404
        
        results = app.evaluation_manager.get_results(evaluation_id)
        
        return render_template('evaluation_detail.html',
                             evaluation=evaluation,
                             results=results,
                             active_page='evaluations')

    @app.route('/models')
    def models():
        """List available models"""
        models_list = []
        
        # Safely access model_registry
        if (hasattr(app, 'evaluation_manager') and 
            app.evaluation_manager and 
            hasattr(app.evaluation_manager, 'model_registry') and
            app.evaluation_manager.model_registry):
            try:
                models_list = app.evaluation_manager.model_registry.list_models()
            except Exception as e:
                logger.error(f"Error listing models: {e}")
                models_list = [{"error": str(e)}]
        else:
            # Return empty list with a message in the template
            logger.warning("Model registry not initialized")
        
        return render_template('models.html',
                             models=models_list,
                             active_page='models')

    @app.route('/benchmarks')
    def benchmarks():
        """List benchmarks"""
        benchmarks_list = []
        if hasattr(app, 'result_aggregator') and app.result_aggregator:
            benchmarks_list = list(app.result_aggregator.benchmarks.values())
        
        return render_template('benchmarks.html',
                             benchmarks=benchmarks_list,
                             active_page='benchmarks')

    @app.route('/reports')
    def reports():
        """List generated reports"""
        reports_dir = Path('reports')
        reports_list = []
        
        if reports_dir.exists():
            for file in reports_dir.glob('*'):
                reports_list.append({
                    'name': file.name,
                    'path': str(file),
                    'size': file.stat().st_size,
                    'modified': datetime.fromtimestamp(file.stat().st_mtime)
                })
        
        return render_template('reports.html',
                             reports=reports_list,
                             active_page='reports')

    @app.route('/settings')
    def settings():
        """Settings page"""
        return render_template('settings.html',
                             active_page='settings')

    # API Routes

    @app.route('/api/health')
    def health():
        """Health check endpoint"""
        return jsonify({
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'disk_space': DiskSpaceManager.get_available_space_gb()
        })

    @app.route('/api/evaluations')
    def api_evaluations():
        """Get all evaluations"""
        if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager:
            return jsonify([])
        
        evaluations_list = []
        for eval_id, evaluation in app.evaluation_manager.evaluations.items():
            eval_dict = evaluation.to_dict()
            eval_dict['results_count'] = len(evaluation.results_ids)
            evaluations_list.append(eval_dict)
        
        return jsonify(evaluations_list)

    @app.route('/api/evaluations/<evaluation_id>')
    def api_evaluation(evaluation_id):
        """Get evaluation details"""
        if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager:
            return jsonify({'error': 'Evaluation manager not initialized'}), 500
        
        evaluation = app.evaluation_manager.get_evaluation(evaluation_id)
        if not evaluation:
            return jsonify({'error': 'Evaluation not found'}), 404
        
        results = app.evaluation_manager.get_results(evaluation_id)
        
        return jsonify({
            'evaluation': evaluation.to_dict(),
            'results': [r.to_dict() for r in results]
        })

    @app.route('/api/evaluations', methods=['POST'])
    def api_create_evaluation():
        """Create a new evaluation"""
        if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager:
            return jsonify({'error': 'Evaluation manager not initialized'}), 500
        
        data = request.get_json()
        
        try:
            evaluation = app.evaluation_manager.create_evaluation(
                user_id=data.get('user_id', 'anonymous'),
                model_ids=data.get('models', []),
                dataset_id=data.get('dataset_id'),
                config=data.get('config', {})
            )
            
            return jsonify(evaluation.to_dict()), 201
            
        except Exception as e:
            logger.error(f"Failed to create evaluation: {e}")
            return jsonify({'error': str(e)}), 400

    @app.route('/api/evaluations/<evaluation_id>/start', methods=['POST'])
    def api_start_evaluation(evaluation_id):
        """Start an evaluation"""
        if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager:
            return jsonify({'error': 'Evaluation manager not initialized'}), 500
        
        try:
            import asyncio
            asyncio.create_task(app.evaluation_manager.run_evaluation(evaluation_id))
            return jsonify({'status': 'started'})
        except Exception as e:
            logger.error(f"Failed to start evaluation: {e}")
            return jsonify({'error': str(e)}), 400

    @app.route('/api/evaluations/<evaluation_id>/cancel', methods=['POST'])
    def api_cancel_evaluation(evaluation_id):
        """Cancel an evaluation"""
        if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager:
            return jsonify({'error': 'Evaluation manager not initialized'}), 500
        
        success = app.evaluation_manager.cancel_evaluation(evaluation_id)
        if success:
            return jsonify({'status': 'cancelled'})
        else:
            return jsonify({'error': 'Failed to cancel evaluation'}), 400

    @app.route('/api/models')
    def api_models():
        """Get available models"""
        if (hasattr(app, 'evaluation_manager') and 
            app.evaluation_manager and 
            hasattr(app.evaluation_manager, 'model_registry') and
            app.evaluation_manager.model_registry):
            try:
                return jsonify(app.evaluation_manager.model_registry.list_models())
            except Exception as e:
                logger.error(f"Error in api_models: {e}")
                return jsonify({'error': str(e)}), 500
        else:
            return jsonify([])

    @app.route('/api/models/<model_id>/test', methods=['POST'])
    def api_test_model(model_id):
        """Test model connection"""
        if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager:
            return jsonify({'error': 'Evaluation manager not initialized'}), 500
        
        if not hasattr(app.evaluation_manager, 'model_registry') or not app.evaluation_manager.model_registry:
            return jsonify({'error': 'Model registry not initialized'}), 500
        
        model = app.evaluation_manager.model_registry.get_model(model_id)
        if not model:
            return jsonify({'error': 'Model not found'}), 404
        
        import asyncio
        success = asyncio.run(model.test_connection())
        
        return jsonify({
            'model_id': model_id,
            'connected': success
        })

    @app.route('/api/results')
    def api_results():
        """Get results with optional filtering"""
        if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager:
            return jsonify([])
        
        evaluation_id = request.args.get('evaluation_id')
        model_id = request.args.get('model_id')
        problem_id = request.args.get('problem_id')
        
        results = []
        for result in app.evaluation_manager.results.values():
            if evaluation_id and result.evaluation_id != evaluation_id:
                continue
            if model_id and result.model_id != model_id:
                continue
            if problem_id and result.problem_id != problem_id:
                continue
            results.append(result.to_dict())
        
        return jsonify(results)

    @app.route('/api/benchmarks')
    def api_benchmarks():
        """Get all benchmarks"""
        if not hasattr(app, 'result_aggregator') or not app.result_aggregator:
            return jsonify([])
        
        benchmarks = []
        for benchmark_id, benchmark in app.result_aggregator.benchmarks.items():
            benchmarks.append(benchmark.to_dict())
        
        return jsonify(benchmarks)

    @app.route('/api/benchmarks', methods=['POST'])
    def api_create_benchmark():
        """Create a new benchmark"""
        if not hasattr(app, 'result_aggregator') or not app.result_aggregator:
            return jsonify({'error': 'Result aggregator not initialized'}), 500
        
        data = request.get_json()
        
        try:
            benchmark = app.result_aggregator.create_benchmark(
                name=data['name'],
                description=data.get('description', ''),
                evaluation_ids=data.get('evaluation_ids', []),
                metric_weights=data.get('metric_weights')
            )
            
            return jsonify(benchmark.to_dict()), 201
            
        except Exception as e:
            logger.error(f"Failed to create benchmark: {e}")
            return jsonify({'error': str(e)}), 400

    @app.route('/api/reports', methods=['POST'])
    def api_generate_report():
        """Generate a report"""
        if not hasattr(app, 'report_generator') or not app.report_generator:
            return jsonify({'error': 'Report generator not initialized'}), 500
        
        data = request.get_json()
        
        try:
            evaluation_id = data.get('evaluation_id')
            format = data.get('format', 'html')
            report_type = data.get('type', 'summary')
            
            if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager:
                return jsonify({'error': 'Evaluation manager not initialized'}), 500
            
            evaluation = app.evaluation_manager.get_evaluation(evaluation_id)
            if not evaluation:
                return jsonify({'error': 'Evaluation not found'}), 404
            
            results = app.evaluation_manager.get_results(evaluation_id)
            
            if report_type == 'summary':
                report = app.report_generator.generate_summary_report(
                    evaluation, results, format
                )
            elif report_type == 'detailed':
                report = app.report_generator.generate_detailed_report(
                    evaluation, results, format
                )
            else:
                return jsonify({'error': f'Unknown report type: {report_type}'}), 400
            
            return jsonify(report.to_dict()), 201
            
        except Exception as e:
            logger.error(f"Failed to generate report: {e}")
            return jsonify({'error': str(e)}), 400

    @app.route('/api/reports/<report_id>/download')
    def api_download_report(report_id):
        """Download a report file"""
        reports_dir = Path('reports')
        
        # Find report file
        for file in reports_dir.glob(f'*{report_id}*'):
            return send_file(
                str(file),
                as_attachment=True,
                download_name=file.name
            )
        
        return jsonify({'error': 'Report not found'}), 404

    @app.route('/api/stats')
    def api_stats():
        """Get system statistics"""
        stats = {
            'disk_space': DiskSpaceManager.get_available_space_gb(),
            'evaluations': 0,
            'results': 0,
            'models': 0,
            'reports': 0
        }
        
        if hasattr(app, 'evaluation_manager') and app.evaluation_manager:
            stats['evaluations'] = len(app.evaluation_manager.evaluations)
            stats['results'] = len(app.evaluation_manager.results)
            
            if hasattr(app.evaluation_manager, 'model_registry') and app.evaluation_manager.model_registry:
                stats['models'] = len(app.evaluation_manager.model_registry.list_models())
        
        reports_dir = Path('reports')
        if reports_dir.exists():
            stats['reports'] = len(list(reports_dir.glob('*')))
        
        return jsonify(stats)

    @app.route('/api/charts/pass_rate')
    def api_chart_pass_rate():
        """Generate pass rate chart"""
        evaluation_id = request.args.get('evaluation_id')
        
        if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager or not evaluation_id:
            return jsonify({'error': 'Invalid parameters'}), 400
        
        results = app.evaluation_manager.get_results(evaluation_id)
        
        # Group by model
        model_results = {}
        for result in results:
            if result.model_id not in model_results:
                model_results[result.model_id] = []
            model_results[result.model_id].append(result)
        
        # Calculate pass rates
        models = []
        pass_rates = []
        for model_id, model_results_list in model_results.items():
            models.append(model_id)
            passed = sum(1 for r in model_results_list if r.passed)
            pass_rates.append(passed / len(model_results_list) * 100)
        
        fig = go.Figure(data=[
            go.Bar(x=models, y=pass_rates, marker_color='#3498db')
        ])
        
        fig.update_layout(
            title='Model Pass Rates',
            xaxis_title='Model',
            yaxis_title='Pass Rate (%)',
            yaxis_range=[0, 100]
        )
        
        return jsonify(json.loads(fig.to_json()))

    @app.route('/api/charts/performance_trend')
    def api_chart_performance_trend():
        """Generate performance trend chart"""
        evaluation_ids = request.args.getlist('evaluation_id')
        
        if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager or not evaluation_ids:
            return jsonify({'error': 'Invalid parameters'}), 400
        
        fig = go.Figure()
        
        for eval_id in evaluation_ids:
            results = app.evaluation_manager.get_results(eval_id)
            
            # Group by model
            model_results = {}
            for result in results:
                if result.model_id not in model_results:
                    model_results[result.model_id] = []
                model_results[result.model_id].append(result)
            
            # Calculate pass rates
            for model_id, model_results_list in model_results.items():
                passed = sum(1 for r in model_results_list if r.passed)
                pass_rate = passed / len(model_results_list) * 100
                
                fig.add_trace(go.Scatter(
                    x=[eval_id],
                    y=[pass_rate],
                    mode='markers+lines',
                    name=model_id,
                    marker=dict(size=10)
                ))
        
        fig.update_layout(
            title='Performance Trend Across Evaluations',
            xaxis_title='Evaluation',
            yaxis_title='Pass Rate (%)',
            yaxis_range=[0, 100]
        )
        
        return jsonify(json.loads(fig.to_json()))

    @app.route('/api/charts/error_distribution')
    def api_chart_error_distribution():
        """Generate error distribution chart"""
        evaluation_id = request.args.get('evaluation_id')
        
        if not hasattr(app, 'evaluation_manager') or not app.evaluation_manager or not evaluation_id:
            return jsonify({'error': 'Invalid parameters'}), 400
        
        results = app.evaluation_manager.get_results(evaluation_id)
        
        # Collect error types
        error_types = {}
        for result in results:
            for error in result.errors:
                error_type = error.get('error_type', 'unknown')
                error_types[error_type] = error_types.get(error_type, 0) + 1
        
        if not error_types:
            return jsonify({'error': 'No errors found'}), 404
        
        fig = go.Figure(data=[
            go.Pie(labels=list(error_types.keys()),
                  values=list(error_types.values()))
        ])
        
        fig.update_layout(title='Error Type Distribution')
        
        return jsonify(json.loads(fig.to_json()))

    @app.route('/api/search')
    def api_search():
        """Search endpoint"""
        query = request.args.get('q', '').lower()
        
        if not query or len(query) < 2:
            return jsonify([])
        
        results = []
        
        # Search evaluations
        if hasattr(app, 'evaluation_manager') and app.evaluation_manager:
            for eval_id, evaluation in app.evaluation_manager.evaluations.items():
                if query in eval_id.lower() or query in str(evaluation.config).lower():
                    results.append({
                        'type': 'evaluation',
                        'id': eval_id,
                        'title': f"Evaluation {eval_id}",
                        'url': f"/evaluation/{eval_id}"
                    })
        
        # Search models
        if (hasattr(app, 'evaluation_manager') and 
            app.evaluation_manager and 
            hasattr(app.evaluation_manager, 'model_registry') and
            app.evaluation_manager.model_registry):
            for model in app.evaluation_manager.model_registry.list_models():
                if query in model['model_id'].lower():
                    results.append({
                        'type': 'model',
                        'id': model['model_id'],
                        'title': f"Model {model['model_id']}",
                        'url': f"/models#{model['model_id']}"
                    })
        
        # Search reports
        reports_dir = Path('reports')
        if reports_dir.exists():
            for file in reports_dir.glob('*'):
                if query in file.name.lower():
                    results.append({
                        'type': 'report',
                        'id': file.stem,
                        'title': f"Report {file.name}",
                        'url': f"/reports#{file.name}"
                    })
        
        return jsonify(results[:10])  # Limit to 10 results

--------------------------------------------------------------------------------
FILE 37/71: src\entities\__init__.py
--------------------------------------------------------------------------------

"""Entity classes for AI Model Evaluation."""

from .user import User
from .evaluation import Evaluation
from .problem import Problem
from .evaluation_result import EvaluationResult
from .metric import Metric
from .error import Error
from .report import Report
from .benchmark import Benchmark

__all__ = [
    'User',
    'Evaluation',
    'Problem', 
    'EvaluationResult',
    'Metric',
    'Error',
    'Report',
    'Benchmark'
]

--------------------------------------------------------------------------------
FILE 38/71: src\entities\benchmark.py
--------------------------------------------------------------------------------


from datetime import datetime
from typing import List, Dict, Any, Optional
import secrets
import numpy as np


class Benchmark:
    def __init__(
        self,
        name: str,
        description: str,
        benchmark_id: Optional[str] = None
    ):
        self.benchmark_id = benchmark_id or self._generate_id()
        self.name = name
        self.description = description
        self.created_at = datetime.now()
        self.updated_at = datetime.now()
        self.models: List[Dict[str, Any]] = []
        self.metrics: List[str] = []
        self.results: Dict[str, Dict[str, float]] = {}  # model_id -> metric -> score
        self.rankings: Dict[str, int] = {}
        self.scores: Dict[str, float] = {}
        self.metadata: Dict[str, Any] = {}

    def _generate_id(self) -> str:
        """Generate a unique benchmark ID"""
        return f"ben_{secrets.token_hex(8)}"

    def add_model(self, model_id: str, model_name: str, metadata: Optional[Dict] = None) -> None:
        """Add a model to the benchmark"""
        self.models.append({
            'model_id': model_id,
            'model_name': model_name,
            'metadata': metadata or {}
        })

    def add_metric(self, metric_name: str, weight: float = 1.0) -> None:
        """Add a metric to the benchmark"""
        self.metrics.append(metric_name)

    def add_result(self, model_id: str, metric_name: str, score: float) -> None:
        """Add a result for a model and metric"""
        if model_id not in self.results:
            self.results[model_id] = {}
        self.results[model_id][metric_name] = score
        self.updated_at = datetime.now()

    def calculate_scores(self, weights: Optional[Dict[str, float]] = None) -> Dict[str, float]:
        """Calculate aggregate scores for all models"""
        if not weights:
            # Default equal weights
            weights = {metric: 1.0 for metric in self.metrics}
        
        # Normalize weights
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v / total_weight for k, v in weights.items()}
        
        scores = {}
        for model_id, model_results in self.results.items():
            score = 0.0
            for metric, weight in weights.items():
                if metric in model_results:
                    score += model_results[metric] * weight
            scores[model_id] = score
        
        self.scores = scores
        return scores

    def calculate_rankings(self) -> Dict[str, int]:
        """Calculate rankings based on aggregate scores"""
        if not self.scores:
            self.calculate_scores()
        
        sorted_models = sorted(self.scores.items(), key=lambda x: x[1], reverse=True)
        rankings = {}
        for rank, (model_id, _) in enumerate(sorted_models, 1):
            rankings[model_id] = rank
        
        self.rankings = rankings
        return rankings

    def compare_models(
        self,
        model_id1: str,
        model_id2: str,
        metrics: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Compare two models"""
        if model_id1 not in self.results or model_id2 not in self.results:
            raise ValueError("One or both models not found in benchmark")
        
        metrics_to_compare = metrics or self.metrics
        
        comparison = {
            'model1': model_id1,
            'model2': model_id2,
            'metrics': {},
            'model1_wins': 0,
            'model2_wins': 0,
            'ties': 0
        }
        
        for metric in metrics_to_compare:
            score1 = self.results[model_id1].get(metric, 0)
            score2 = self.results[model_id2].get(metric, 0)
            
            comparison['metrics'][metric] = {
                'model1': score1,
                'model2': score2,
                'difference': score1 - score2
            }
            
            if score1 > score2:
                comparison['model1_wins'] += 1
            elif score2 > score1:
                comparison['model2_wins'] += 1
            else:
                comparison['ties'] += 1
        
        return comparison

    def get_top_models(self, n: int = 5) -> List[Dict[str, Any]]:
        """Get top N models"""
        if not self.rankings:
            self.calculate_rankings()
        
        top_model_ids = sorted(self.rankings.items(), key=lambda x: x[1])[:n]
        
        top_models = []
        for model_id, rank in top_model_ids:
            model_info = next((m for m in self.models if m['model_id'] == model_id), {})
            top_models.append({
                'rank': rank,
                'model_id': model_id,
                'model_name': model_info.get('model_name', model_id),
                'score': self.scores.get(model_id, 0),
                'metrics': self.results.get(model_id, {})
            })
        
        return top_models

    def to_dict(self) -> Dict[str, Any]:
        """Convert benchmark to dictionary"""
        return {
            'benchmark_id': self.benchmark_id,
            'name': self.name,
            'description': self.description,
            'created_at': self.created_at.isoformat(),
            'updated_at': self.updated_at.isoformat(),
            'models': self.models,
            'metrics': self.metrics,
            'results': self.results,
            'rankings': self.rankings or self.calculate_rankings(),
            'scores': self.scores or self.calculate_scores(),
            'metadata': self.metadata
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Benchmark':
        """Create benchmark from dictionary"""
        benchmark = cls(
            name=data['name'],
            description=data['description'],
            benchmark_id=data.get('benchmark_id')
        )
        benchmark.created_at = datetime.fromisoformat(data['created_at']) if 'created_at' in data else datetime.now()
        benchmark.updated_at = datetime.fromisoformat(data['updated_at']) if 'updated_at' in data else datetime.now()
        benchmark.models = data.get('models', [])
        benchmark.metrics = data.get('metrics', [])
        benchmark.results = data.get('results', {})
        benchmark.rankings = data.get('rankings', {})
        benchmark.scores = data.get('scores', {})
        benchmark.metadata = data.get('metadata', {})
        return benchmark

    @classmethod
    def create_humaneval_benchmark(cls) -> 'Benchmark':
        """Create a benchmark for HumanEval"""
        benchmark = cls(
            name="HumanEval Benchmark",
            description="Standard benchmark for code generation models on HumanEval dataset"
        )
        benchmark.add_metric('pass@1', weight=0.4)
        benchmark.add_metric('pass@5', weight=0.3)
        benchmark.add_metric('codebleu', weight=0.2)
        benchmark.add_metric('execution_time', weight=0.1)
        return benchmark

--------------------------------------------------------------------------------
FILE 39/71: src\entities\error.py
--------------------------------------------------------------------------------


from datetime import datetime
from typing import List, Dict, Any, Optional
import secrets
import re


class Error:
    def __init__(
        self,
        result_id: str,
        error_type: str,
        error_message: str,
        error_id: Optional[str] = None
    ):
        self.error_id = error_id or self._generate_id()
        self.result_id = result_id
        self.error_type = error_type  # syntax, semantic, runtime, logical, timeout
        self.error_message = error_message
        self.error_location: Optional[Dict[str, Any]] = None
        self.severity: str = "error"  # error, warning, info
        self.stack_trace: Optional[str] = None
        self.suggested_fix: Optional[str] = None
        self.pattern_id: Optional[str] = None
        self.metadata: Dict[str, Any] = {}
        self.recorded_at = datetime.now()
        self.frequency: int = 1

    def _generate_id(self) -> str:
        """Generate a unique error ID"""
        return f"err_{secrets.token_hex(8)}"

    def set_location(
        self,
        line: Optional[int] = None,
        column: Optional[int] = None,
        file: Optional[str] = None,
        function: Optional[str] = None
    ) -> None:
        """Set error location"""
        self.error_location = {
            'line': line,
            'column': column,
            'file': file,
            'function': function
        }

    def set_stack_trace(self, stack_trace: str) -> None:
        """Set stack trace"""
        self.stack_trace = stack_trace

    def suggest_fix(self, fix: str) -> None:
        """Suggest a fix for this error"""
        self.suggested_fix = fix

    def set_pattern(self, pattern_id: str) -> None:
        """Set the pattern ID for this error"""
        self.pattern_id = pattern_id

    def increment_frequency(self) -> None:
        """Increment error frequency"""
        self.frequency += 1

    def classify_severity(self) -> str:
        """Classify error severity based on type and message"""
        if self.error_type in ['timeout', 'memory_error']:
            self.severity = 'critical'
        elif self.error_type in ['runtime', 'logical']:
            self.severity = 'error'
        elif self.error_type in ['syntax', 'semantic']:
            self.severity = 'warning'
        else:
            self.severity = 'info'
        
        return self.severity

    def extract_pattern_signature(self) -> str:
        """Extract a pattern signature from the error message"""
        # Remove variable parts (numbers, specific names)
        pattern = self.error_message
        
        # Replace numbers with placeholders
        pattern = re.sub(r'\d+', '{num}', pattern)
        
        # Replace quoted strings with placeholders
        pattern = re.sub(r'"[^"]*"', '{str}', pattern)
        pattern = re.sub(r"'[^']*'", '{str}', pattern)
        
        # Replace variable names (common patterns)
        pattern = re.sub(r'\b[a-z][a-z0-9_]*\b', '{var}', pattern)
        
        return pattern

    def to_dict(self) -> Dict[str, Any]:
        """Convert error to dictionary"""
        return {
            'error_id': self.error_id,
            'result_id': self.result_id,
            'error_type': self.error_type,
            'error_message': self.error_message,
            'error_location': self.error_location,
            'severity': self.severity,
            'stack_trace': self.stack_trace,
            'suggested_fix': self.suggested_fix,
            'pattern_id': self.pattern_id,
            'metadata': self.metadata,
            'recorded_at': self.recorded_at.isoformat(),
            'frequency': self.frequency,
            'pattern_signature': self.extract_pattern_signature()
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Error':
        """Create error from dictionary"""
        error = cls(
            result_id=data['result_id'],
            error_type=data['error_type'],
            error_message=data['error_message'],
            error_id=data.get('error_id')
        )
        error.error_location = data.get('error_location')
        error.severity = data.get('severity', 'error')
        error.stack_trace = data.get('stack_trace')
        error.suggested_fix = data.get('suggested_fix')
        error.pattern_id = data.get('pattern_id')
        error.metadata = data.get('metadata', {})
        error.recorded_at = datetime.fromisoformat(data['recorded_at']) if 'recorded_at' in data else datetime.now()
        error.frequency = data.get('frequency', 1)
        return error

    @classmethod
    def from_exception(cls, result_id: str, exception: Exception) -> 'Error':
        """Create error from exception"""
        error_type = type(exception).__name__
        error_msg = str(exception)
        
        # Categorize error type
        if 'timeout' in error_msg.lower():
            category = 'timeout'
        elif 'memory' in error_msg.lower():
            category = 'memory_error'
        elif 'syntax' in error_msg.lower():
            category = 'syntax'
        elif 'name' in error_msg.lower() and 'is not defined' in error_msg:
            category = 'semantic'
        else:
            category = 'runtime'
        
        error = cls(
            result_id=result_id,
            error_type=category,
            error_message=error_msg
        )
        error.classify_severity()
        return error

--------------------------------------------------------------------------------
FILE 40/71: src\entities\evaluation.py
--------------------------------------------------------------------------------


from datetime import datetime
from typing import List, Dict, Any, Optional
import json
import secrets
from enum import Enum


class EvaluationStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class Evaluation:
    def __init__(
        self,
        user_id: str,
        config: Dict[str, Any],
        evaluation_id: Optional[str] = None
    ):
        self.evaluation_id = evaluation_id or self._generate_id()
        self.user_id = user_id
        self.model_ids: List[str] = []
        self.dataset_id: Optional[str] = None
        self.status = EvaluationStatus.PENDING
        self.config = config
        self.created_at = datetime.now()
        self.started_at: Optional[datetime] = None
        self.completed_at: Optional[datetime] = None
        self.progress: float = 0.0
        self.current_stage: str = "initializing"
        self.results_ids: List[str] = []
        self.report_ids: List[str] = []
        self.error_message: Optional[str] = None
        self.metadata: Dict[str, Any] = {}
        self.notes: str = ""

    def _generate_id(self) -> str:
        """Generate a unique evaluation ID"""
        return f"eval_{secrets.token_hex(8)}"

    def start(self) -> bool:
        """Start the evaluation"""
        if self.status not in [EvaluationStatus.PENDING, EvaluationStatus.PAUSED]:
            return False
        self.status = EvaluationStatus.RUNNING
        self.started_at = datetime.now()
        self.current_stage = "starting"
        return True

    def pause(self) -> bool:
        """Pause the evaluation"""
        if self.status != EvaluationStatus.RUNNING:
            return False
        self.status = EvaluationStatus.PAUSED
        self.current_stage = "paused"
        return True

    def resume(self) -> bool:
        """Resume the evaluation"""
        if self.status != EvaluationStatus.PAUSED:
            return False
        self.status = EvaluationStatus.RUNNING
        self.current_stage = "resumed"
        return True

    def cancel(self) -> bool:
        """Cancel the evaluation"""
        if self.status in [EvaluationStatus.COMPLETED, EvaluationStatus.FAILED]:
            return False
        self.status = EvaluationStatus.CANCELLED
        self.completed_at = datetime.now()
        self.current_stage = "cancelled"
        return True

    def complete(self) -> None:
        """Mark evaluation as completed"""
        self.status = EvaluationStatus.COMPLETED
        self.completed_at = datetime.now()
        self.progress = 100.0
        self.current_stage = "completed"

    def fail(self, error_message: str) -> None:
        """Mark evaluation as failed"""
        self.status = EvaluationStatus.FAILED
        self.completed_at = datetime.now()
        self.error_message = error_message
        self.current_stage = "failed"

    def update_progress(self, progress: float, stage: str) -> None:
        """Update evaluation progress"""
        self.progress = min(max(progress, 0.0), 100.0)
        self.current_stage = stage

    def add_model(self, model_id: str) -> None:
        """Add a model to the evaluation"""
        if model_id not in self.model_ids:
            self.model_ids.append(model_id)

    def set_dataset(self, dataset_id: str) -> None:
        """Set the dataset for evaluation"""
        self.dataset_id = dataset_id

    def add_result(self, result_id: str) -> None:
        """Add a result ID to the evaluation"""
        if result_id not in self.results_ids:
            self.results_ids.append(result_id)

    def add_report(self, report_id: str) -> None:
        """Add a report ID to the evaluation"""
        if report_id not in self.report_ids:
            self.report_ids.append(report_id)

    def get_duration(self) -> Optional[float]:
        """Get evaluation duration in seconds"""
        if not self.started_at:
            return None
        end_time = self.completed_at or datetime.now()
        return (end_time - self.started_at).total_seconds()

    def to_dict(self) -> Dict[str, Any]:
        """Convert evaluation to dictionary"""
        return {
            'evaluation_id': self.evaluation_id,
            'user_id': self.user_id,
            'model_ids': self.model_ids,
            'dataset_id': self.dataset_id,
            'status': self.status.value,
            'config': self.config,
            'created_at': self.created_at.isoformat(),
            'started_at': self.started_at.isoformat() if self.started_at else None,
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'progress': self.progress,
            'current_stage': self.current_stage,
            'results_ids': self.results_ids,
            'report_ids': self.report_ids,
            'error_message': self.error_message,
            'metadata': self.metadata,
            'notes': self.notes,
            'duration': self.get_duration()
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Evaluation':
        """Create evaluation from dictionary"""
        eval_obj = cls(
            user_id=data['user_id'],
            config=data['config'],
            evaluation_id=data.get('evaluation_id')
        )
        eval_obj.model_ids = data.get('model_ids', [])
        eval_obj.dataset_id = data.get('dataset_id')
        eval_obj.status = EvaluationStatus(data.get('status', 'pending'))
        eval_obj.created_at = datetime.fromisoformat(data['created_at']) if 'created_at' in data else datetime.now()
        eval_obj.started_at = datetime.fromisoformat(data['started_at']) if data.get('started_at') else None
        eval_obj.completed_at = datetime.fromisoformat(data['completed_at']) if data.get('completed_at') else None
        eval_obj.progress = data.get('progress', 0.0)
        eval_obj.current_stage = data.get('current_stage', 'initializing')
        eval_obj.results_ids = data.get('results_ids', [])
        eval_obj.report_ids = data.get('report_ids', [])
        eval_obj.error_message = data.get('error_message')
        eval_obj.metadata = data.get('metadata', {})
        eval_obj.notes = data.get('notes', '')
        return eval_obj

--------------------------------------------------------------------------------
FILE 41/71: src\entities\evaluation_result.py
--------------------------------------------------------------------------------


from datetime import datetime
from typing import List, Dict, Any, Optional
import secrets
import json


class EvaluationResult:
    def __init__(
        self,
        evaluation_id: str,
        problem_id: str,
        model_id: str,
        sample_id: int,
        result_id: Optional[str] = None
    ):
        self.result_id = result_id or self._generate_id()
        self.evaluation_id = evaluation_id
        self.problem_id = problem_id
        self.model_id = model_id
        self.sample_id = sample_id
        self.generated_code: Optional[str] = None
        self.execution_output: Optional[str] = None
        self.passed: bool = False
        self.execution_time_ms: Optional[float] = None
        self.memory_usage_kb: Optional[float] = None
        self.test_results: List[Dict[str, Any]] = []
        self.errors: List[Dict[str, Any]] = []
        self.metrics: Dict[str, float] = {}
        self.metadata: Dict[str, Any] = {}
        self.created_at = datetime.now()
        self.source: str = "generated"

    def _generate_id(self) -> str:
        """Generate a unique result ID"""
        return f"res_{secrets.token_hex(8)}"

    def set_generated_code(self, code: str) -> None:
        """Set the generated code"""
        self.generated_code = code

    def set_execution_result(
        self,
        passed: bool,
        output: str,
        execution_time_ms: float,
        memory_usage_kb: Optional[float] = None
    ) -> None:
        """Set execution results"""
        self.passed = passed
        self.execution_output = output
        self.execution_time_ms = execution_time_ms
        self.memory_usage_kb = memory_usage_kb

    def add_test_result(
        self,
        test_id: int,
        passed: bool,
        message: str,
        test_case: Optional[str] = None
    ) -> None:
        """Add a test result"""
        self.test_results.append({
            'test_id': test_id,
            'passed': passed,
            'message': message,
            'test_case': test_case
        })

    def add_error(
        self,
        error_type: str,
        error_message: str,
        severity: str = "error",
        location: Optional[Dict[str, Any]] = None
    ) -> None:
        """Add an error"""
        self.errors.append({
            'error_id': len(self.errors),
            'error_type': error_type,
            'error_message': error_message,
            'severity': severity,
            'location': location,
            'timestamp': datetime.now().isoformat()
        })

    def add_metric(self, name: str, value: float) -> None:
        """Add a metric"""
        self.metrics[name] = value

    def calculate_metrics(self) -> Dict[str, float]:
        """Calculate aggregate metrics"""
        metrics = {}
        
        # Pass rate
        if self.test_results:
            passed = sum(1 for t in self.test_results if t.get('passed', False))
            metrics['pass_rate'] = passed / len(self.test_results)
        
        # Execution stats
        if self.execution_time_ms:
            metrics['execution_time'] = self.execution_time_ms
        
        if self.memory_usage_kb:
            metrics['memory_usage'] = self.memory_usage_kb
        
        # Error count
        metrics['error_count'] = len(self.errors)
        
        # Code quality metrics
        if self.generated_code:
            metrics['code_length'] = len(self.generated_code)
            metrics['line_count'] = len(self.generated_code.split('\n'))
        
        self.metrics.update(metrics)
        return metrics

    def is_passing(self) -> bool:
        """Check if all tests passed"""
        return self.passed and all(t.get('passed', False) for t in self.test_results)

    def get_error_analysis(self) -> Dict[str, Any]:
        """Get error analysis summary"""
        if not self.errors:
            return {'has_errors': False}
        
        error_types = {}
        for error in self.errors:
            error_type = error.get('error_type', 'unknown')
            error_types[error_type] = error_types.get(error_type, 0) + 1
        
        return {
            'has_errors': True,
            'total_errors': len(self.errors),
            'error_types': error_types,
            'first_error': self.errors[0] if self.errors else None
        }

    def to_dict(self) -> Dict[str, Any]:
        """Convert result to dictionary"""
        return {
            'result_id': self.result_id,
            'evaluation_id': self.evaluation_id,
            'problem_id': self.problem_id,
            'model_id': self.model_id,
            'sample_id': self.sample_id,
            'generated_code': self.generated_code,
            'execution_output': self.execution_output,
            'passed': self.passed,
            'execution_time_ms': self.execution_time_ms,
            'memory_usage_kb': self.memory_usage_kb,
            'test_results': self.test_results,
            'errors': self.errors,
            'metrics': self.metrics or self.calculate_metrics(),
            'metadata': self.metadata,
            'created_at': self.created_at.isoformat(),
            'source': self.source,
            'is_passing': self.is_passing()
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'EvaluationResult':
        """Create result from dictionary"""
        result = cls(
            evaluation_id=data['evaluation_id'],
            problem_id=data['problem_id'],
            model_id=data['model_id'],
            sample_id=data['sample_id'],
            result_id=data.get('result_id')
        )
        result.generated_code = data.get('generated_code')
        result.execution_output = data.get('execution_output')
        result.passed = data.get('passed', False)
        result.execution_time_ms = data.get('execution_time_ms')
        result.memory_usage_kb = data.get('memory_usage_kb')
        result.test_results = data.get('test_results', [])
        result.errors = data.get('errors', [])
        result.metrics = data.get('metrics', {})
        result.metadata = data.get('metadata', {})
        result.created_at = datetime.fromisoformat(data['created_at']) if 'created_at' in data else datetime.now()
        result.source = data.get('source', 'generated')
        return result

--------------------------------------------------------------------------------
FILE 42/71: src\entities\metric.py
--------------------------------------------------------------------------------


from datetime import datetime
from typing import Dict, Any, Optional
import secrets
import json


class Metric:
    def __init__(
        self,
        result_id: str,
        metric_name: str,
        metric_value: float,
        metric_id: Optional[str] = None
    ):
        self.metric_id = metric_id or self._generate_id()
        self.result_id = result_id
        self.metric_name = metric_name
        self.metric_value = metric_value
        self.details: Dict[str, Any] = {}
        self.calculated_at = datetime.now()
        self.normalized_value: Optional[float] = None
        self.threshold: Optional[float] = None
        self.weight: float = 1.0
        self.metadata: Dict[str, Any] = {}

    def _generate_id(self) -> str:
        """Generate a unique metric ID"""
        return f"met_{secrets.token_hex(8)}"

    def set_details(self, details: Dict[str, Any]) -> None:
        """Set detailed calculation data"""
        self.details = details

    def normalize(
        self,
        min_value: Optional[float] = None,
        max_value: Optional[float] = None,
        target_range: tuple = (0, 1)
    ) -> float:
        """Normalize metric value to target range"""
        if min_value is not None and max_value is not None:
            # Min-max normalization
            if max_value == min_value:
                self.normalized_value = target_range[0]
            else:
                self.normalized_value = (
                    (self.metric_value - min_value) / (max_value - min_value) *
                    (target_range[1] - target_range[0]) + target_range[0]
                )
        elif hasattr(self, 'range') and self.range:
            # Use predefined range
            min_val, max_val = self.range
            if max_val == min_val:
                self.normalized_value = target_range[0]
            else:
                self.normalized_value = (
                    (self.metric_value - min_val) / (max_val - min_val) *
                    (target_range[1] - target_range[0]) + target_range[0]
                )
        else:
            self.normalized_value = self.metric_value
        
        return self.normalized_value

    def set_threshold(self, threshold: float) -> None:
        """Set threshold for this metric"""
        self.threshold = threshold

    def is_passing(self) -> Optional[bool]:
        """Check if metric passes threshold"""
        if self.threshold is None:
            return None
        
        higher_is_better = self.metadata.get('higher_is_better', True)
        if higher_is_better:
            return self.metric_value >= self.threshold
        else:
            return self.metric_value <= self.threshold

    def compare(self, other: 'Metric') -> float:
        """Compare with another metric"""
        if self.metric_name != other.metric_name:
            raise ValueError("Cannot compare different metric types")
        
        return self.metric_value - other.metric_value

    def to_dict(self) -> Dict[str, Any]:
        """Convert metric to dictionary"""
        return {
            'metric_id': self.metric_id,
            'result_id': self.result_id,
            'metric_name': self.metric_name,
            'metric_value': self.metric_value,
            'normalized_value': self.normalized_value,
            'details': self.details,
            'threshold': self.threshold,
            'weight': self.weight,
            'calculated_at': self.calculated_at.isoformat(),
            'metadata': self.metadata,
            'is_passing': self.is_passing()
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Metric':
        """Create metric from dictionary"""
        metric = cls(
            result_id=data['result_id'],
            metric_name=data['metric_name'],
            metric_value=data['metric_value'],
            metric_id=data.get('metric_id')
        )
        metric.details = data.get('details', {})
        metric.normalized_value = data.get('normalized_value')
        metric.threshold = data.get('threshold')
        metric.weight = data.get('weight', 1.0)
        metric.calculated_at = datetime.fromisoformat(data['calculated_at']) if 'calculated_at' in data else datetime.now()
        metric.metadata = data.get('metadata', {})
        return metric

    @classmethod
    def create_pass_rate(cls, result_id: str, passed: int, total: int) -> 'Metric':
        """Create a pass rate metric"""
        value = passed / total if total > 0 else 0
        metric = cls(
            result_id=result_id,
            metric_name='pass_rate',
            metric_value=value
        )
        metric.metadata = {
            'higher_is_better': True,
            'range': (0, 1),
            'description': 'Percentage of passing tests'
        }
        metric.set_details({
            'passed': passed,
            'total': total
        })
        return metric

    @classmethod
    def create_execution_time(cls, result_id: str, time_ms: float) -> 'Metric':
        """Create an execution time metric"""
        metric = cls(
            result_id=result_id,
            metric_name='execution_time',
            metric_value=time_ms
        )
        metric.metadata = {
            'higher_is_better': False,
            'range': (0, float('inf')),
            'description': 'Execution time in milliseconds',
            'unit': 'ms'
        }
        return metric

    @classmethod
    def create_codebleu(cls, result_id: str, score: float, components: Dict[str, float]) -> 'Metric':
        """Create a CodeBLEU metric"""
        metric = cls(
            result_id=result_id,
            metric_name='codebleu',
            metric_value=score
        )
        metric.metadata = {
            'higher_is_better': True,
            'range': (0, 1),
            'description': 'CodeBLEU score for semantic similarity'
        }
        metric.set_details({
            'components': components,
            'bleu': components.get('bleu', 0),
            'syntax_match': components.get('syntax_match', 0),
            'dataflow_match': components.get('dataflow_match', 0)
        })
        return metric

--------------------------------------------------------------------------------
FILE 43/71: src\entities\problem.py
--------------------------------------------------------------------------------


from datetime import datetime
from typing import List, Dict, Any, Optional
import json
import hashlib
import ast
import radon


class Problem:
    def __init__(
        self,
        task_id: str,
        prompt: str,
        dataset_id: str,
        problem_id: Optional[str] = None
    ):
        self.problem_id = problem_id or self._generate_id(task_id)
        self.task_id = task_id
        self.dataset_id = dataset_id
        self.prompt = prompt
        self.canonical_solution: Optional[str] = None
        self.test_cases: List[Dict[str, Any]] = []
        self.function_signature: Optional[str] = None
        self.entry_point: Optional[str] = None
        self.difficulty: str = "unknown"
        self.categories: List[str] = []
        self.metadata: Dict[str, Any] = {}
        self.created_at = datetime.now()
        self.stats: Dict[str, Any] = {}

    def _generate_id(self, task_id: str) -> str:
        """Generate a unique problem ID"""
        hash_obj = hashlib.md5(task_id.encode())
        return f"prob_{hash_obj.hexdigest()[:8]}"

    def set_canonical_solution(self, solution: str) -> None:
        """Set the canonical solution"""
        self.canonical_solution = solution
        self._extract_function_info()

    def _extract_function_info(self) -> None:
        """Extract function information from solution"""
        if not self.canonical_solution:
            return
        
        try:
            tree = ast.parse(self.canonical_solution)
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    self.function_signature = ast.unparse(node)
                    self.entry_point = node.name
                    break
        except Exception:
            pass

    def add_test_case(self, test_case: Dict[str, Any]) -> None:
        """Add a test case"""
        self.test_cases.append(test_case)

    def set_test_cases(self, test_code: str) -> None:
        """Parse and set test cases from test code"""
        # Parse assert statements from test code
        lines = test_code.strip().split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('assert'):
                self.test_cases.append({
                    'assertion': line,
                    'type': 'assert'
                })

    def get_prompt(self, include_signature: bool = True) -> str:
        """Get the problem prompt"""
        if include_signature and self.function_signature:
            return f"{self.prompt}\n\n# Complete the function:\n{self.function_signature}"
        return self.prompt

    def run_tests(self, code: str) -> List[Dict[str, Any]]:
        """Run test cases against provided code"""
        results = []
        test_globals = {}
        
        try:
            # Execute the code
            exec(code, test_globals)
            
            # Run each test case
            for i, test in enumerate(self.test_cases):
                try:
                    if 'assertion' in test:
                        exec(test['assertion'], test_globals)
                        results.append({
                            'test_id': i,
                            'passed': True,
                            'message': f'Test {i+1} passed'
                        })
                except AssertionError as e:
                    results.append({
                        'test_id': i,
                        'passed': False,
                        'message': f'Test {i+1} failed: {str(e)}',
                        'assertion': test.get('assertion')
                    })
                except Exception as e:
                    results.append({
                        'test_id': i,
                        'passed': False,
                        'message': f'Test {i+1} error: {str(e)}',
                        'assertion': test.get('assertion')
                    })
                    
        except Exception as e:
            results.append({
                'test_id': 0,
                'passed': False,
                'message': f'Code execution failed: {str(e)}'
            })
        
        return results

    def validate_solution(self, code: str) -> bool:
        """Validate if code passes all tests"""
        results = self.run_tests(code)
        return all(r.get('passed', False) for r in results)

    def calculate_complexity(self) -> Dict[str, Any]:
        """Calculate code complexity metrics"""
        if not self.canonical_solution:
            return {}
        
        try:
            from radon.raw import analyze
            from radon.complexity import cc_visit
            
            raw = analyze(self.canonical_solution)
            cc = cc_visit(self.canonical_solution)
            
            return {
                'loc': raw.loc,
                'lloc': raw.lloc,
                'comments': raw.comments,
                'cyclomatic_complexity': sum(c.complexity for c in cc) / len(cc) if cc else 0,
                'functions': len(cc)
            }
        except Exception:
            return {}

    def to_dict(self) -> Dict[str, Any]:
        """Convert problem to dictionary"""
        return {
            'problem_id': self.problem_id,
            'task_id': self.task_id,
            'dataset_id': self.dataset_id,
            'prompt': self.prompt,
            'canonical_solution': self.canonical_solution,
            'test_cases': self.test_cases,
            'function_signature': self.function_signature,
            'entry_point': self.entry_point,
            'difficulty': self.difficulty,
            'categories': self.categories,
            'metadata': self.metadata,
            'created_at': self.created_at.isoformat(),
            'stats': self.stats or self.calculate_complexity()
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Problem':
        """Create problem from dictionary"""
        problem = cls(
            task_id=data['task_id'],
            prompt=data['prompt'],
            dataset_id=data['dataset_id'],
            problem_id=data.get('problem_id')
        )
        problem.canonical_solution = data.get('canonical_solution')
        problem.test_cases = data.get('test_cases', [])
        problem.function_signature = data.get('function_signature')
        problem.entry_point = data.get('entry_point')
        problem.difficulty = data.get('difficulty', 'unknown')
        problem.categories = data.get('categories', [])
        problem.metadata = data.get('metadata', {})
        problem.created_at = datetime.fromisoformat(data['created_at']) if 'created_at' in data else datetime.now()
        problem.stats = data.get('stats', {})
        return problem

--------------------------------------------------------------------------------
FILE 44/71: src\entities\report.py
--------------------------------------------------------------------------------


from datetime import datetime
from typing import List, Dict, Any, Optional
import secrets
from enum import Enum


class ReportFormat(Enum):
    HTML = "html"
    PDF = "pdf"
    JSON = "json"
    CSV = "csv"
    MARKDOWN = "md"
    LATEX = "latex"


class ReportType(Enum):
    SUMMARY = "summary"
    DETAILED = "detailed"
    COMPARATIVE = "comparative"
    ERROR_ANALYSIS = "error_analysis"
    TUNING = "tuning"


class Report:
    def __init__(
        self,
        evaluation_id: str,
        report_type: ReportType,
        report_format: ReportFormat,
        report_id: Optional[str] = None
    ):
        self.report_id = report_id or self._generate_id()
        self.evaluation_id = evaluation_id
        self.report_type = report_type
        self.format = report_format
        self.file_path: Optional[str] = None
        self.summary_data: Dict[str, Any] = {}
        self.detailed_data: Dict[str, Any] = {}
        self.charts: Dict[str, Any] = {}
        self.tables: List[Dict[str, Any]] = []
        self.generated_at = datetime.now()
        self.download_count: int = 0
        self.last_downloaded: Optional[datetime] = None
        self.metadata: Dict[str, Any] = {}
        self.is_public: bool = False
        self.tags: List[str] = []

    def _generate_id(self) -> str:
        """Generate a unique report ID"""
        return f"rpt_{secrets.token_hex(8)}"

    def set_file_path(self, path: str) -> None:
        """Set the file path"""
        self.file_path = path

    def add_summary(self, data: Dict[str, Any]) -> None:
        """Add summary data"""
        self.summary_data.update(data)

    def add_detailed_data(self, data: Dict[str, Any]) -> None:
        """Add detailed data"""
        self.detailed_data.update(data)

    def add_chart(self, name: str, chart_data: Dict[str, Any]) -> None:
        """Add a chart"""
        self.charts[name] = chart_data

    def add_table(self, table: Dict[str, Any]) -> None:
        """Add a table"""
        self.tables.append(table)

    def increment_download(self) -> None:
        """Increment download count"""
        self.download_count += 1
        self.last_downloaded = datetime.now()

    def make_public(self) -> None:
        """Make report public"""
        self.is_public = True

    def make_private(self) -> None:
        """Make report private"""
        self.is_public = False

    def add_tag(self, tag: str) -> None:
        """Add a tag"""
        if tag not in self.tags:
            self.tags.append(tag)

    def get_filename(self) -> str:
        """Get the filename for this report"""
        timestamp = self.generated_at.strftime('%Y%m%d_%H%M%S')
        return f"report_{self.evaluation_id}_{timestamp}.{self.format.value}"

    def to_dict(self) -> Dict[str, Any]:
        """Convert report to dictionary"""
        return {
            'report_id': self.report_id,
            'evaluation_id': self.evaluation_id,
            'report_type': self.report_type.value,
            'format': self.format.value,
            'file_path': self.file_path,
            'summary_data': self.summary_data,
            'detailed_data': self.detailed_data,
            'charts': self.charts,
            'tables': self.tables,
            'generated_at': self.generated_at.isoformat(),
            'download_count': self.download_count,
            'last_downloaded': self.last_downloaded.isoformat() if self.last_downloaded else None,
            'metadata': self.metadata,
            'is_public': self.is_public,
            'tags': self.tags,
            'filename': self.get_filename()
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Report':
        """Create report from dictionary"""
        report = cls(
            evaluation_id=data['evaluation_id'],
            report_type=ReportType(data['report_type']),
            report_format=ReportFormat(data['format']),
            report_id=data.get('report_id')
        )
        report.file_path = data.get('file_path')
        report.summary_data = data.get('summary_data', {})
        report.detailed_data = data.get('detailed_data', {})
        report.charts = data.get('charts', {})
        report.tables = data.get('tables', [])
        report.generated_at = datetime.fromisoformat(data['generated_at']) if 'generated_at' in data else datetime.now()
        report.download_count = data.get('download_count', 0)
        report.last_downloaded = datetime.fromisoformat(data['last_downloaded']) if data.get('last_downloaded') else None
        report.metadata = data.get('metadata', {})
        report.is_public = data.get('is_public', False)
        report.tags = data.get('tags', [])
        return report

    @classmethod
    def create_summary_report(
        cls,
        evaluation_id: str,
        format: ReportFormat = ReportFormat.HTML
    ) -> 'Report':
        """Create a summary report"""
        return cls(
            evaluation_id=evaluation_id,
            report_type=ReportType.SUMMARY,
            report_format=format
        )

    @classmethod
    def create_detailed_report(
        cls,
        evaluation_id: str,
        format: ReportFormat = ReportFormat.HTML
    ) -> 'Report':
        """Create a detailed report"""
        return cls(
            evaluation_id=evaluation_id,
            report_type=ReportType.DETAILED,
            report_format=format
        )

--------------------------------------------------------------------------------
FILE 45/71: src\entities\user.py
--------------------------------------------------------------------------------


from datetime import datetime
from typing import List, Dict, Any, Optional
import hashlib
import secrets
import json


class User:
    def __init__(
        self,
        username: str,
        email: str,
        role: str,
        organization: str = "",
        user_id: Optional[str] = None
    ):
        self.user_id = user_id or self._generate_id()
        self.username = username
        self.email = email
        self.password_hash: Optional[str] = None
        self.role = role  # researcher, developer, admin, viewer
        self.organization = organization
        self.created_at = datetime.now()
        self.last_login: Optional[datetime] = None
        self.is_active = True
        self.preferences: Dict[str, Any] = {
            'theme': 'light',
            'notifications': True,
            'default_models': [],
            'export_format': 'json'
        }
        self.session_token: Optional[str] = None

    def _generate_id(self) -> str:
        """Generate a unique user ID"""
        return f"user_{secrets.token_hex(8)}"

    def set_password(self, password: str) -> None:
        """Set password hash"""
        salt = secrets.token_hex(16)
        self.password_hash = self._hash_password(password, salt)

    def authenticate(self, password: str) -> bool:
        """Authenticate user with password"""
        if not self.password_hash:
            return False
        stored_hash, salt = self.password_hash.split(':')
        computed_hash = self._hash_password(password, salt)
        return secrets.compare_digest(stored_hash, computed_hash)

    def _hash_password(self, password: str, salt: str) -> str:
        """Hash password with salt"""
        hash_obj = hashlib.sha256((password + salt).encode())
        return f"{hash_obj.hexdigest()}:{salt}"

    def create_session(self) -> str:
        """Create a new session token"""
        self.session_token = secrets.token_urlsafe(32)
        self.last_login = datetime.now()
        return self.session_token

    def validate_session(self, token: str) -> bool:
        """Validate session token"""
        return self.session_token == token and self.is_active

    def has_permission(self, action: str) -> bool:
        """Check if user has permission for an action"""
        permissions = {
            'admin': ['*'],
            'researcher': [
                'create_evaluation', 'view_results', 'export',
                'compare_models', 'tune_models', 'create_reports'
            ],
            'developer': [
                'create_evaluation', 'view_results', 'export'
            ],
            'viewer': ['view_results']
        }
        
        if self.role == 'admin':
            return True
        
        user_perms = permissions.get(self.role, [])
        return action in user_perms or '*' in user_perms

    def update_profile(self, info: Dict[str, Any]) -> bool:
        """Update user profile information"""
        try:
            allowed_fields = ['email', 'organization', 'preferences']
            for field, value in info.items():
                if field in allowed_fields:
                    setattr(self, field, value)
            return True
        except Exception:
            return False

    def change_password(self, old_password: str, new_password: str) -> bool:
        """Change user password"""
        if not self.authenticate(old_password):
            return False
        self.set_password(new_password)
        return True

    def update_preferences(self, preferences: Dict[str, Any]) -> None:
        """Update user preferences"""
        self.preferences.update(preferences)

    def to_dict(self) -> Dict[str, Any]:
        """Convert user to dictionary (safe version without sensitive data)"""
        return {
            'user_id': self.user_id,
            'username': self.username,
            'email': self.email,
            'role': self.role,
            'organization': self.organization,
            'created_at': self.created_at.isoformat(),
            'last_login': self.last_login.isoformat() if self.last_login else None,
            'is_active': self.is_active,
            'preferences': self.preferences
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'User':
        """Create user from dictionary"""
        user = cls(
            username=data['username'],
            email=data['email'],
            role=data['role'],
            organization=data.get('organization', ''),
            user_id=data.get('user_id')
        )
        user.password_hash = data.get('password_hash')
        user.created_at = datetime.fromisoformat(data['created_at']) if 'created_at' in data else datetime.now()
        user.last_login = datetime.fromisoformat(data['last_login']) if data.get('last_login') else None
        user.is_active = data.get('is_active', True)
        user.preferences = data.get('preferences', {})
        return user

--------------------------------------------------------------------------------
FILE 46/71: src\executors\__init__.py
--------------------------------------------------------------------------------


from .sandbox_executor import SandboxExecutor
from .resource_manager import ResourceManager

__all__ = ['SandboxExecutor', 'ResourceManager']

--------------------------------------------------------------------------------
FILE 47/71: src\executors\resource_manager.py
--------------------------------------------------------------------------------


from typing import Dict, Any, Optional
import psutil
import os
import threading
import time
import logging
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class ResourceLimits:
    """Resource limits for execution"""
    max_cpu_percent: float = 80.0
    max_memory_mb: float = 1024.0
    max_disk_mb: float = 1024.0
    max_processes: int = 10
    max_execution_time: int = 60


@dataclass
class ResourceUsage:
    """Resource usage statistics"""
    cpu_percent: float
    memory_mb: float
    disk_mb: float
    process_count: int
    execution_time: float


class ResourceManager:
    """Manages system resources"""
    
    def __init__(self, limits: Optional[ResourceLimits] = None):
        self.limits = limits or ResourceLimits()
        self.processes: Dict[int, Dict[str, Any]] = {}
        self.monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None
        self.lock = threading.Lock()

    def check_resources(self) -> Dict[str, Any]:
        """Check current resource usage"""
        try:
            # CPU usage
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # Memory usage
            memory = psutil.virtual_memory()
            memory_mb = memory.used / (1024 * 1024)
            
            # Disk usage
            disk = psutil.disk_usage('/')
            disk_mb = disk.used / (1024 * 1024)
            
            # Process count
            process_count = len(psutil.pids())
            
            return {
                'cpu_percent': cpu_percent,
                'memory_mb': memory_mb,
                'memory_percent': memory.percent,
                'disk_mb': disk_mb,
                'disk_percent': disk.percent,
                'process_count': process_count,
                'available_memory_mb': memory.available / (1024 * 1024)
            }
            
        except Exception as e:
            logger.error(f"Failed to check resources: {e}")
            return {
                'cpu_percent': 0,
                'memory_mb': 0,
                'process_count': 0,
                'error': str(e)
            }

    def can_execute(self, estimated_memory_mb: float = 100) -> bool:
        """Check if we can execute a new process"""
        resources = self.check_resources()
        
        with self.lock:
            # Check CPU
            if resources.get('cpu_percent', 0) > self.limits.max_cpu_percent:
                logger.warning(f"CPU usage too high: {resources['cpu_percent']}%")
                return False
            
            # Check memory
            if resources.get('memory_mb', 0) + estimated_memory_mb > self.limits.max_memory_mb:
                logger.warning(f"Memory usage would exceed limit")
                return False
            
            # Check process count
            if resources.get('process_count', 0) > self.limits.max_processes:
                logger.warning(f"Too many processes: {resources['process_count']}")
                return False
            
            return True

    def register_process(self, pid: int, metadata: Dict[str, Any]) -> None:
        """Register a process for monitoring"""
        with self.lock:
            self.processes[pid] = {
                'pid': pid,
                'start_time': time.time(),
                'metadata': metadata,
                'status': 'running'
            }
        logger.info(f"Registered process {pid}")

    def unregister_process(self, pid: int) -> None:
        """Unregister a process"""
        with self.lock:
            if pid in self.processes:
                self.processes[pid]['status'] = 'completed'
                self.processes[pid]['end_time'] = time.time()
                logger.info(f"Unregistered process {pid}")

    def get_process_stats(self, pid: int) -> Optional[Dict[str, Any]]:
        """Get statistics for a process"""
        try:
            process = psutil.Process(pid)
            
            with process.oneshot():
                cpu_percent = process.cpu_percent(interval=0.1)
                memory_info = process.memory_info()
                memory_mb = memory_info.rss / (1024 * 1024)
                
                stats = {
                    'pid': pid,
                    'cpu_percent': cpu_percent,
                    'memory_mb': memory_mb,
                    'status': process.status(),
                    'create_time': process.create_time(),
                    'num_threads': process.num_threads()
                }
                
                # Add registered metadata if available
                with self.lock:
                    if pid in self.processes:
                        stats.update(self.processes[pid].get('metadata', {}))
                
                return stats
                
        except psutil.NoSuchProcess:
            return None
        except Exception as e:
            logger.error(f"Failed to get process stats for {pid}: {e}")
            return None

    def kill_process(self, pid: int) -> bool:
        """Kill a process"""
        try:
            process = psutil.Process(pid)
            process.terminate()
            
            # Wait for termination
            gone, alive = psutil.wait_procs([process], timeout=3)
            if alive:
                process.kill()
            
            self.unregister_process(pid)
            logger.info(f"Killed process {pid}")
            return True
            
        except psutil.NoSuchProcess:
            self.unregister_process(pid)
            return True
        except Exception as e:
            logger.error(f"Failed to kill process {pid}: {e}")
            return False

    def kill_all_processes(self) -> None:
        """Kill all registered processes"""
        with self.lock:
            pids = list(self.processes.keys())
        
        for pid in pids:
            self.kill_process(pid)

    def start_monitoring(self, interval: float = 5.0) -> None:
        """Start resource monitoring"""
        if self.monitoring:
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(interval,),
            daemon=True
        )
        self.monitor_thread.start()
        logger.info("Resource monitoring started")

    def stop_monitoring(self) -> None:
        """Stop resource monitoring"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=10)
        logger.info("Resource monitoring stopped")

    def _monitor_loop(self, interval: float) -> None:
        """Main monitoring loop"""
        while self.monitoring:
            try:
                # Check system resources
                resources = self.check_resources()
                
                # Check registered processes
                with self.lock:
                    for pid in list(self.processes.keys()):
                        stats = self.get_process_stats(pid)
                        if stats:
                            self.processes[pid]['last_stats'] = stats
                        else:
                            # Process died unexpectedly
                            self.processes[pid]['status'] = 'terminated'
                            self.processes[pid]['end_time'] = time.time()
                
                # Check if we need to kill any processes
                if resources.get('cpu_percent', 0) > self.limits.max_cpu_percent * 1.5:
                    logger.warning("CPU usage critical, killing oldest process")
                    self._kill_oldest_process()
                
                if resources.get('memory_mb', 0) > self.limits.max_memory_mb * 1.5:
                    logger.warning("Memory usage critical, killing largest process")
                    self._kill_largest_process()
                
            except Exception as e:
                logger.error(f"Monitoring error: {e}")
            
            time.sleep(interval)

    def _kill_oldest_process(self) -> None:
        """Kill the oldest running process"""
        with self.lock:
            if not self.processes:
                return
            
            oldest_pid = min(
                self.processes.items(),
                key=lambda x: x[1].get('start_time', float('inf'))
            )[0]
        
        self.kill_process(oldest_pid)

    def _kill_largest_process(self) -> None:
        """Kill the process using most memory"""
        with self.lock:
            if not self.processes:
                return
            
            largest_pid = None
            largest_memory = 0
            
            for pid in self.processes:
                stats = self.get_process_stats(pid)
                if stats and stats.get('memory_mb', 0) > largest_memory:
                    largest_memory = stats['memory_mb']
                    largest_pid = pid
            
            if largest_pid:
                self.kill_process(largest_pid)

    def get_summary(self) -> Dict[str, Any]:
        """Get resource management summary"""
        resources = self.check_resources()
        
        with self.lock:
            running_processes = [
                p for p in self.processes.values()
                if p.get('status') == 'running'
            ]
            
            return {
                'system': resources,
                'limits': {
                    'max_cpu_percent': self.limits.max_cpu_percent,
                    'max_memory_mb': self.limits.max_memory_mb,
                    'max_disk_mb': self.limits.max_disk_mb,
                    'max_processes': self.limits.max_processes,
                    'max_execution_time': self.limits.max_execution_time
                },
                'processes': {
                    'total': len(self.processes),
                    'running': len(running_processes),
                    'completed': len(self.processes) - len(running_processes)
                },
                'can_execute': self.can_execute()
            }

--------------------------------------------------------------------------------
FILE 48/71: src\executors\sandbox_executor.py
--------------------------------------------------------------------------------


from typing import List, Dict, Any, Optional, Callable
import asyncio
import docker
import tempfile
import os
import signal
import threading
import queue
import json
import time
import logging
from datetime import datetime

logger = logging.getLogger(__name__)


class SandboxExecutor:
    """Safe code execution in Docker sandbox"""
    
    def __init__(
        self,
        timeout: int = 30,
        memory_limit: str = "512m",
        cpu_limit: float = 1.0,
        network_enabled: bool = False,
        docker_client: Optional[docker.DockerClient] = None
    ):
        self.timeout = timeout
        self.memory_limit = memory_limit
        self.cpu_limit = cpu_limit
        self.network_enabled = network_enabled
        self.docker_client = docker_client or self._init_docker()
        self.temp_dir = tempfile.mkdtemp(prefix="sandbox_")
        self.supported_languages = self._detect_supported_languages()
        
        logger.info(f"SandboxExecutor initialized with timeout={timeout}s, memory={memory_limit}")

    def _init_docker(self) -> Optional[docker.DockerClient]:
        """Initialize Docker client"""
        try:
            client = docker.from_env()
            # Test connection
            client.ping()
            logger.info("Docker client initialized successfully")
            return client
        except Exception as e:
            logger.warning(f"Docker not available: {e}. Using fallback execution.")
            return None

    def _detect_supported_languages(self) -> List[str]:
        """Detect supported languages"""
        languages = ['python']
        
        if self.docker_client:
            # Check if we have images for other languages
            try:
                images = self.docker_client.images.list()
                image_tags = []
                for img in images:
                    image_tags.extend(img.tags)
                
                language_images = {
                    'python': ['python:3.9-slim', 'python:3.8-slim'],
                    'javascript': ['node:16-slim'],
                    'java': ['openjdk:11-slim'],
                    'cpp': ['gcc:latest'],
                    'go': ['golang:1.17']
                }
                
                for lang, required_images in language_images.items():
                    if any(any(req in tag for req in required_images) for tag in image_tags):
                        languages.append(lang)
            except Exception as e:
                logger.warning(f"Failed to detect language images: {e}")
        
        return languages

    async def execute_safely(
        self,
        code: str,
        test_cases: List[Dict[str, Any]],
        problem_id: str,
        language: str = "python"
    ) -> Dict[str, Any]:
        """Execute code safely"""
        if language not in self.supported_languages:
            return {
                'passed': False,
                'result': 'unsupported_language',
                'output': f"Language {language} not supported",
                'test_results': [],
                'errors': [{
                    'error_type': 'unsupported_language',
                    'error_message': f"Language {language} not supported"
                }],
                'execution_time_ms': 0
            }
        
        if self.docker_client:
            return await self._execute_with_docker(code, test_cases, problem_id, language)
        else:
            return await self._execute_with_fallback(code, test_cases, language)

    async def _execute_with_docker(
        self,
        code: str,
        test_cases: List[Dict[str, Any]],
        problem_id: str,
        language: str
    ) -> Dict[str, Any]:
        """Execute with Docker sandbox"""
        result_queue = queue.Queue()
        start_time = time.time()
        
        def run_container():
            temp_file = None
            try:
                # Create test program
                test_program = self._create_test_program(code, test_cases, language)
                
                # Write to temp file
                with tempfile.NamedTemporaryFile(
                    mode="w",
                    suffix=self._get_file_extension(language),
                    encoding='utf-8',
                    delete=False
                ) as f:
                    f.write(test_program)
                    temp_file = f.name
                
                # Get container image
                image = self._get_language_image(language)
                
                # Run container
                container = self.docker_client.containers.run(
                    image,
                    self._get_execution_command(language, "/tmp/test"),
                    volumes={temp_file: {"bind": "/tmp/test", "mode": "ro"}},
                    working_dir="/tmp",
                    stderr=True,
                    stdout=True,
                    remove=False,
                    mem_limit=self.memory_limit,
                    nano_cpus=int(self.cpu_limit * 1e9),
                    network_disabled=not self.network_enabled,
                    detach=True
                )
                
                # Wait for completion with timeout
                try:
                    result = container.wait(timeout=self.timeout)
                    logs = container.logs(stdout=True, stderr=True).decode('utf-8', errors='replace')
                    
                    # Parse results
                    execution_result = self._parse_execution_output(logs)
                    
                except docker.errors.APIError as e:
                    if "Timeout" in str(e):
                        container.kill()
                        execution_result = {
                            'passed': False,
                            'result': 'timeout',
                            'output': f"Execution timed out after {self.timeout} seconds",
                            'test_results': []
                        }
                    else:
                        raise
                
                finally:
                    container.remove()
                
                result_queue.put(execution_result)
                
            except Exception as e:
                logger.error(f"Docker execution failed: {e}")
                result_queue.put({
                    'passed': False,
                    'result': f'container_error: {str(e)}',
                    'output': str(e),
                    'test_results': []
                })
            
            finally:
                if temp_file and os.path.exists(temp_file):
                    try:
                        os.unlink(temp_file)
                    except Exception as e:
                        logger.warning(f"Could not delete temp file: {e}")
        
        # Run in thread
        thread = threading.Thread(target=run_container)
        thread.daemon = True
        thread.start()
        thread.join(timeout=self.timeout + 5)
        
        execution_time_ms = (time.time() - start_time) * 1000
        
        if thread.is_alive():
            return {
                'passed': False,
                'result': 'timeout',
                'output': f"Execution timed out after {self.timeout} seconds",
                'test_results': [],
                'execution_time_ms': execution_time_ms
            }
        
        if result_queue.empty():
            return {
                'passed': False,
                'result': 'unknown_error',
                'output': "No result from container",
                'test_results': [],
                'execution_time_ms': execution_time_ms
            }
        
        result = result_queue.get()
        result['execution_time_ms'] = execution_time_ms
        return result

    def _create_test_program(
        self,
        code: str,
        test_cases: List[Dict[str, Any]],
        language: str
    ) -> str:
        """Create a test program for the given language"""
        if language == 'python':
            return self._create_python_test_program(code, test_cases)
        elif language == 'javascript':
            return self._create_javascript_test_program(code, test_cases)
        elif language == 'java':
            return self._create_java_test_program(code, test_cases)
        else:
            return code

    def _create_python_test_program(self, code: str, test_cases: List[Dict]) -> str:
        """Create Python test program"""
        test_functions = ""
        for i, test in enumerate(test_cases):
            if 'assertion' in test:
                test_functions += f"""
def test_{i}():
    try:
        {test['assertion']}
        return True, "Test {i+1} passed"
    except AssertionError as e:
        return False, f"Test {i+1} failed: {{e}}"
    except Exception as e:
        return False, f"Test {i+1} error: {{e}}"
"""
        
        return f"""
{code}

{test_functions}

import json
import sys

test_results = []
passed_count = 0
failed_count = 0

for i in range({len(test_cases)}):
    passed, message = locals()[f"test_{{i}}"]()
    test_results.append({{
        "test_id": i,
        "passed": passed,
        "message": message,
        "test_case": {json.dumps(test_cases[i] if i < len(test_cases) else {})}
    }})
    if passed:
        passed_count += 1
    else:
        failed_count += 1

output = {{
    "total_tests": {len(test_cases)},
    "passed": passed_count,
    "failed": failed_count,
    "test_results": test_results
}}

print("JSON_RESULTS:" + json.dumps(output))

if failed_count == 0:
    sys.exit(0)
else:
    sys.exit(1)
"""

    def _parse_execution_output(self, output: str) -> Dict[str, Any]:
        """Parse execution output"""
        test_results = []
        passed = False
        result = "unknown"
        
        # Look for JSON results
        import re
        json_match = re.search(r'JSON_RESULTS:(\{.*?\})(?:\n|$)', output, re.DOTALL)
        
        if json_match:
            try:
                json_str = json_match.group(1)
                data = json.loads(json_str)
                test_results = data.get('test_results', [])
                passed = data.get('failed', 0) == 0
                result = "passed" if passed else "failed"
            except json.JSONDecodeError:
                pass
        
        # Fallback to simple parsing
        if not test_results:
            if "RESULT: PASSED" in output or "All tests passed" in output:
                passed = True
                result = "passed"
            elif "FAILED" in output or "Error" in output:
                passed = False
                result = "failed"
        
        return {
            'passed': passed,
            'result': result,
            'output': output.strip(),
            'test_results': test_results
        }

    def _get_file_extension(self, language: str) -> str:
        """Get file extension for language"""
        extensions = {
            'python': '.py',
            'javascript': '.js',
            'java': '.java',
            'cpp': '.cpp',
            'go': '.go'
        }
        return extensions.get(language, '.txt')

    def _get_language_image(self, language: str) -> str:
        """Get Docker image for language"""
        images = {
            'python': 'python:3.9-slim',
            'javascript': 'node:16-slim',
            'java': 'openjdk:11-slim',
            'cpp': 'gcc:latest',
            'go': 'golang:1.17'
        }
        return images.get(language, 'python:3.9-slim')

    def _get_execution_command(self, language: str, file_path: str) -> List[str]:
        """Get execution command for language"""
        commands = {
            'python': ['python', file_path],
            'javascript': ['node', file_path],
            'java': ['java', file_path],
            'cpp': ['sh', '-c', f'g++ {file_path}.cpp -o /tmp/a.out && /tmp/a.out'],
            'go': ['go', 'run', file_path]
        }
        return commands.get(language, ['python', file_path])

    async def _execute_with_fallback(
        self,
        code: str,
        test_cases: List[Dict[str, Any]],
        language: str
    ) -> Dict[str, Any]:
        """Fallback execution without Docker"""
        if language != 'python':
            return {
                'passed': False,
                'result': 'unsupported_language',
                'output': f"Fallback only supports Python, not {language}",
                'test_results': [],
                'errors': [{
                    'error_type': 'unsupported_language',
                    'error_message': f"Fallback only supports Python"
                }]
            }
        
        start_time = time.time()
        test_results = []
        passed_count = 0
        
        try:
            # Execute code to define functions
            exec_globals = {}
            exec(code, exec_globals)
            
            # Run test cases
            for i, test in enumerate(test_cases):
                try:
                    if 'assertion' in test:
                        exec(test['assertion'], exec_globals)
                        test_results.append({
                            'test_id': i,
                            'passed': True,
                            'message': f"Test {i+1} passed"
                        })
                        passed_count += 1
                except AssertionError as e:
                    test_results.append({
                        'test_id': i,
                        'passed': False,
                        'message': f"Test {i+1} failed: {e}",
                        'test_case': test.get('assertion')
                    })
                except Exception as e:
                    test_results.append({
                        'test_id': i,
                        'passed': False,
                        'message': f"Test {i+1} error: {e}",
                        'test_case': test.get('assertion')
                    })
            
            execution_time_ms = (time.time() - start_time) * 1000
            all_passed = passed_count == len(test_cases)
            
            return {
                'passed': all_passed,
                'result': 'passed' if all_passed else 'failed',
                'output': f"Executed {len(test_cases)} tests, {passed_count} passed",
                'test_results': test_results,
                'execution_time_ms': execution_time_ms,
                'errors': []
            }
            
        except Exception as e:
            execution_time_ms = (time.time() - start_time) * 1000
            return {
                'passed': False,
                'result': f'execution_error: {str(e)}',
                'output': str(e),
                'test_results': test_results,
                'execution_time_ms': execution_time_ms,
                'errors': [{
                    'error_type': 'execution_error',
                    'error_message': str(e)
                }]
            }

    def set_resource_limits(
        self,
        timeout: Optional[int] = None,
        memory_limit: Optional[str] = None,
        cpu_limit: Optional[float] = None
    ) -> None:
        """Set resource limits"""
        if timeout is not None:
            self.timeout = timeout
        if memory_limit is not None:
            self.memory_limit = memory_limit
        if cpu_limit is not None:
            self.cpu_limit = cpu_limit
        
        logger.info(f"Updated limits: timeout={self.timeout}, memory={self.memory_limit}, cpu={self.cpu_limit}")

    def get_supported_languages(self) -> List[str]:
        """Get list of supported languages"""
        return self.supported_languages.copy()

    def cleanup(self) -> None:
        """Clean up temporary files"""
        import shutil
        if os.path.exists(self.temp_dir):
            try:
                shutil.rmtree(self.temp_dir)
                logger.info(f"Cleaned up temp directory: {self.temp_dir}")
            except Exception as e:
                logger.warning(f"Failed to clean up temp directory: {e}")

--------------------------------------------------------------------------------
FILE 49/71: src\generators\__init__.py
--------------------------------------------------------------------------------

"""Generator classes for reports and exports."""

from .report_generator import ReportGenerator
from .exporters import CSVExporter, JSONExporter, PDFExporter, HTMLExporter

__all__ = [
    'ReportGenerator',
    'CSVExporter',
    'JSONExporter', 
    'PDFExporter',
    'HTMLExporter'
]

--------------------------------------------------------------------------------
FILE 50/71: src\generators\exporters.py
--------------------------------------------------------------------------------


from abc import ABC, abstractmethod
import json
import csv
from typing import Dict, Any, List
from datetime import datetime
import logging
from pathlib import Path

from ..entities import Report

logger = logging.getLogger(__name__)


class BaseExporter(ABC):
    """Base class for all exporters"""
    
    @abstractmethod
    def export(self, report: Report, file_path: str) -> bool:
        """Export report to file"""
        pass


class CSVExporter(BaseExporter):
    """Export report as CSV"""
    
    def export(self, report: Report, file_path: str) -> bool:
        try:
            with open(file_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                # Write header
                writer.writerow(['Report ID', report.report_id])
                writer.writerow(['Evaluation ID', report.evaluation_id])
                writer.writerow(['Generated At', report.generated_at.isoformat()])
                writer.writerow([])
                
                # Write summary data
                if report.summary_data:
                    writer.writerow(['SUMMARY'])
                    self._write_dict_to_csv(writer, report.summary_data)
                
                # Write tables
                for table in report.tables:
                    writer.writerow([])
                    writer.writerow([table.get('title', 'Table')])
                    writer.writerow(table.get('headers', []))
                    for row in table.get('rows', []):
                        writer.writerow(row)
            
            logger.info(f"Exported CSV report to {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to export CSV: {e}")
            return False
    
    def _write_dict_to_csv(self, writer, data: Dict, prefix: str = ''):
        """Write dictionary to CSV"""
        for key, value in data.items():
            if isinstance(value, dict):
                self._write_dict_to_csv(writer, value, f"{prefix}{key}.")
            else:
                writer.writerow([f"{prefix}{key}", str(value)])


class JSONExporter(BaseExporter):
    """Export report as JSON"""
    
    def export(self, report: Report, file_path: str) -> bool:
        try:
            data = {
                'report_id': report.report_id,
                'evaluation_id': report.evaluation_id,
                'report_type': report.report_type.value,
                'format': report.format.value,
                'generated_at': report.generated_at.isoformat(),
                'summary': report.summary_data,
                'detailed': report.detailed_data,
                'charts': report.charts,
                'tables': report.tables,
                'metadata': report.metadata,
                'tags': report.tags
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, default=str)
            
            logger.info(f"Exported JSON report to {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to export JSON: {e}")
            return False


class PDFExporter(BaseExporter):
    """Export report as PDF"""
    
    def export(self, report: Report, file_path: str) -> bool:
        try:
            from reportlab.lib import colors
            from reportlab.lib.pagesizes import letter, landscape
            from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image
            from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
            from reportlab.lib.units import inch
            import io
            import base64
            
            doc = SimpleDocTemplate(file_path, pagesize=letter)
            styles = getSampleStyleSheet()
            story = []
            
            # Title
            title_style = ParagraphStyle(
                'CustomTitle',
                parent=styles['Heading1'],
                fontSize=24,
                spaceAfter=30
            )
            story.append(Paragraph(f"Evaluation Report: {report.report_id}", title_style))
            story.append(Spacer(1, 12))
            
            # Metadata
            story.append(Paragraph(f"Generated: {report.generated_at.strftime('%Y-%m-%d %H:%M:%S')}", styles['Normal']))
            story.append(Paragraph(f"Evaluation ID: {report.evaluation_id}", styles['Normal']))
            story.append(Spacer(1, 12))
            
            # Summary
            if report.summary_data:
                story.append(Paragraph("Summary", styles['Heading2']))
                story.append(Spacer(1, 6))
                
                summary_text = f"Total Results: {report.summary_data.get('total_results', 0)}<br/>"
                summary_text += f"Passed: {report.summary_data.get('passed_results', 0)}<br/>"
                summary_text += f"Pass Rate: {report.summary_data.get('pass_rate', 0)*100:.1f}%"
                
                story.append(Paragraph(summary_text, styles['Normal']))
                story.append(Spacer(1, 12))
            
            # Tables
            for table_data in report.tables:
                story.append(Paragraph(table_data.get('title', 'Table'), styles['Heading2']))
                story.append(Spacer(1, 6))
                
                data = [table_data.get('headers', [])]
                data.extend(table_data.get('rows', []))
                
                if data:
                    table = Table(data)
                    table.setStyle(TableStyle([
                        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                        ('FONTSIZE', (0, 0), (-1, 0), 12),
                        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                        ('GRID', (0, 0), (-1, -1), 1, colors.black)
                    ]))
                    story.append(table)
                    story.append(Spacer(1, 12))
            
            # Charts
            for name, chart_data in report.charts.items():
                if 'image' in chart_data:
                    story.append(Paragraph(name.replace('_', ' ').title(), styles['Heading2']))
                    story.append(Spacer(1, 6))
                    
                    # Decode base64 image
                    img_data = base64.b64decode(chart_data['image'])
                    img_buffer = io.BytesIO(img_data)
                    img = Image(img_buffer, width=6*inch, height=4*inch)
                    story.append(img)
                    story.append(Spacer(1, 12))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Exported PDF report to {file_path}")
            return True
            
        except ImportError:
            logger.error("ReportLab not installed. Install with: pip install reportlab")
            return False
        except Exception as e:
            logger.error(f"Failed to export PDF: {e}")
            return False


class HTMLExporter(BaseExporter):
    """Export report as HTML"""
    
    def export(self, report: Report, file_path: str) -> bool:
        try:
            html = self._generate_html(report)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(html)
            
            logger.info(f"Exported HTML report to {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to export HTML: {e}")
            return False
    
    def _generate_html(self, report: Report) -> str:
        """Generate HTML content"""
        html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Evaluation Report: {report.report_id}</title>
    <style>
        body {{
            font-family: Arial, sans-serif;
            margin: 40px;
            line-height: 1.6;
            color: #333;
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }}
        h2 {{
            color: #34495e;
            margin-top: 30px;
        }}
        .summary {{
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
        }}
        table {{
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }}
        th, td {{
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }}
        th {{
            background-color: #3498db;
            color: white;
        }}
        tr:nth-child(even) {{
            background-color: #f2f2f2;
        }}
        .chart {{
            margin: 30px 0;
            text-align: center;
        }}
        .chart img {{
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
        }}
        .footer {{
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
            text-align: center;
            color: #7f8c8d;
        }}
    </style>
</head>
<body>
    <h1>Evaluation Report: {report.report_id}</h1>
    
    <div class="metadata">
        <p><strong>Generated:</strong> {report.generated_at.strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p><strong>Evaluation ID:</strong> {report.evaluation_id}</p>
        <p><strong>Report Type:</strong> {report.report_type.value}</p>
    </div>
"""
        
        # Summary
        if report.summary_data:
            html += f"""
    <h2>Summary</h2>
    <div class="summary">
        <p><strong>Total Results:</strong> {report.summary_data.get('total_results', 0)}</p>
        <p><strong>Passed:</strong> {report.summary_data.get('passed_results', 0)}</p>
        <p><strong>Failed:</strong> {report.summary_data.get('total_results', 0) - report.summary_data.get('passed_results', 0)}</p>
        <p><strong>Pass Rate:</strong> {report.summary_data.get('pass_rate', 0)*100:.1f}%</p>
    </div>
"""
        
        # Detailed data
        if report.detailed_data:
            html += f"""
    <h2>Detailed Analysis</h2>
"""
            # Model statistics
            if 'model_statistics' in report.detailed_data:
                html += """
    <h3>Model Performance</h3>
    <table>
        <tr>
            <th>Model</th>
            <th>Total</th>
            <th>Passed</th>
            <th>Failed</th>
            <th>Pass Rate</th>
            <th>Avg Time (ms)</th>
            <th>Errors</th>
        </tr>
"""
                for model_id, stats in report.detailed_data['model_statistics'].items():
                    html += f"""
        <tr>
            <td>{model_id}</td>
            <td>{stats.get('total', 0)}</td>
            <td>{stats.get('passed', 0)}</td>
            <td>{stats.get('total', 0) - stats.get('passed', 0)}</td>
            <td>{stats.get('pass_rate', 0)*100:.1f}%</td>
            <td>{stats.get('avg_execution_time', 0):.2f}</td>
            <td>{stats.get('error_count', 0)}</td>
        </tr>
"""
                html += "</table>"
        
        # Tables
        for table in report.tables:
            html += f"""
    <h3>{table.get('title', 'Table')}</h3>
    <table>
        <tr>
"""
            for header in table.get('headers', []):
                html += f"            <th>{header}</th>\n"
            html += "        </tr>\n"
            
            for row in table.get('rows', []):
                html += "        <tr>\n"
                for cell in row:
                    html += f"            <td>{cell}</td>\n"
                html += "        </tr>\n"
            
            html += "    </table>\n"
        
        # Charts
        for name, chart_data in report.charts.items():
            if 'image' in chart_data:
                html += f"""
    <div class="chart">
        <h3>{name.replace('_', ' ').title()}</h3>
        <img src="data:image/png;base64,{chart_data['image']}" alt="{name}">
    </div>
"""
        
        # Footer
        html += f"""
    <div class="footer">
        <p>Generated by AI_ModelEval Report Generator</p>
        <p>Report ID: {report.report_id}</p>
    </div>
</body>
</html>
"""
        
        return html


class MarkdownExporter(BaseExporter):
    """Export report as Markdown"""
    
    def export(self, report: Report, file_path: str) -> bool:
        try:
            md = self._generate_markdown(report)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(md)
            
            logger.info(f"Exported Markdown report to {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to export Markdown: {e}")
            return False
    
    def _generate_markdown(self, report: Report) -> str:
        """Generate Markdown content"""
        md = f"""# Evaluation Report: {report.report_id}

**Generated:** {report.generated_at.strftime('%Y-%m-%d %H:%M:%S')}  
**Evaluation ID:** {report.evaluation_id}  
**Report Type:** {report.report_type.value}

"""
        
        # Summary
        if report.summary_data:
            md += f"""## Summary

- **Total Results:** {report.summary_data.get('total_results', 0)}
- **Passed:** {report.summary_data.get('passed_results', 0)}
- **Failed:** {report.summary_data.get('total_results', 0) - report.summary_data.get('passed_results', 0)}
- **Pass Rate:** {report.summary_data.get('pass_rate', 0)*100:.1f}%

"""
        
        # Tables
        for table in report.tables:
            md += f"## {table.get('title', 'Table')}\n\n"
            
            # Headers
            md += "| " + " | ".join(table.get('headers', [])) + " |\n"
            md += "|" + "|".join([" --- " for _ in table.get('headers', [])]) + "|\n"
            
            # Rows
            for row in table.get('rows', []):
                md += "| " + " | ".join(str(cell) for cell in row) + " |\n"
            
            md += "\n"
        
        return md

--------------------------------------------------------------------------------
FILE 51/71: src\generators\report_generator.py
--------------------------------------------------------------------------------

from typing import List, Dict, Any, Optional, Union
import json
import csv
import os
from datetime import datetime
import logging
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

# Change from relative to absolute imports
from src.entities import Report, Evaluation, EvaluationResult, Benchmark
from .exporters import CSVExporter, JSONExporter, PDFExporter, HTMLExporter, MarkdownExporter

logger = logging.getLogger(__name__)


class ReportGenerator:
    """Generates reports from evaluation results"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.generator_id = self._generate_id()
        self.output_dir = Path(config.get('output_dir', 'reports'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize exporters
        self.exporters = {
            'csv': CSVExporter(),
            'json': JSONExporter(),
            'pdf': PDFExporter(),
            'html': HTMLExporter(),
            'md': MarkdownExporter()
        }
        
        # Templates directory
        self.templates_dir = Path(__file__).parent / 'templates'
        
        logger.info(f"ReportGenerator initialized with output dir: {self.output_dir}")

    def _generate_id(self) -> str:
        """Generate a unique generator ID"""
        import secrets
        return f"gen_{secrets.token_hex(8)}"

    def generate_summary_report(
        self,
        evaluation: Evaluation,
        results: List[EvaluationResult],
        format: str = 'html'
    ) -> Report:
        """Generate a summary report"""
        report = Report(
            evaluation_id=evaluation.evaluation_id,
            report_type='summary',
            report_format=format
        )
        
        # Prepare summary data
        summary_data = self._prepare_summary_data(evaluation, results)
        report.add_summary(summary_data)
        
        # Generate charts
        charts = self._generate_summary_charts(results)
        for name, chart in charts.items():
            report.add_chart(name, chart)
        
        # Generate tables
        tables = self._generate_summary_tables(results)
        for table in tables:
            report.add_table(table)
        
        # Export to file
        if format in self.exporters:
            exporter = self.exporters[format]
            file_path = self.output_dir / report.get_filename()
            exporter.export(report, str(file_path))
            report.set_file_path(str(file_path))
        
        return report

    def generate_detailed_report(
        self,
        evaluation: Evaluation,
        results: List[EvaluationResult],
        format: str = 'html'
    ) -> Report:
        """Generate a detailed report"""
        report = Report(
            evaluation_id=evaluation.evaluation_id,
            report_type='detailed',
            report_format=format
        )
        
        # Prepare detailed data
        detailed_data = self._prepare_detailed_data(evaluation, results)
        report.add_detailed_data(detailed_data)
        
        # Generate detailed charts
        charts = self._generate_detailed_charts(results)
        for name, chart in charts.items():
            report.add_chart(name, chart)
        
        # Generate detailed tables
        tables = self._generate_detailed_tables(results)
        for table in tables:
            report.add_table(table)
        
        # Export to file
        if format in self.exporters:
            exporter = self.exporters[format]
            file_path = self.output_dir / report.get_filename()
            exporter.export(report, str(file_path))
            report.set_file_path(str(file_path))
        
        return report

    def generate_comparative_report(
        self,
        evaluations: List[Evaluation],
        results_dict: Dict[str, List[EvaluationResult]],
        benchmark: Optional[Benchmark] = None,
        format: str = 'html'
    ) -> Report:
        """Generate a comparative report across evaluations"""
        report = Report(
            evaluation_id='comparative',
            report_type='comparative',
            report_format=format
        )
        
        # Prepare comparative data
        comparative_data = self._prepare_comparative_data(evaluations, results_dict, benchmark)
        report.add_detailed_data(comparative_data)
        
        # Generate comparative charts
        charts = self._generate_comparative_charts(results_dict, benchmark)
        for name, chart in charts.items():
            report.add_chart(name, chart)
        
        # Generate comparative tables
        tables = self._generate_comparative_tables(results_dict, benchmark)
        for table in tables:
            report.add_table(table)
        
        # Export to file
        if format in self.exporters:
            exporter = self.exporters[format]
            file_path = self.output_dir / report.get_filename()
            exporter.export(report, str(file_path))
            report.set_file_path(str(file_path))
        
        return report

    def _prepare_summary_data(
        self,
        evaluation: Evaluation,
        results: List[EvaluationResult]
    ) -> Dict[str, Any]:
        """Prepare summary data"""
        total_results = len(results)
        passed_results = sum(1 for r in results if r.passed)
        
        # Aggregate metrics
        all_metrics = {}
        for result in results:
            for name, value in result.metrics.items():
                if name not in all_metrics:
                    all_metrics[name] = []
                all_metrics[name].append(value)
        
        avg_metrics = {}
        for name, values in all_metrics.items():
            avg_metrics[name] = sum(values) / len(values) if values else 0
        
        return {
            'evaluation_id': evaluation.evaluation_id,
            'created_at': evaluation.created_at.isoformat(),
            'duration': evaluation.get_duration(),
            'models': evaluation.model_ids,
            'total_results': total_results,
            'passed_results': passed_results,
            'pass_rate': passed_results / total_results if total_results > 0 else 0,
            'average_metrics': avg_metrics,
            'status': evaluation.status.value
        }

    def _prepare_detailed_data(
        self,
        evaluation: Evaluation,
        results: List[EvaluationResult]
    ) -> Dict[str, Any]:
        """Prepare detailed data"""
        summary = self._prepare_summary_data(evaluation, results)
        
        # Group by model
        model_results = {}
        for result in results:
            if result.model_id not in model_results:
                model_results[result.model_id] = []
            model_results[result.model_id].append(result)
        
        # Per-model statistics
        model_stats = {}
        for model_id, model_results_list in model_results.items():
            model_passed = sum(1 for r in model_results_list if r.passed)
            model_stats[model_id] = {
                'total': len(model_results_list),
                'passed': model_passed,
                'pass_rate': model_passed / len(model_results_list) if model_results_list else 0,
                'avg_execution_time': np.mean([r.execution_time_ms for r in model_results_list if r.execution_time_ms]) if model_results_list else 0,
                'error_count': sum(len(r.errors) for r in model_results_list)
            }
        
        # Per-problem statistics
        problem_stats = {}
        for result in results:
            if result.problem_id not in problem_stats:
                problem_stats[result.problem_id] = {
                    'total_samples': 0,
                    'passed_samples': 0,
                    'by_model': {}
                }
            
            stats = problem_stats[result.problem_id]
            stats['total_samples'] += 1
            if result.passed:
                stats['passed_samples'] += 1
            
            if result.model_id not in stats['by_model']:
                stats['by_model'][result.model_id] = {'total': 0, 'passed': 0}
            
            stats['by_model'][result.model_id]['total'] += 1
            if result.passed:
                stats['by_model'][result.model_id]['passed'] += 1
        
        return {
            'summary': summary,
            'model_statistics': model_stats,
            'problem_statistics': problem_stats,
            'detailed_results': [r.to_dict() for r in results[:100]]  # Limit for performance
        }

    def _prepare_comparative_data(
        self,
        evaluations: List[Evaluation],
        results_dict: Dict[str, List[EvaluationResult]],
        benchmark: Optional[Benchmark]
    ) -> Dict[str, Any]:
        """Prepare comparative data"""
        from collections import defaultdict
        
        comparative_data = {
            'evaluations': [],
            'benchmark': benchmark.to_dict() if benchmark else None,
            'model_comparison': {}
        }
        
        # Collect all models
        all_models = set()
        for eval_id, results in results_dict.items():
            for result in results:
                all_models.add(result.model_id)
        
        # Initialize comparison structure
        for model_id in all_models:
            comparative_data['model_comparison'][model_id] = {
                'evaluations': {},
                'aggregate': {
                    'total_results': 0,
                    'passed_results': 0,
                    'avg_execution_time': 0,
                    'metrics': {}
                }
            }
        
        # Aggregate by evaluation
        for eval_id, results in results_dict.items():
            evaluation = next((e for e in evaluations if e.evaluation_id == eval_id), None)
            if evaluation:
                comparative_data['evaluations'].append({
                    'evaluation_id': eval_id,
                    'created_at': evaluation.created_at.isoformat(),
                    'config': evaluation.config
                })
            
            # Group by model for this evaluation
            for result in results:
                model_data = comparative_data['model_comparison'][result.model_id]
                
                # Per-evaluation data
                if eval_id not in model_data['evaluations']:
                    model_data['evaluations'][eval_id] = {
                        'total': 0,
                        'passed': 0,
                        'execution_times': [],
                        'metrics': {}
                    }
                
                eval_stats = model_data['evaluations'][eval_id]
                eval_stats['total'] += 1
                if result.passed:
                    eval_stats['passed'] += 1
                
                if result.execution_time_ms:
                    eval_stats['execution_times'].append(result.execution_time_ms)
                
                # Aggregate metrics
                for name, value in result.metrics.items():
                    if name not in eval_stats['metrics']:
                        eval_stats['metrics'][name] = []
                    eval_stats['metrics'][name].append(value)
        
        # Calculate aggregates
        for model_id, model_data in comparative_data['model_comparison'].items():
            total_results = 0
            passed_results = 0
            all_exec_times = []
            all_metrics = defaultdict(list)
            
            for eval_stats in model_data['evaluations'].values():
                total_results += eval_stats['total']
                passed_results += eval_stats['passed']
                all_exec_times.extend(eval_stats['execution_times'])
                
                for name, values in eval_stats['metrics'].items():
                    all_metrics[name].extend(values)
                
                # Calculate per-evaluation averages
                if eval_stats['total'] > 0:
                    eval_stats['pass_rate'] = eval_stats['passed'] / eval_stats['total']
                    if eval_stats['execution_times']:
                        eval_stats['avg_execution_time'] = np.mean(eval_stats['execution_times'])
                    
                    for name, values in eval_stats['metrics'].items():
                        if values:
                            eval_stats['metrics'][name] = np.mean(values)
            
            # Calculate aggregate
            if total_results > 0:
                model_data['aggregate']['total_results'] = total_results
                model_data['aggregate']['passed_results'] = passed_results
                model_data['aggregate']['pass_rate'] = passed_results / total_results
                if all_exec_times:
                    model_data['aggregate']['avg_execution_time'] = np.mean(all_exec_times)
                
                for name, values in all_metrics.items():
                    if values:
                        model_data['aggregate']['metrics'][name] = np.mean(values)
        
        return comparative_data

    def _generate_summary_charts(
        self,
        results: List[EvaluationResult]
    ) -> Dict[str, Any]:
        """Generate summary charts"""
        charts = {}
        
        # Pass rate pie chart
        passed = sum(1 for r in results if r.passed)
        failed = len(results) - passed
        
        if results:
            fig, ax = plt.subplots()
            ax.pie([passed, failed], labels=['Passed', 'Failed'], autopct='%1.1f%%')
            ax.set_title('Overall Pass Rate')
            
            charts['pass_rate_pie'] = self._fig_to_dict(fig)
            plt.close(fig)
        
        # Model performance bar chart
        from collections import defaultdict
        model_results = defaultdict(list)
        for result in results:
            model_results[result.model_id].append(result)
        
        if model_results:
            models = []
            pass_rates = []
            for model_id, model_results_list in model_results.items():
                models.append(model_id)
                model_passed = sum(1 for r in model_results_list if r.passed)
                pass_rates.append(model_passed / len(model_results_list) * 100 if model_results_list else 0)
            
            fig, ax = plt.subplots()
            ax.bar(models, pass_rates)
            ax.set_xlabel('Model')
            ax.set_ylabel('Pass Rate (%)')
            ax.set_title('Model Performance Comparison')
            ax.tick_params(axis='x', rotation=45)
            
            charts['model_performance'] = self._fig_to_dict(fig)
            plt.close(fig)
        
        return charts

    def _generate_detailed_charts(
        self,
        results: List[EvaluationResult]
    ) -> Dict[str, Any]:
        """Generate detailed charts"""
        charts = self._generate_summary_charts(results)
        
        # Execution time histogram
        exec_times = [r.execution_time_ms for r in results if r.execution_time_ms]
        if exec_times:
            fig, ax = plt.subplots()
            ax.hist(exec_times, bins=20)
            ax.set_xlabel('Execution Time (ms)')
            ax.set_ylabel('Frequency')
            ax.set_title('Execution Time Distribution')
            
            charts['execution_time_hist'] = self._fig_to_dict(fig)
            plt.close(fig)
        
        # Error types pie chart
        from collections import defaultdict
        error_types = defaultdict(int)
        for result in results:
            for error in result.errors:
                error_types[error.get('error_type', 'unknown')] += 1
        
        if error_types:
            fig, ax = plt.subplots()
            ax.pie(error_types.values(), labels=error_types.keys(), autopct='%1.1f%%')
            ax.set_title('Error Types Distribution')
            
            charts['error_types'] = self._fig_to_dict(fig)
            plt.close(fig)
        
        return charts

    def _generate_comparative_charts(
        self,
        results_dict: Dict[str, List[EvaluationResult]],
        benchmark: Optional[Benchmark]
    ) -> Dict[str, Any]:
        """Generate comparative charts"""
        charts = {}
        
        # Model ranking chart
        if benchmark and benchmark.rankings:
            models = []
            scores = []
            for model_id, rank in sorted(benchmark.rankings.items(), key=lambda x: x[1]):
                model_info = next((m for m in benchmark.models if m['model_id'] == model_id), {})
                models.append(model_info.get('model_name', model_id))
                scores.append(benchmark.scores.get(model_id, 0))
            
            fig, ax = plt.subplots()
            ax.barh(models, scores)
            ax.set_xlabel('Score')
            ax.set_title('Model Rankings')
            
            charts['model_rankings'] = self._fig_to_dict(fig)
            plt.close(fig)
        
        # Performance over time
        eval_ids = list(results_dict.keys())
        if len(eval_ids) > 1:
            pass_rates = []
            for eval_id in eval_ids:
                results = results_dict[eval_id]
                passed = sum(1 for r in results if r.passed)
                pass_rates.append(passed / len(results) * 100 if results else 0)
            
            fig, ax = plt.subplots()
            ax.plot(range(len(eval_ids)), pass_rates, marker='o')
            ax.set_xlabel('Evaluation')
            ax.set_ylabel('Pass Rate (%)')
            ax.set_title('Performance Trend')
            ax.set_xticks(range(len(eval_ids)))
            ax.set_xticklabels([f'E{i+1}' for i in range(len(eval_ids))])
            
            charts['performance_trend'] = self._fig_to_dict(fig)
            plt.close(fig)
        
        return charts

    def _generate_summary_tables(
        self,
        results: List[EvaluationResult]
    ) -> List[Dict[str, Any]]:
        """Generate summary tables"""
        tables = []
        
        # Model summary table
        from collections import defaultdict
        model_results = defaultdict(list)
        for result in results:
            model_results[result.model_id].append(result)
        
        model_table = {
            'title': 'Model Performance Summary',
            'headers': ['Model', 'Total Samples', 'Passed', 'Failed', 'Pass Rate', 'Avg Time (ms)'],
            'rows': []
        }
        
        for model_id, model_results_list in model_results.items():
            total = len(model_results_list)
            passed = sum(1 for r in model_results_list if r.passed)
            failed = total - passed
            pass_rate = passed / total * 100 if total > 0 else 0
            avg_time = np.mean([r.execution_time_ms for r in model_results_list if r.execution_time_ms]) if model_results_list else 0
            
            model_table['rows'].append([
                model_id,
                str(total),
                str(passed),
                str(failed),
                f"{pass_rate:.1f}%",
                f"{avg_time:.2f}" if avg_time else 'N/A'
            ])
        
        if model_table['rows']:
            tables.append(model_table)
        
        return tables

    def _generate_detailed_tables(
        self,
        results: List[EvaluationResult]
    ) -> List[Dict[str, Any]]:
        """Generate detailed tables"""
        tables = self._generate_summary_tables(results)
        
        # Problem-level table
        from collections import defaultdict
        problem_results = defaultdict(list)
        for result in results:
            problem_results[result.problem_id].append(result)
        
        problem_table = {
            'title': 'Problem-level Results',
            'headers': ['Problem ID', 'Total Samples', 'Passed', 'Failed', 'Pass Rate'],
            'rows': []
        }
        
        for problem_id, problem_results_list in problem_results.items():
            total = len(problem_results_list)
            passed = sum(1 for r in problem_results_list if r.passed)
            failed = total - passed
            pass_rate = passed / total * 100 if total > 0 else 0
            
            problem_table['rows'].append([
                problem_id,
                str(total),
                str(passed),
                str(failed),
                f"{pass_rate:.1f}%"
            ])
        
        if problem_table['rows']:
            tables.append(problem_table)
        
        return tables

    def _generate_comparative_tables(
        self,
        results_dict: Dict[str, List[EvaluationResult]],
        benchmark: Optional[Benchmark]
    ) -> List[Dict[str, Any]]:
        """Generate comparative tables"""
        tables = []
        
        # Benchmark ranking table
        if benchmark:
            ranking_table = {
                'title': 'Benchmark Rankings',
                'headers': ['Rank', 'Model', 'Score', 'Pass@1', 'Pass@5', 'CodeBLEU'],
                'rows': []
            }
            
            sorted_models = sorted(
                benchmark.rankings.items(),
                key=lambda x: x[1]
            )
            
            for model_id, rank in sorted_models:
                model_info = next((m for m in benchmark.models if m['model_id'] == model_id), {})
                model_name = model_info.get('model_name', model_id)
                score = benchmark.scores.get(model_id, 0)
                
                # Get metrics
                metrics = benchmark.results.get(model_id, {})
                pass1 = metrics.get('pass@1', 0) * 100
                pass5 = metrics.get('pass@5', 0) * 100
                codebleu = metrics.get('codebleu', 0) * 100
                
                ranking_table['rows'].append([
                    str(rank),
                    model_name,
                    f"{score:.3f}",
                    f"{pass1:.1f}%",
                    f"{pass5:.1f}%",
                    f"{codebleu:.1f}%"
                ])
            
            if ranking_table['rows']:
                tables.append(ranking_table)
        
        # Cross-evaluation comparison table
        if len(results_dict) > 1:
            comparison_table = {
                'title': 'Cross-Evaluation Comparison',
                'headers': ['Model'] + [f'Eval {i+1}' for i in range(len(results_dict))] + ['Average'],
                'rows': []
            }
            
            # Collect all models
            all_models = set()
            for results in results_dict.values():
                for result in results:
                    all_models.add(result.model_id)
            
            for model_id in sorted(all_models):
                row = [model_id]
                pass_rates = []
                
                for eval_id, results in results_dict.items():
                    model_results = [r for r in results if r.model_id == model_id]
                    if model_results:
                        passed = sum(1 for r in model_results if r.passed)
                        pass_rate = passed / len(model_results) * 100
                        pass_rates.append(pass_rate)
                        row.append(f"{pass_rate:.1f}%")
                    else:
                        row.append('N/A')
                
                if pass_rates:
                    avg_pass = np.mean(pass_rates)
                    row.append(f"{avg_pass:.1f}%")
                else:
                    row.append('N/A')
                
                comparison_table['rows'].append(row)
            
            if comparison_table['rows']:
                tables.append(comparison_table)
        
        return tables

    def _fig_to_dict(self, fig: plt.Figure) -> Dict[str, Any]:
        """Convert matplotlib figure to dict for serialization"""
        import io
        import base64
        
        buf = io.BytesIO()
        fig.savefig(buf, format='png', dpi=100, bbox_inches='tight')
        buf.seek(0)
        img_str = base64.b64encode(buf.read()).decode('utf-8')
        
        return {
            'image': img_str,
            'format': 'png'
        }

--------------------------------------------------------------------------------
FILE 52/71: src\loaders\__init__.py
--------------------------------------------------------------------------------

"""Loader classes for datasets."""

from .dataset_loader import DatasetLoader
from .humaneval_loader import HumanEvalLoader

__all__ = ['DatasetLoader', 'HumanEvalLoader']

--------------------------------------------------------------------------------
FILE 53/71: src\loaders\dataset_loader.py
--------------------------------------------------------------------------------


from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Iterator
import hashlib
import json
import logging
from datetime import datetime

from ..entities import Problem

logger = logging.getLogger(__name__)


class DatasetLoader(ABC):
    """Abstract base class for all dataset loaders"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.dataset_id = self._generate_dataset_id()
        self.name = config.get('name', 'unknown')
        self.description = config.get('description', '')
        self.file_path = config.get('file_path')
        self.cache_enabled = config.get('cache_enabled', True)
        self.cache: Dict[str, Any] = {}
        self.metadata: Dict[str, Any] = {}
        self.loaded_at: Optional[datetime] = None
        self.problems: List[Problem] = []
        
        logger.info(f"Initialized dataset loader: {self.name}")

    def _generate_dataset_id(self) -> str:
        """Generate a unique dataset ID"""
        import secrets
        return f"dataset_{secrets.token_hex(8)}"

    @abstractmethod
    def load_dataset(self) -> List[Problem]:
        """Load the dataset"""
        pass

    @abstractmethod
    def validate_dataset(self) -> bool:
        """Validate dataset structure"""
        pass

    def get_problem(self, problem_id: str) -> Optional[Problem]:
        """Get a specific problem by ID"""
        for problem in self.problems:
            if problem.problem_id == problem_id:
                return problem
        return None

    def get_problems(self, indices: Optional[List[int]] = None) -> List[Problem]:
        """Get problems by indices"""
        if indices is None:
            return self.problems
        
        return [self.problems[i] for i in indices if 0 <= i < len(self.problems)]

    def get_random_problems(self, count: int) -> List[Problem]:
        """Get random problems"""
        import random
        if count >= len(self.problems):
            return self.problems.copy()
        
        indices = random.sample(range(len(self.problems)), count)
        return [self.problems[i] for i in indices]

    def get_statistics(self) -> Dict[str, Any]:
        """Get dataset statistics"""
        if not self.problems:
            return {}
        
        stats = {
            'total_problems': len(self.problems),
            'languages': {},
            'difficulties': {},
            'categories': {},
            'avg_complexity': 0,
            'total_tests': 0
        }
        
        total_complexity = 0
        total_tests = 0
        
        for problem in self.problems:
            # Count languages
            lang = problem.metadata.get('language', 'python')
            stats['languages'][lang] = stats['languages'].get(lang, 0) + 1
            
            # Count difficulties
            diff = problem.difficulty
            stats['difficulties'][diff] = stats['difficulties'].get(diff, 0) + 1
            
            # Count categories
            for cat in problem.categories:
                stats['categories'][cat] = stats['categories'].get(cat, 0) + 1
            
            # Sum complexity
            if problem.stats:
                total_complexity += problem.stats.get('cyclomatic_complexity', 0)
            
            total_tests += len(problem.test_cases)
        
        stats['avg_complexity'] = total_complexity / len(self.problems) if self.problems else 0
        stats['total_tests'] = total_tests
        stats['avg_tests_per_problem'] = total_tests / len(self.problems) if self.problems else 0
        
        return stats

    def split_dataset(
        self,
        train_ratio: float = 0.7,
        val_ratio: float = 0.15,
        test_ratio: float = 0.15,
        seed: Optional[int] = None
    ) -> Dict[str, List[Problem]]:
        """Split dataset into train/val/test sets"""
        if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
            raise ValueError("Ratios must sum to 1.0")
        
        import random
        if seed is not None:
            random.seed(seed)
        
        problems = self.problems.copy()
        random.shuffle(problems)
        
        total = len(problems)
        train_end = int(total * train_ratio)
        val_end = train_end + int(total * val_ratio)
        
        return {
            'train': problems[:train_end],
            'validation': problems[train_end:val_end],
            'test': problems[val_end:]
        }

    def export_dataset(self, format: str = 'json') -> str:
        """Export dataset to specified format"""
        data = [p.to_dict() for p in self.problems]
        
        if format == 'json':
            return json.dumps(data, indent=2, default=str)
        elif format == 'jsonl':
            return '\n'.join(json.dumps(p, default=str) for p in data)
        else:
            raise ValueError(f"Unsupported export format: {format}")

    def get_metadata(self) -> Dict[str, Any]:
        """Get dataset metadata"""
        return {
            'dataset_id': self.dataset_id,
            'name': self.name,
            'description': self.description,
            'file_path': self.file_path,
            'loaded_at': self.loaded_at.isoformat() if self.loaded_at else None,
            'problem_count': len(self.problems),
            'metadata': self.metadata,
            'statistics': self.get_statistics()
        }

    def clear_cache(self) -> None:
        """Clear the cache"""
        self.cache.clear()
        logger.info("Cache cleared")

    def __len__(self) -> int:
        return len(self.problems)

    def __getitem__(self, idx: int) -> Problem:
        return self.problems[idx]

    def __iter__(self) -> Iterator[Problem]:
        return iter(self.problems)

--------------------------------------------------------------------------------
FILE 54/71: src\loaders\humaneval_loader.py
--------------------------------------------------------------------------------


from typing import List, Dict, Any, Optional
import os
import json
import gzip
from git import Repo
from tqdm import tqdm
import logging
from datetime import datetime  


from .dataset_loader import DatasetLoader
from ..entities import Problem

logger = logging.getLogger(__name__)


class HumanEvalLoader(DatasetLoader):
    """Loader for the HumanEval dataset"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.repo_url = config.get('repo_url', 'https://github.com/openai/human-eval.git')
        self.data_dir = config.get('data_dir', 'data/human_eval')
        self.dataset_path = os.path.join(
            self.data_dir,
            'data',
            'HumanEval.jsonl.gz'
        )
        self.metadata = {
            'source': 'OpenAI',
            'paper': 'Evaluating Large Language Models Trained on Code',
            'year': 2021,
            'tasks': 164,
            'languages': ['python']
        }

    def fetch_repo(self) -> bool:
        """Clone the repository if not exists"""
        if not os.path.exists(os.path.join(self.data_dir, '.git')):
            try:
                logger.info(f"Cloning HumanEval repository from {self.repo_url}")
                Repo.clone_from(self.repo_url, self.data_dir)
                return True
            except Exception as e:
                logger.error(f"Failed to clone repository: {e}")
                return False
        else:
            logger.info("Repository already exists")
            return True

    def load_dataset(self) -> List[Problem]:
        """Load the HumanEval dataset"""
        if not self.fetch_repo():
            raise RuntimeError("Failed to fetch repository")
        
        if not os.path.exists(self.dataset_path):
            raise FileNotFoundError(f"Dataset not found at {self.dataset_path}")
        
        logger.info(f"Loading HumanEval dataset from {self.dataset_path}")
        
        problems = []
        with gzip.open(self.dataset_path, 'rt', encoding='utf-8') as f:
            for line in tqdm(f, desc="Loading problems"):
                data = json.loads(line)
                
                # Create Problem entity
                problem = Problem(
                    task_id=data['task_id'],
                    prompt=data['prompt'],
                    dataset_id=self.dataset_id
                )
                
                # Set canonical solution
                problem.set_canonical_solution(data['canonical_solution'])
                
                # Set test cases
                test_code = data['test']
                problem.set_test_cases(test_code)
                
                # Set entry point
                problem.entry_point = data['entry_point']
                
                # Set metadata
                problem.metadata = {
                    'signature': data.get('signature', ''),
                    'docstring': data.get('docstring', ''),
                    'language': 'python',
                    'source': 'humaneval'
                }
                
                # Calculate initial stats
                problem.stats = problem.calculate_complexity()
                
                problems.append(problem)
        
        self.problems = problems
        self.loaded_at = datetime.now()
        
        logger.info(f"Loaded {len(problems)} problems from HumanEval")
        
        return problems

    def validate_dataset(self) -> bool:
        """Validate the dataset structure"""
        if not self.problems:
            return False
        
        required_fields = ['task_id', 'prompt', 'canonical_solution', 'test', 'entry_point']
        
        for problem in self.problems:
            # Check if we can access all required data
            if not problem.task_id:
                return False
            if not problem.prompt:
                return False
            if not problem.canonical_solution:
                return False
            if not problem.test_cases:
                return False
            if not problem.entry_point:
                return False
        
        logger.info("Dataset validation passed")
        return True

    def get_problem_by_name(self, function_name: str) -> Optional[Problem]:
        """Get a problem by function name"""
        for problem in self.problems:
            if problem.entry_point == function_name:
                return problem
        return None

    def get_problems_by_difficulty(self, difficulty: str) -> List[Problem]:
        """Get problems by difficulty level"""
        # HumanEval doesn't have built-in difficulty, so we estimate
        problems_with_complexity = []
        for problem in self.problems:
            if problem.stats:
                complexity = problem.stats.get('cyclomatic_complexity', 0)
                
                # Estimate difficulty based on complexity
                if difficulty == 'easy' and complexity <= 3:
                    problems_with_complexity.append(problem)
                elif difficulty == 'medium' and 3 < complexity <= 7:
                    problems_with_complexity.append(problem)
                elif difficulty == 'hard' and complexity > 7:
                    problems_with_complexity.append(problem)
        
        return problems_with_complexity

--------------------------------------------------------------------------------
FILE 55/71: src\managers\__init__.py
--------------------------------------------------------------------------------

"""Manager classes for AI Model Evaluation."""
from .evaluation_manager import EvaluationManager
from .result_aggregator import ResultAggregator

__all__ = ['EvaluationManager', 'ResultAggregator']

--------------------------------------------------------------------------------
FILE 56/71: src\managers\evaluation_manager.py
--------------------------------------------------------------------------------


from datetime import datetime
from typing import List, Dict, Any, Optional, Callable
import asyncio
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..entities import Evaluation, EvaluationResult, Problem, Metric, Error
from ..adapters import ModelRegistry
from ..loaders import DatasetLoader
from ..executors import SandboxExecutor
from ..calculators import MetricCalculator
from ..analyzers import ErrorAnalyzer
from ..utils.debug_timer import DebugTimer

logger = logging.getLogger(__name__)


class EvaluationManager:
    def __init__(
        self,
        model_registry: ModelRegistry,
        dataset_loader: DatasetLoader,
        sandbox_executor: SandboxExecutor,
        metric_calculator: MetricCalculator,
        error_analyzer: ErrorAnalyzer,
        max_workers: int = 4
    ):
        self.model_registry = model_registry
        self.dataset_loader = dataset_loader
        self.sandbox = sandbox_executor
        self.metric_calculator = metric_calculator
        self.error_analyzer = error_analyzer
        self.max_workers = max_workers
        
        self.evaluations: Dict[str, Evaluation] = {}
        self.results: Dict[str, EvaluationResult] = {}
        self.progress_callbacks: List[Callable] = []
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
        logger.info("EvaluationManager initialized with %d workers", max_workers)

    def create_evaluation(
        self,
        user_id: str,
        model_ids: List[str],
        dataset_id: str,
        config: Dict[str, Any]
    ) -> Evaluation:
        """Create a new evaluation"""
        evaluation = Evaluation(user_id=user_id, config=config)
        
        for model_id in model_ids:
            evaluation.add_model(model_id)
        
        evaluation.set_dataset(dataset_id)
        
        self.evaluations[evaluation.evaluation_id] = evaluation
        logger.info(f"Created evaluation {evaluation.evaluation_id}")
        
        return evaluation

    async def run_evaluation(
        self,
        evaluation_id: str,
        progress_callback: Optional[Callable] = None
    ) -> Evaluation:
        """Run an evaluation asynchronously"""
        evaluation = self.evaluations.get(evaluation_id)
        if not evaluation:
            raise ValueError(f"Evaluation {evaluation_id} not found")
        
        if progress_callback:
            self.progress_callbacks.append(progress_callback)
        
        try:
            # Start evaluation
            if not evaluation.start():
                raise RuntimeError("Failed to start evaluation")
            
            self._notify_progress(evaluation, "starting", "Starting evaluation")
            
            # Load dataset
            with DebugTimer("Loading dataset"):
                problems = await self._load_dataset(evaluation.dataset_id)
                evaluation.update_progress(10, "dataset_loaded")
                self._notify_progress(evaluation, "dataset_loaded", 
                                    f"Loaded {len(problems)} problems")
            
            # Run evaluations for each model
            all_results = []
            for model_id in evaluation.model_ids:
                with DebugTimer(f"Evaluating model {model_id}"):
                    model_results = await self._evaluate_model(
                        model_id, problems, evaluation
                    )
                    all_results.extend(model_results)
            
            evaluation.update_progress(70, "evaluation_complete")
            self._notify_progress(evaluation, "evaluation_complete",
                                f"Completed {len(all_results)} evaluations")
            
            # Calculate metrics
            with DebugTimer("Calculating metrics"):
                metrics = await self._calculate_metrics(all_results)
                evaluation.update_progress(85, "metrics_calculated")
                self._notify_progress(evaluation, "metrics_calculated", 
                                    f"Calculated {len(metrics)} metrics")
            
            # Analyze errors
            with DebugTimer("Analyzing errors"):
                error_analysis = await self._analyze_errors(all_results)
                evaluation.update_progress(95, "errors_analyzed")
                self._notify_progress(evaluation, "errors_analyzed",
                                    f"Found {error_analysis['total_errors']} errors")
            
            # Store results
            for result in all_results:
                self.results[result.result_id] = result
                evaluation.add_result(result.result_id)
            
            # Complete evaluation
            evaluation.complete()
            evaluation.metadata['metrics'] = metrics
            evaluation.metadata['error_analysis'] = error_analysis
            
            self._notify_progress(evaluation, "completed", 
                                "Evaluation completed successfully")
            
            return evaluation
            
        except Exception as e:
            logger.error(f"Evaluation failed: {e}")
            evaluation.fail(str(e))
            self._notify_progress(evaluation, "failed", f"Evaluation failed: {e}")
            raise

    async def _load_dataset(self, dataset_id: str) -> List[Problem]:
        """Load dataset asynchronously"""
        return await asyncio.get_event_loop().run_in_executor(
            self.executor,
            self.dataset_loader.load_dataset,
            dataset_id
        )

    async def _evaluate_model(
        self,
        model_id: str,
        problems: List[Problem],
        evaluation: Evaluation
    ) -> List[EvaluationResult]:
        """Evaluate a single model on all problems"""
        model = self.model_registry.get_model(model_id)
        if not model:
            raise ValueError(f"Model {model_id} not found")
        
        results = []
        total = len(problems)
        
        for i, problem in enumerate(problems):
            # Generate multiple samples
            for sample_idx in range(evaluation.config.get('num_samples', 5)):
                result = EvaluationResult(
                    evaluation_id=evaluation.evaluation_id,
                    problem_id=problem.problem_id,
                    model_id=model_id,
                    sample_id=sample_idx
                )
                
                try:
                    # Generate code
                    prompt = problem.get_prompt()
                    generated_code = await model.generate_code(
                        prompt,
                        evaluation.config.get('generation_config', {})
                    )
                    result.set_generated_code(generated_code)
                    
                    # Execute code
                    execution_result = await self._execute_code(
                        generated_code,
                        problem
                    )
                    
                    result.set_execution_result(
                        passed=execution_result['passed'],
                        output=execution_result['output'],
                        execution_time_ms=execution_result['execution_time_ms'],
                        memory_usage_kb=execution_result.get('memory_usage_kb')
                    )
                    
                    # Add test results
                    for test in execution_result.get('test_results', []):
                        result.add_test_result(
                            test_id=test['test_id'],
                            passed=test['passed'],
                            message=test['message'],
                            test_case=test.get('test_case')
                        )
                    
                    # Add errors
                    for error in execution_result.get('errors', []):
                        result.add_error(
                            error_type=error['error_type'],
                            error_message=error['error_message'],
                            severity=error.get('severity', 'error')
                        )
                    
                    # Calculate metrics
                    metrics = self.metric_calculator.calculate_for_result(result)
                    for name, value in metrics.items():
                        result.add_metric(name, value)
                    
                except Exception as e:
                    logger.error(f"Error evaluating {problem.problem_id}: {e}")
                    result.add_error('evaluation_error', str(e))
                
                results.append(result)
            
            # Update progress
            progress = 10 + (i + 1) / total * 60
            evaluation.update_progress(progress, f"evaluating_{model_id}")
        
        return results

    async def _execute_code(
        self,
        code: str,
        problem: Problem
    ) -> Dict[str, Any]:
        """Execute code in sandbox"""
        return await asyncio.get_event_loop().run_in_executor(
            self.executor,
            self.sandbox.execute_safely,
            code,
            problem.test_cases,
            problem.problem_id
        )

    async def _calculate_metrics(
        self,
        results: List[EvaluationResult]
    ) -> Dict[str, Any]:
        """Calculate aggregate metrics"""
        return await asyncio.get_event_loop().run_in_executor(
            self.executor,
            self.metric_calculator.calculate_aggregate_metrics,
            results
        )

    async def _analyze_errors(
        self,
        results: List[EvaluationResult]
    ) -> Dict[str, Any]:
        """Analyze errors across results"""
        all_errors = []
        for result in results:
            all_errors.extend(result.errors)
        
        return await asyncio.get_event_loop().run_in_executor(
            self.executor,
            self.error_analyzer.analyze_errors,
            all_errors
        )

    def _notify_progress(
        self,
        evaluation: Evaluation,
        stage: str,
        message: str,
        data: Optional[Dict] = None
    ) -> None:
        """Notify progress callbacks"""
        for callback in self.progress_callbacks:
            try:
                callback(
                    evaluation.evaluation_id,
                    stage,
                    message,
                    {
                        'progress': evaluation.progress,
                        'current_stage': evaluation.current_stage,
                        'data': data or {}
                    }
                )
            except Exception as e:
                logger.error(f"Progress callback failed: {e}")

    def get_evaluation(self, evaluation_id: str) -> Optional[Evaluation]:
        """Get evaluation by ID"""
        return self.evaluations.get(evaluation_id)

    def get_results(self, evaluation_id: str) -> List[EvaluationResult]:
        """Get all results for an evaluation"""
        evaluation = self.evaluations.get(evaluation_id)
        if not evaluation:
            return []
        
        return [
            self.results[result_id]
            for result_id in evaluation.results_ids
            if result_id in self.results
        ]

    def cancel_evaluation(self, evaluation_id: str) -> bool:
        """Cancel an evaluation"""
        evaluation = self.evaluations.get(evaluation_id)
        if not evaluation:
            return False
        
        return evaluation.cancel()

    def pause_evaluation(self, evaluation_id: str) -> bool:
        """Pause an evaluation"""
        evaluation = self.evaluations.get(evaluation_id)
        if not evaluation:
            return False
        
        return evaluation.pause()

    def resume_evaluation(self, evaluation_id: str) -> bool:
        """Resume a paused evaluation"""
        evaluation = self.evaluations.get(evaluation_id)
        if not evaluation:
            return False
        
        return evaluation.resume()

--------------------------------------------------------------------------------
FILE 57/71: src\managers\result_aggregator.py
--------------------------------------------------------------------------------


from typing import List, Dict, Any, Optional, Tuple
import numpy as np
import pandas as pd
from collections import defaultdict
from datetime import datetime, timedelta

from ..entities import EvaluationResult, Metric, Error, Benchmark


class ResultAggregator:
    def __init__(self):
        self.results: Dict[str, List[EvaluationResult]] = defaultdict(list)
        self.benchmarks: Dict[str, Benchmark] = {}

    def add_results(self, evaluation_id: str, results: List[EvaluationResult]) -> None:
        """Add results for aggregation"""
        self.results[evaluation_id].extend(results)

    def aggregate_by_model(
        self,
        evaluation_id: str
    ) -> Dict[str, Dict[str, Any]]:
        """Aggregate results by model"""
        results = self.results.get(evaluation_id, [])
        if not results:
            return {}
        
        model_results = defaultdict(list)
        for result in results:
            model_results[result.model_id].append(result)
        
        aggregation = {}
        for model_id, model_results_list in model_results.items():
            aggregation[model_id] = self._aggregate_model_results(model_results_list)
        
        return aggregation

    def _aggregate_model_results(
        self,
        results: List[EvaluationResult]
    ) -> Dict[str, Any]:
        """Aggregate results for a single model"""
        if not results:
            return {}
        
        # Calculate pass rate
        passed = sum(1 for r in results if r.passed)
        total = len(results)
        
        # Collect metrics
        metrics = defaultdict(list)
        for result in results:
            for name, value in result.metrics.items():
                metrics[name].append(value)
        
        # Calculate statistics
        stats = {
            'total_evaluations': total,
            'passed': passed,
            'failed': total - passed,
            'pass_rate': passed / total if total > 0 else 0,
            'metrics': {}
        }
        
        for name, values in metrics.items():
            if values:
                stats['metrics'][name] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'median': np.median(values),
                    'count': len(values)
                }
        
        # Error analysis
        all_errors = []
        for result in results:
            all_errors.extend(result.errors)
        
        stats['errors'] = self._aggregate_errors(all_errors)
        
        return stats

    def _aggregate_errors(self, errors: List[Dict]) -> Dict[str, Any]:
        """Aggregate error statistics"""
        if not errors:
            return {'total': 0}
        
        error_types = defaultdict(int)
        severities = defaultdict(int)
        
        for error in errors:
            error_types[error.get('error_type', 'unknown')] += 1
            severities[error.get('severity', 'error')] += 1
        
        return {
            'total': len(errors),
            'by_type': dict(error_types),
            'by_severity': dict(severities),
            'unique_patterns': len(set(e.get('pattern_id') for e in errors if e.get('pattern_id')))
        }

    def compare_models(
        self,
        evaluation_id: str,
        model_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Compare multiple models"""
        aggregation = self.aggregate_by_model(evaluation_id)
        
        if model_ids:
            aggregation = {k: v for k, v in aggregation.items() if k in model_ids}
        
        if not aggregation:
            return {}
        
        # Create comparison
        comparison = {
            'models': list(aggregation.keys()),
            'pass_rates': {},
            'metrics_comparison': {},
            'ranking': []
        }
        
        # Collect pass rates
        for model_id, stats in aggregation.items():
            comparison['pass_rates'][model_id] = stats['pass_rate']
        
        # Compare each metric
        all_metrics = set()
        for stats in aggregation.values():
            all_metrics.update(stats.get('metrics', {}).keys())
        
        for metric in all_metrics:
            comparison['metrics_comparison'][metric] = {}
            for model_id, stats in aggregation.items():
                if metric in stats.get('metrics', {}):
                    comparison['metrics_comparison'][metric][model_id] = \
                        stats['metrics'][metric]['mean']
        
        # Rank models by pass rate
        comparison['ranking'] = sorted(
            comparison['pass_rates'].items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        return comparison

    def create_benchmark(
        self,
        name: str,
        description: str,
        evaluation_ids: List[str],
        metric_weights: Optional[Dict[str, float]] = None
    ) -> Benchmark:
        """Create a benchmark from multiple evaluations"""
        benchmark = Benchmark(name=name, description=description)
        
        # Collect all results
        all_results = []
        for eval_id in evaluation_ids:
            all_results.extend(self.results.get(eval_id, []))
        
        if not all_results:
            return benchmark
        
        # Group by model
        model_results = defaultdict(list)
        for result in all_results:
            model_results[result.model_id].append(result)
        
        # Add models to benchmark
        for model_id, results in model_results.items():
            # Get model name from first result's metadata
            model_name = results[0].metadata.get('model_name', model_id)
            benchmark.add_model(model_id, model_name)
        
        # Add metrics
        if metric_weights:
            for metric, weight in metric_weights.items():
                benchmark.add_metric(metric, weight)
        else:
            # Default metrics
            benchmark.add_metric('pass_rate', weight=0.4)
            benchmark.add_metric('execution_time', weight=0.2)
            benchmark.add_metric('code_quality', weight=0.2)
            benchmark.add_metric('error_count', weight=0.2)
        
        # Add results
        for model_id, results in model_results.items():
            # Calculate aggregate metrics
            stats = self._aggregate_model_results(results)
            
            for metric in benchmark.metrics:
                if metric == 'pass_rate':
                    benchmark.add_result(model_id, metric, stats['pass_rate'])
                elif metric == 'execution_time':
                    # Average execution time (normalized)
                    times = [r.execution_time_ms for r in results if r.execution_time_ms]
                    avg_time = np.mean(times) if times else float('inf')
                    benchmark.add_result(model_id, metric, avg_time)
                elif metric == 'error_count':
                    benchmark.add_result(model_id, metric, stats['errors']['total'])
                elif metric == 'code_quality':
                    # Composite quality score
                    quality_scores = []
                    for result in results:
                        if 'maintainability_index' in result.metrics:
                            quality_scores.append(result.metrics['maintainability_index'])
                    avg_quality = np.mean(quality_scores) if quality_scores else 0
                    benchmark.add_result(model_id, metric, avg_quality)
        
        # Calculate rankings
        benchmark.calculate_scores()
        benchmark.calculate_rankings()
        
        # Store benchmark
        self.benchmarks[benchmark.benchmark_id] = benchmark
        
        return benchmark

    def get_performance_trends(
        self,
        evaluation_ids: List[str],
        model_id: Optional[str] = None,
        time_window: Optional[timedelta] = None
    ) -> Dict[str, Any]:
        """Analyze performance trends over time"""
        trends = {
            'pass_rate': [],
            'execution_time': [],
            'error_rate': [],
            'timestamps': []
        }
        
        for eval_id in evaluation_ids:
            results = self.results.get(eval_id, [])
            
            if model_id:
                results = [r for r in results if r.model_id == model_id]
            
            if not results:
                continue
            
            # Get evaluation time from first result
            eval_time = min(r.created_at for r in results)
            
            if time_window and datetime.now() - eval_time > time_window:
                continue
            
            # Calculate metrics for this evaluation
            passed = sum(1 for r in results if r.passed)
            total = len(results)
            
            trends['timestamps'].append(eval_time)
            trends['pass_rate'].append(passed / total if total > 0 else 0)
            
            # Average execution time
            times = [r.execution_time_ms for r in results if r.execution_time_ms]
            trends['execution_time'].append(np.mean(times) if times else 0)
            
            # Error rate
            errors = sum(len(r.errors) for r in results)
            trends['error_rate'].append(errors / total if total > 0 else 0)
        
        return trends

    def export_to_dataframe(self, evaluation_id: str) -> pd.DataFrame:
        """Export results to pandas DataFrame"""
        results = self.results.get(evaluation_id, [])
        
        data = []
        for result in results:
            row = {
                'result_id': result.result_id,
                'model_id': result.model_id,
                'problem_id': result.problem_id,
                'sample_id': result.sample_id,
                'passed': result.passed,
                'execution_time_ms': result.execution_time_ms,
                'memory_usage_kb': result.memory_usage_kb,
                'created_at': result.created_at,
                'error_count': len(result.errors)
            }
            
            # Add metrics
            for name, value in result.metrics.items():
                row[f'metric_{name}'] = value
            
            data.append(row)
        
        return pd.DataFrame(data)

--------------------------------------------------------------------------------
FILE 58/71: src\prompts\__init__.py
--------------------------------------------------------------------------------

"""Prompt strategy classes."""

from .prompt_engine import PromptEngine
from .strategies import ZeroShotStrategy, FewShotStrategy, ChainOfThoughtStrategy

__all__ = [
    'PromptEngine',
    'ZeroShotStrategy',
    'FewShotStrategy',
    'ChainOfThoughtStrategy'
]

--------------------------------------------------------------------------------
FILE 59/71: src\prompts\prompt_engine.py
--------------------------------------------------------------------------------

from typing import Dict, Any, Optional, List
import logging
from pathlib import Path
import json

from .strategies import PromptStrategy, ZeroShotStrategy, FewShotStrategy, ChainOfThoughtStrategy

logger = logging.getLogger(__name__)


class PromptEngine:
    """Engine for managing prompt strategies"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.strategies: Dict[str, PromptStrategy] = {}
        self.default_strategy = config.get('default_strategy', 'zero_shot')
        
        # Load examples first
        self.examples = self._load_examples()
        
        # Initialize default strategies after examples are loaded
        self._register_default_strategies()
        
        logger.info(f"PromptEngine initialized with {len(self.strategies)} strategies")

    def _register_default_strategies(self):
        """Register default prompt strategies"""
        self.register_strategy('zero_shot', ZeroShotStrategy())
        self.register_strategy('few_shot', FewShotStrategy(self.examples))
        self.register_strategy('chain_of_thought', ChainOfThoughtStrategy())

    def register_strategy(self, name: str, strategy: PromptStrategy) -> None:
        """Register a new prompt strategy"""
        self.strategies[name] = strategy
        logger.debug(f"Registered strategy: {name}")

    def format_prompt(
        self,
        problem: Dict[str, Any],
        strategy: str = 'zero_shot',
        **kwargs
    ) -> str:
        """Format prompt using specified strategy"""
        if strategy not in self.strategies:
            logger.warning(f"Strategy {strategy} not found, using {self.default_strategy}")
            strategy = self.default_strategy
        
        prompt_strategy = self.strategies[strategy]
        return prompt_strategy.format(problem, **kwargs)

    def get_available_strategies(self) -> List[str]:
        """Get list of available strategies"""
        return list(self.strategies.keys())

    def _load_examples(self) -> List[Dict[str, str]]:
        """Load examples for few-shot learning"""
        examples = []
        
        # Default examples
        default_examples = [
            {
                'problem': 'Write a function to add two numbers',
                'solution': '''
def add(a, b):
    """Add two numbers and return the result."""
    return a + b
'''
            },
            {
                'problem': 'Write a function to check if a number is even',
                'solution': '''
def is_even(n):
    """Check if a number is even."""
    return n % 2 == 0
'''
            },
            {
                'problem': 'Write a function to find the maximum of three numbers',
                'solution': '''
def max_of_three(a, b, c):
    """Find the maximum of three numbers."""
    return max(a, b, c)
'''
            }
        ]
        
        examples.extend(default_examples)
        
        # Load custom examples from file if exists
        examples_file = self.config.get('examples_file')
        if examples_file and Path(examples_file).exists():
            try:
                with open(examples_file, 'r') as f:
                    custom_examples = json.load(f)
                    examples.extend(custom_examples)
            except Exception as e:
                logger.error(f"Failed to load examples file: {e}")
        
        return examples

--------------------------------------------------------------------------------
FILE 60/71: src\prompts\strategies.py
--------------------------------------------------------------------------------


from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
import textwrap


class PromptStrategy(ABC):
    """Abstract base class for prompt strategies"""
    
    @abstractmethod
    def format(self, problem: Dict[str, Any], **kwargs) -> str:
        """Format the prompt"""
        pass


class ZeroShotStrategy(PromptStrategy):
    """Zero-shot prompting strategy"""
    
    def format(self, problem: Dict[str, Any], **kwargs) -> str:
        prompt = problem.get('prompt', '')
        
        # Add any additional context
        if 'context' in kwargs:
            prompt = f"{kwargs['context']}\n\n{prompt}"
        
        # Add instructions
        prompt += "\n\n# Complete the function below:\n"
        
        # Add function signature if available
        if 'function_signature' in problem:
            prompt += f"\n{problem['function_signature']}\n"
        
        return prompt


class FewShotStrategy(PromptStrategy):
    """Few-shot prompting strategy with examples"""
    
    def __init__(self, examples: List[Dict[str, str]]):
        self.examples = examples
    
    def format(self, problem: Dict[str, Any], **kwargs) -> str:
        # Use provided examples or default ones
        examples = kwargs.get('examples', self.examples)
        num_examples = kwargs.get('num_examples', 2)
        
        # Format examples
        examples_text = ""
        for i, example in enumerate(examples[:num_examples]):
            examples_text += f"""
Example {i + 1}:
Problem: {example['problem']}
Solution:
{textwrap.dedent(example['solution']).strip()}

"""
        
        # Add current problem
        prompt = f"""{examples_text}
Now solve this problem:

{problem.get('prompt', '')}

# Complete the function below:
"""
        
        # Add function signature if available
        if 'function_signature' in problem:
            prompt += f"\n{problem['function_signature']}\n"
        
        return prompt


class ChainOfThoughtStrategy(PromptStrategy):
    """Chain-of-thought prompting strategy"""
    
    def format(self, problem: Dict[str, Any], **kwargs) -> str:
        prompt = f"""{problem.get('prompt', '')}

# Let's think through this step by step:

# Step 1: Understand the problem requirements
- What inputs does the function need?
- What output should it produce?
- Are there any edge cases to consider?

# Step 2: Plan the approach
- What algorithm or method should we use?
- What are the key steps in the solution?

# Step 3: Write the solution
"""
        
        # Add function signature if available
        if 'function_signature' in problem:
            prompt += f"\n{problem['function_signature']}\n"
        
        return prompt


class InstructionStrategy(PromptStrategy):
    """Instruction-based prompting with detailed guidelines"""
    
    def format(self, problem: Dict[str, Any], **kwargs) -> str:
        instructions = kwargs.get('instructions', """
Follow these guidelines when writing the solution:
1. Write clean, readable code with appropriate comments
2. Handle edge cases (empty input, invalid values)
3. Use descriptive variable names
4. Follow PEP 8 style guidelines
5. Include docstrings for the function
""")
        
        prompt = f"""{instructions}

Now solve this problem:

{problem.get('prompt', '')}

# Complete the function below:
"""
        
        # Add function signature if available
        if 'function_signature' in problem:
            prompt += f"\n{problem['function_signature']}\n"
        
        return prompt


class CodeReviewStrategy(PromptStrategy):
    """Strategy that asks the model to review and improve its own code"""
    
    def format(self, problem: Dict[str, Any], **kwargs) -> str:
        prompt = f"""{problem.get('prompt', '')}

# First, write a solution to this problem:

"""
        
        # Add function signature if available
        if 'function_signature' in problem:
            prompt += f"\n{problem['function_signature']}\n"
        
        prompt += """
# Now, review your solution and identify any issues:
# - Are there any edge cases not handled?
# - Is the code efficient?
# - Is it readable and well-documented?

# Finally, provide an improved version of the solution:
"""
        
        return prompt

--------------------------------------------------------------------------------
FILE 61/71: src\utils\__init__.py
--------------------------------------------------------------------------------

"""Utility functions and classes."""

from .disk_space_manager import DiskSpaceManager
from .debug_timer import DebugTimer
from .validators import Validators

__all__ = ['DiskSpaceManager', 'DebugTimer', 'Validators']

--------------------------------------------------------------------------------
FILE 62/71: src\utils\debug_timer.py
--------------------------------------------------------------------------------


import time
import logging
from contextlib import ContextDecorator
from functools import wraps
from typing import Optional, Dict, Any
from collections import defaultdict

logger = logging.getLogger(__name__)


class DebugTimer(ContextDecorator):
    """Context manager and decorator for timing operations"""
    
    def __init__(self, operation_name: str, log_level: str = 'info'):
        self.operation_name = operation_name
        self.log_level = log_level
        self.start_time = None
        self.elapsed = None
        
        # Statistics tracking
        self.stats = defaultdict(list)

    def __enter__(self):
        self.start_time = time.time()
        self._log(f"STARTING: {self.operation_name}")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.elapsed = time.time() - self.start_time
        
        if exc_type:
            self._log(f"FAILED: {self.operation_name} after {self.elapsed:.2f}s - {exc_val}")
        else:
            self._log(f"COMPLETED: {self.operation_name} in {self.elapsed:.2f}s")
            
        # Track statistics
        self.stats[self.operation_name].append(self.elapsed)

    def __call__(self, func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            with self:
                return func(*args, **kwargs)
        return wrapper

    def _log(self, message: str):
        """Log message at specified level"""
        log_func = getattr(logger, self.log_level.lower(), logger.info)
        log_func(message)

    def get_stats(self) -> Dict[str, Any]:
        """Get timing statistics"""
        stats = {}
        for op_name, times in self.stats.items():
            if times:
                stats[op_name] = {
                    'count': len(times),
                    'total': sum(times),
                    'average': sum(times) / len(times),
                    'min': min(times),
                    'max': max(times),
                    'last': times[-1]
                }
        return stats

    def reset(self):
        """Reset statistics"""
        self.stats.clear()


class PerformanceMonitor:
    """Monitor performance of multiple operations"""
    
    def __init__(self):
        self.timers: Dict[str, DebugTimer] = {}
        self.results: Dict[str, list] = defaultdict(list)

    def start(self, operation: str) -> DebugTimer:
        """Start timing an operation"""
        timer = DebugTimer(operation)
        timer.__enter__()
        self.timers[operation] = timer
        return timer

    def stop(self, operation: str) -> float:
        """Stop timing an operation"""
        if operation in self.timers:
            timer = self.timers.pop(operation)
            timer.__exit__(None, None, None)
            self.results[operation].append(timer.elapsed)
            return timer.elapsed
        return 0.0

    def get_report(self) -> Dict[str, Any]:
        """Get performance report"""
        report = {}
        for operation, times in self.results.items():
            if times:
                report[operation] = {
                    'count': len(times),
                    'total_time': sum(times),
                    'average_time': sum(times) / len(times),
                    'min_time': min(times),
                    'max_time': max(times)
                }
        return report

    def print_report(self):
        """Print performance report"""
        report = self.get_report()
        
        print("\n" + "="*60)
        print("PERFORMANCE REPORT")
        print("="*60)
        
        for operation, stats in report.items():
            print(f"\n{operation}:")
            print(f"  Count: {stats['count']}")
            print(f"  Total: {stats['total_time']:.2f}s")
            print(f"  Average: {stats['average_time']:.2f}s")
            print(f"  Min: {stats['min_time']:.2f}s")
            print(f"  Max: {stats['max_time']:.2f}s")
        
        print("\n" + "="*60)


def time_function(func):
    """Decorator to time a function"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        timer = DebugTimer(func.__name__)
        with timer:
            return func(*args, **kwargs)
    return wrapper


class ProgressTracker:
    """Track progress of long-running operations"""
    
    def __init__(self, total: int, description: str = "Progress"):
        self.total = total
        self.current = 0
        self.description = description
        self.start_time = time.time()
        self.last_update = self.start_time

    def update(self, amount: int = 1):
        """Update progress"""
        self.current += amount
        self.last_update = time.time()

    def get_progress(self) -> float:
        """Get progress percentage"""
        return (self.current / self.total) * 100 if self.total > 0 else 0

    def get_eta(self) -> float:
        """Get estimated time remaining in seconds"""
        if self.current == 0:
            return 0
        
        elapsed = time.time() - self.start_time
        rate = self.current / elapsed
        remaining = (self.total - self.current) / rate if rate > 0 else 0
        
        return remaining

    def format_eta(self) -> str:
        """Format ETA as human-readable string"""
        eta = self.get_eta()
        
        if eta < 60:
            return f"{eta:.0f}s"
        elif eta < 3600:
            return f"{eta/60:.1f}m"
        else:
            return f"{eta/3600:.1f}h"

    def get_status(self) -> Dict[str, Any]:
        """Get status dictionary"""
        return {
            'description': self.description,
            'current': self.current,
            'total': self.total,
            'percentage': self.get_progress(),
            'elapsed': time.time() - self.start_time,
            'eta': self.get_eta(),
            'eta_formatted': self.format_eta()
        }

    def print_status(self):
        """Print current status"""
        status = self.get_status()
        print(f"\r{status['description']}: {status['current']}/{status['total']} "
              f"({status['percentage']:.1f}%) - ETA: {status['eta_formatted']}", end='')

--------------------------------------------------------------------------------
FILE 63/71: src\utils\disk_space_manager.py
--------------------------------------------------------------------------------


import shutil
import psutil
import logging
from typing import List, Dict, Optional, Tuple, Any

logger = logging.getLogger(__name__)


class DiskSpaceManager:
    """Manages disk space and model selection based on available space"""
    
    # Model hierarchy from largest to smallest (approximate sizes in GB)
    MODEL_HIERARCHY = [
        {"name": "codellama:34b", "size_gb": 20, "priority": 1},
        {"name": "codellama:13b", "size_gb": 7, "priority": 2},
        {"name": "codellama:7b", "size_gb": 4, "priority": 3},
        {"name": "deepseek-coder:6.7b", "size_gb": 4, "priority": 3},
        {"name": "mistral:7b", "size_gb": 4, "priority": 3},
        {"name": "llama2:7b", "size_gb": 4, "priority": 3},
        {"name": "phi:2.7b", "size_gb": 2, "priority": 4},
        {"name": "tinyllama:1.1b", "size_gb": 0.7, "priority": 5},
        {"name": "starcoder:1b", "size_gb": 0.6, "priority": 5},
        {"name": "qwen:0.5b", "size_gb": 0.3, "priority": 6},
    ]
    
    # Required buffer space in GB
    REQUIRED_BUFFER_GB = 2.0
    
    def __init__(self, base_path: str = "."):
        self.base_path = base_path

    @staticmethod
    def get_available_space_gb(path: str = ".") -> Optional[float]:
        """Get available disk space in GB"""
        try:
            disk_usage = shutil.disk_usage(path)
            available_gb = disk_usage.free / (1024**3)
            return available_gb
        except Exception as e:
            logger.warning(f"Could not check disk space: {e}")
            return None

    @staticmethod
    def get_available_ram_gb() -> Optional[float]:
        """Get available RAM in GB"""
        try:
            mem = psutil.virtual_memory()
            available_ram_gb = mem.available / (1024**3)
            return available_ram_gb
        except Exception as e:
            logger.warning(f"Could not check RAM: {e}")
            return None

    @staticmethod
    def get_disk_usage_percent(path: str = ".") -> Optional[float]:
        """Get disk usage percentage"""
        try:
            disk_usage = shutil.disk_usage(path)
            return (disk_usage.used / disk_usage.total) * 100
        except Exception as e:
            logger.warning(f"Could not check disk usage: {e}")
            return None

    def select_models_based_on_space(
        self,
        requested_models: List[str],
        available_gb: Optional[float] = None,
        available_ram_gb: Optional[float] = None
    ) -> List[str]:
        """Select appropriate models based on available space"""
        if available_gb is None:
            available_gb = self.get_available_space_gb(self.base_path)
        
        if available_ram_gb is None:
            available_ram_gb = self.get_available_ram_gb()
        
        logger.info(f"System Resources - Disk: {available_gb:.2f} GB free, "
                   f"RAM: {available_ram_gb:.2f} GB available")
        
        if available_gb is None:
            logger.warning("Could not determine disk space, using requested models")
            return requested_models
        
        # Check minimum space for smallest model
        smallest_model = min(self.MODEL_HIERARCHY, key=lambda x: x["size_gb"])
        min_required = smallest_model["size_gb"] + self.REQUIRED_BUFFER_GB
        
        if available_gb < min_required:
            logger.error(f"Insufficient disk space. Need at least {min_required:.1f} GB, "
                        f"but only {available_gb:.1f} GB available")
            return []
        
        # Sort models by priority
        sorted_models = sorted(self.MODEL_HIERARCHY, key=lambda x: x["priority"])
        
        # Filter to requested models
        available_models = [
            m for m in sorted_models
            if m["name"] in requested_models or not requested_models
        ]
        
        if not available_models and requested_models:
            logger.warning("Requested models not in hierarchy, using defaults")
            available_models = sorted_models
        
        # Select models that fit
        selected_models = []
        total_size_gb = 0
        
        for model in available_models:
            model_size = model["size_gb"]
            
            # Check if model fits
            required_space = total_size_gb + model_size + self.REQUIRED_BUFFER_GB
            required_ram = model_size * 1.5  # Models need RAM for loading
            
            if required_space <= available_gb and required_ram <= available_ram_gb:
                selected_models.append(model["name"])
                total_size_gb += model_size
                logger.debug(f"Selected model: {model['name']} ({model_size} GB)")
        
        if not selected_models:
            # Try smallest model
            for model in reversed(available_models):
                if model["size_gb"] + self.REQUIRED_BUFFER_GB <= available_gb:
                    selected_models = [model["name"]]
                    logger.info(f"Selected minimal model: {model['name']}")
                    break
        
        logger.info(f"Selected {len(selected_models)} models: {selected_models}")
        logger.info(f"Estimated total model size: {total_size_gb:.1f} GB")
        
        return selected_models

    def estimate_file_space(
        self,
        num_models: int,
        num_problems: int,
        samples_per_problem: int,
        bytes_per_file: int = 5000
    ) -> Dict[str, float]:
        """Estimate space needed for generated files"""
        total_files = num_models * num_problems * samples_per_problem
        total_bytes = total_files * bytes_per_file
        
        return {
            'files': total_files,
            'bytes': total_bytes,
            'kb': total_bytes / 1024,
            'mb': total_bytes / (1024 * 1024),
            'gb': total_bytes / (1024 * 1024 * 1024)
        }

    def check_model_availability(
        self,
        model_manager,
        requested_models: List[str]
    ) -> Tuple[List[str], Dict[str, Any]]:
        """Check model availability with space-based fallback"""
        logger.info("Checking model availability with space-based fallback...")
        
        # Check available space
        available_gb = self.get_available_space_gb(self.base_path)
        available_ram_gb = self.get_available_ram_gb()
        usage_percent = self.get_disk_usage_percent(self.base_path)
        
        space_info = {
            'disk_gb': available_gb,
            'ram_gb': available_ram_gb,
            'usage_percent': usage_percent
        }
        
        # Select models based on space
        space_based_models = self.select_models_based_on_space(
            requested_models, available_gb, available_ram_gb
        )
        
        if not space_based_models:
            logger.error("No models can fit in available space!")
            return [], space_info
        
        # Check API availability
        available_api_models = model_manager.get_available_models()
        logger.info(f"API available models: {available_api_models}")
        
        # Filter to available models
        final_models = [
            m for m in space_based_models
            if m in available_api_models
        ]
        
        if not final_models and space_based_models:
            logger.warning("No space-appropriate models available via API")
            # Try to pull a model
            for model in space_based_models:
                try:
                    logger.info(f"Attempting to pull model: {model}")
                    if model_manager.pull_model(model):
                        final_models = [model]
                        logger.info(f"Successfully pulled model: {model}")
                        break
                except Exception as e:
                    logger.warning(f"Failed to pull model {model}: {e}")
        
        logger.info(f"Final model selection: {final_models}")
        return final_models, space_info

    def get_cleanup_recommendations(self) -> List[Dict[str, Any]]:
        """Get recommendations for freeing up space"""
        recommendations = []
        
        # Check temp directories
        temp_paths = ['/tmp', './cache', './logs', './results']
        for path in temp_paths:
            if os.path.exists(path):
                size = self._get_directory_size(path)
                if size > 1024 * 1024 * 1024:  # > 1GB
                    recommendations.append({
                        'path': path,
                        'size_gb': size / (1024**3),
                        'action': f"Clean up {path} directory"
                    })
        
        # Check Docker images
        try:
            import docker
            client = docker.from_env()
            images = client.images.list()
            total_size = sum(img.attrs['Size'] for img in images)
            
            if total_size > 5 * 1024**3:  # > 5GB
                recommendations.append({
                    'path': 'docker',
                    'size_gb': total_size / (1024**3),
                    'action': "Run 'docker system prune' to clean unused images"
                })
        except:
            pass
        
        return recommendations

    def _get_directory_size(self, path: str) -> int:
        """Get total size of directory in bytes"""
        total = 0
        try:
            for entry in os.scandir(path):
                if entry.is_file():
                    total += entry.stat().st_size
                elif entry.is_dir():
                    total += self._get_directory_size(entry.path)
        except:
            pass
        return total

--------------------------------------------------------------------------------
FILE 64/71: src\utils\validators.py
--------------------------------------------------------------------------------


import re
import ast
from typing import Any, Dict, List, Optional, Union
from datetime import datetime


class Validators:
    """Collection of validation methods"""
    
    @staticmethod
    def validate_email(email: str) -> bool:
        """Validate email format"""
        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return bool(re.match(pattern, email))

    @staticmethod
    def validate_python_code(code: str) -> Dict[str, Any]:
        """Validate Python code syntax"""
        result = {
            'valid': False,
            'error': None,
            'error_line': None,
            'error_type': None
        }
        
        try:
            ast.parse(code)
            result['valid'] = True
        except SyntaxError as e:
            result['error'] = str(e)
            result['error_line'] = e.lineno
            result['error_type'] = 'syntax_error'
        except Exception as e:
            result['error'] = str(e)
            result['error_type'] = 'unknown_error'
        
        return result

    @staticmethod
    def validate_model_name(model_name: str) -> bool:
        """Validate model name format"""
        # Model names typically: provider:name or provider/name
        pattern = r'^[a-zA-Z0-9_\-]+[:/][a-zA-Z0-9_\-]+$'
        return bool(re.match(pattern, model_name))

    @staticmethod
    def validate_file_path(path: str, must_exist: bool = False) -> bool:
        """Validate file path"""
        import os
        if must_exist:
            return os.path.exists(path)
        # Check if path is valid (no invalid characters)
        invalid_chars = '<>:"|?*'
        return not any(c in path for c in invalid_chars)

    @staticmethod
    def validate_json_schema(data: Dict, schema: Dict) -> List[str]:
        """Validate data against JSON schema"""
        errors = []
        
        def validate_type(value, expected_type, path):
            if expected_type == 'string' and not isinstance(value, str):
                errors.append(f"{path}: expected string, got {type(value).__name__}")
            elif expected_type == 'number' and not isinstance(value, (int, float)):
                errors.append(f"{path}: expected number, got {type(value).__name__}")
            elif expected_type == 'integer' and not isinstance(value, int):
                errors.append(f"{path}: expected integer, got {type(value).__name__}")
            elif expected_type == 'boolean' and not isinstance(value, bool):
                errors.append(f"{path}: expected boolean, got {type(value).__name__}")
            elif expected_type == 'array' and not isinstance(value, list):
                errors.append(f"{path}: expected array, got {type(value).__name__}")
            elif expected_type == 'object' and not isinstance(value, dict):
                errors.append(f"{path}: expected object, got {type(value).__name__}")
        
        def validate_required(data, schema, path=''):
            for key, rules in schema.get('properties', {}).items():
                if key in data:
                    value = data[key]
                    if 'type' in rules:
                        validate_type(value, rules['type'], f"{path}.{key}" if path else key)
                    if 'pattern' in rules and isinstance(value, str):
                        if not re.match(rules['pattern'], value):
                            errors.append(f"{path}.{key}: does not match pattern {rules['pattern']}")
                    if 'min' in rules and isinstance(value, (int, float)):
                        if value < rules['min']:
                            errors.append(f"{path}.{key}: less than minimum {rules['min']}")
                    if 'max' in rules and isinstance(value, (int, float)):
                        if value > rules['max']:
                            errors.append(f"{path}.{key}: greater than maximum {rules['max']}")
                elif key in schema.get('required', []):
                    errors.append(f"{path}.{key}: required field missing")
        
        validate_required(data, schema)
        return errors

    @staticmethod
    def validate_date(date_str: str, format: str = '%Y-%m-%d') -> bool:
        """Validate date string format"""
        try:
            datetime.strptime(date_str, format)
            return True
        except ValueError:
            return False

    @staticmethod
    def validate_range(value: Union[int, float], min_val: Any, max_val: Any) -> bool:
        """Validate value is within range"""
        if min_val is not None and value < min_val:
            return False
        if max_val is not None and value > max_val:
            return False
        return True

    @staticmethod
    def validate_non_empty(value: Any) -> bool:
        """Validate value is not empty"""
        if value is None:
            return False
        if isinstance(value, str):
            return len(value.strip()) > 0
        if isinstance(value, (list, dict, tuple, set)):
            return len(value) > 0
        return True

    @staticmethod
    def validate_identifier(name: str) -> bool:
        """Validate Python identifier"""
        if not name:
            return False
        return name.isidentifier()

    @staticmethod
    def validate_url(url: str) -> bool:
        """Validate URL format"""
        pattern = r'^https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+[^\s]*$'
        return bool(re.match(pattern, url))

    @staticmethod
    def validate_port(port: int) -> bool:
        """Validate port number"""
        return 1 <= port <= 65535

    @staticmethod
    def validate_config(config: Dict[str, Any]) -> List[str]:
        """Validate configuration dictionary"""
        errors = []
        
        # Required sections
        required_sections = ['paths', 'models', 'evaluation', 'dashboard']
        for section in required_sections:
            if section not in config:
                errors.append(f"Missing required section: {section}")
        
        # Validate paths
        if 'paths' in config:
            paths = config['paths']
            required_paths = ['data_dir', 'results_dir']
            for path in required_paths:
                if path not in paths:
                    errors.append(f"Missing required path: {path}")
        
        # Validate models
        if 'models' in config:
            models = config['models']
            if 'default_models' in models and not isinstance(models['default_models'], list):
                errors.append("default_models must be a list")
        
        # Validate evaluation config
        if 'evaluation' in config:
            eval_config = config['evaluation']
            if 'timeout_seconds' in eval_config:
                timeout = eval_config['timeout_seconds']
                if not isinstance(timeout, (int, float)) or timeout <= 0:
                    errors.append("timeout_seconds must be positive number")
        
        return errors

--------------------------------------------------------------------------------
FILE 65/71: src\version.py
--------------------------------------------------------------------------------

"""
Version information and utilities.
"""

__version__ = '1.0.0'
__version_info__ = (1, 0, 0)
__release__ = 'stable'


def get_version_string() -> str:
    """Get full version string."""
    return f"{__version__} ({__release__})"


def compare_versions(version1: str, version2: str) -> int:
    """Compare two version strings.
    
    Returns:
        -1 if version1 < version2
         0 if version1 == version2
         1 if version1 > version2
    """
    def parse_version(v):
        return tuple(map(int, (v.split("."))))
    
    v1 = parse_version(version1)
    v2 = parse_version(version2)
    
    if v1 < v2:
        return -1
    elif v1 > v2:
        return 1
    else:
        return 0


if __name__ == '__main__':
    print(f"AI Model Evaluation Framework v{get_version_string()}")

--------------------------------------------------------------------------------
FILE 66/71: test_windows.py
--------------------------------------------------------------------------------

# test_windows_fixed.py
import asyncio
import sys
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent / 'src'))

from src.loaders import HumanEvalLoader
from src.executors import SandboxExecutor

async def test_fixed_sandbox():
    print("=" * 60)
    print("FIXED SANDBOX EXECUTION TEST")
    print("=" * 60)
    
    # Load dataset
    loader = HumanEvalLoader({
        'repo_url': 'https://github.com/openai/human-eval.git',
        'data_dir': 'data/human_eval'
    })
    
    problems = loader.load_dataset()
    problem = problems[0]  # First problem
    
    print(f"\nTesting problem: {problem.task_id}")
    print(f"Entry point: {problem.entry_point}")
    
    # Create sandbox with proper settings
    sandbox = SandboxExecutor(
        timeout=10,
        memory_limit="512m",
        cpu_limit=1.0,
        network_enabled=False
    )
    
    # TEST 1: Fix the canonical solution format
    print("\n--- Test 1: Fixed canonical solution ---")
    
    # The canonical solution might just be the function body
    # We need to wrap it with the function signature
    if problem.canonical_solution and problem.entry_point:
        # Create full function with signature
        full_code = f"""
from typing import List

def {problem.entry_point}(numbers: List[float], threshold: float) -> bool:
{problem.canonical_solution}
"""
        print("Running with properly formatted code...")
        
        result = await sandbox.execute_safely(
            code=full_code,
            test_cases=problem.test_cases,
            problem_id=problem.problem_id,
            language="python"
        )
        
        print(f"  Passed: {result.get('passed', False)}")
        print(f"  Result: {result.get('result', 'unknown')}")
        print(f"  Time: {result.get('execution_time_ms', 0):.2f} ms")
        
        if result.get('test_results'):
            passed = sum(1 for t in result['test_results'] if t.get('passed'))
            total = len(result['test_results'])
            print(f"  Tests passed: {passed}/{total}")
            
            # Show which tests failed
            for i, test in enumerate(result['test_results']):
                if not test.get('passed'):
                    print(f"    Test {i+1} failed: {test.get('message', '')[:100]}")
    
    # TEST 2: Test with a known working solution
    print("\n--- Test 2: Simple working solution ---")
    
    simple_code = """
from typing import List

def has_close_elements(numbers: List[float], threshold: float) -> bool:
    for i in range(len(numbers)):
        for j in range(i + 1, len(numbers)):
            if abs(numbers[i] - numbers[j]) < threshold:
                return True
    return False
"""
    
    result = await sandbox.execute_safely(
        code=simple_code,
        test_cases=problem.test_cases,
        problem_id=problem.problem_id,
        language="python"
    )
    
    print(f"  Passed: {result.get('passed', False)}")
    print(f"  Time: {result.get('execution_time_ms', 0):.2f} ms")
    
    if result.get('test_results'):
        passed = sum(1 for t in result['test_results'] if t.get('passed'))
        total = len(result['test_results'])
        print(f"  Tests passed: {passed}/{total}")
    
    # TEST 3: Test with the problematic infinite loop
    print("\n--- Test 3: Fixed timeout test ---")
    
    # Instead of an infinite loop, use a loop that will timeout
    timeout_code = """
import time
counter = 0
while counter < 1000000:  # This will finish quickly
    counter += 1
print("Loop completed")
"""
    
    result = await sandbox.execute_safely(
        code=timeout_code,
        test_cases=[],
        problem_id="timeout_test",
        language="python"
    )
    
    print(f"  Passed: {result.get('passed', False)}")
    print(f"  Result: {result.get('result', 'unknown')}")
    print(f"  Output: {result.get('output', '')[:100]}")
    print(f"  Time: {result.get('execution_time_ms', 0):.2f} ms")
    
    sandbox.cleanup()

async def test_with_multiple_problems():
    """Test the sandbox with multiple problems"""
    print("\n" + "=" * 60)
    print("TESTING WITH MULTIPLE PROBLEMS")
    print("=" * 60)
    
    loader = HumanEvalLoader({
        'data_dir': 'data/human_eval'
    })
    
    problems = loader.load_dataset()
    sandbox = SandboxExecutor(timeout=5, memory_limit="256m")
    
    # Test first 3 problems
    for i, problem in enumerate(problems[:3]):
        print(f"\n--- Problem {i+1}: {problem.task_id} ---")
        
        # Create a simple solution based on the problem
        if problem.entry_point == "has_close_elements":
            code = """
from typing import List
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    for i in range(len(numbers)):
        for j in range(i + 1, len(numbers)):
            if abs(numbers[i] - numbers[j]) < threshold:
                return True
    return False
"""
        elif problem.entry_point == "separate_paren_groups":
            code = """
from typing import List
def separate_paren_groups(paren_string: str) -> List[str]:
    groups = []
    current = []
    balance = 0
    for char in paren_string:
        if char == '(':
            current.append(char)
            balance += 1
        elif char == ')':
            current.append(char)
            balance -= 1
            if balance == 0:
                groups.append(''.join(current))
                current = []
    return groups
"""
        else:
            # Generic solution
            code = f"""
def {problem.entry_point}(*args, **kwargs):
    return None
"""
        
        result = await sandbox.execute_safely(
            code=code,
            test_cases=problem.test_cases,
            problem_id=problem.problem_id,
            language="python"
        )
        
        print(f"  Passed: {result.get('passed', False)}")
        if result.get('test_results'):
            passed = sum(1 for t in result['test_results'] if t.get('passed'))
            total = len(result['test_results'])
            print(f"  Tests: {passed}/{total}")
    
    sandbox.cleanup()

async def debug_docker_issue():
    """Debug the Docker container error"""
    print("\n" + "=" * 60)
    print("DEBUGGING DOCKER ISSUE")
    print("=" * 60)
    
    import docker
    
    try:
        client = docker.from_env()
        print("âœ… Docker client created")
        
        # Test a simple container
        print("\nTesting simple container...")
        container = client.containers.run(
            "python:3.9-slim",
            "python -c 'print(\"Hello from Docker\")'",
            remove=True
        )
        print(f"  Output: {container}")
        
        # Check available images
        print("\nAvailable Python images:")
        images = client.images.list()
        for img in images:
            if 'python' in str(img.tags):
                print(f"  {img.tags}")
        
    except Exception as e:
        print(f"âŒ Docker error: {e}")

if __name__ == "__main__":
    print("Choose test to run:")
    print("1. Fixed sandbox test")
    print("2. Multiple problems test")
    print("3. Debug Docker issue")
    
    choice = input("\nEnter choice (1-3): ").strip()
    
    if choice == '1':
        asyncio.run(test_fixed_sandbox())
    elif choice == '2':
        asyncio.run(test_with_multiple_problems())
    elif choice == '3':
        asyncio.run(debug_docker_issue())
    else:
        print("Invalid choice")

--------------------------------------------------------------------------------
FILE 67/71: tests\__init__.py
--------------------------------------------------------------------------------

"""Tests package."""
import unittest
import sys
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))


def discover_tests():
    """Discover and return all tests in the tests package."""
    loader = unittest.TestLoader()
    suite = loader.discover('.', pattern='test_*.py')
    return suite


def run_all_tests():
    """Run all tests in the package."""
    runner = unittest.TextTestRunner(verbosity=2)
    runner.run(discover_tests())


if __name__ == '__main__':
    run_all_tests()

--------------------------------------------------------------------------------
FILE 68/71: tests\test_adapters\__init__.py
--------------------------------------------------------------------------------

"""Test suite for adapters."""
import unittest
from unittest.mock import Mock, patch, MagicMock

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.adapters import ModelAdapter, ModelRegistry


class TestModelAdapter(unittest.TestCase):
    """Test cases for ModelAdapter abstract class."""
    
    def test_adapter_is_abstract(self):
        """Test that ModelAdapter cannot be instantiated directly."""
        with self.assertRaises(TypeError):
            ModelAdapter()
    
    def test_adapter_requires_load_model(self):
        """Test that subclasses must implement load_model."""
        class IncompleteAdapter(ModelAdapter):
            def generate(self, prompt):
                pass
        
        with self.assertRaises(TypeError):
            IncompleteAdapter()
    
    def test_adapter_requires_generate(self):
        """Test that subclasses must implement generate."""
        class IncompleteAdapter(ModelAdapter):
            def load_model(self):
                pass
        
        with self.assertRaises(TypeError):
            IncompleteAdapter()


class TestModelRegistry(unittest.TestCase):
    """Test cases for ModelRegistry."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.registry = ModelRegistry()
    
    def test_registry_creation(self):
        """Test creating a model registry."""
        self.assertIsNotNone(self.registry)
    
    def test_register_adapter(self):
        """Test registering an adapter."""
        mock_adapter_class = Mock()
        self.registry.register('test_model', mock_adapter_class)
        
        retrieved = self.registry.get('test_model')
        self.assertEqual(retrieved, mock_adapter_class)
    
    def test_get_nonexistent_adapter(self):
        """Test getting a non-existent adapter."""
        result = self.registry.get('nonexistent')
        self.assertIsNone(result)
    
    def test_multiple_adapters(self):
        """Test registering multiple adapters."""
        mock_adapter1 = Mock()
        mock_adapter2 = Mock()
        
        self.registry.register('model1', mock_adapter1)
        self.registry.register('model2', mock_adapter2)
        
        self.assertEqual(self.registry.get('model1'), mock_adapter1)
        self.assertEqual(self.registry.get('model2'), mock_adapter2)


if __name__ == '__main__':
    unittest.main()

--------------------------------------------------------------------------------
FILE 69/71: tests\test_entities\__init__.py
--------------------------------------------------------------------------------

"""Test suite for entities."""
import unittest
from datetime import datetime
from unittest.mock import Mock, patch, MagicMock

# Import entities
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.entities import User, Evaluation, Problem, EvaluationResult, Metric, Error, Report, Benchmark


class TestUser(unittest.TestCase):
    """Test cases for User entity."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.user_data = {
            'username': 'test_user',
            'email': 'test@example.com'
        }
    
    def test_user_creation(self):
        """Test creating a user."""
        user = User(**self.user_data)
        self.assertEqual(user.username, 'test_user')
        self.assertEqual(user.email, 'test@example.com')
    
    def test_user_password_hashing(self):
        """Test user password hashing."""
        user = User(username='test', email='test@example.com', password='secret123')
        self.assertIsNotNone(user.get_password_hash())


class TestEvaluation(unittest.TestCase):
    """Test cases for Evaluation entity."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.eval_data = {
            'name': 'Test Evaluation',
            'model': 'test-model'
        }
    
    def test_evaluation_creation(self):
        """Test creating an evaluation."""
        evaluation = Evaluation(**self.eval_data)
        self.assertEqual(evaluation.name, 'Test Evaluation')
        self.assertEqual(evaluation.model, 'test-model')


class TestProblem(unittest.TestCase):
    """Test cases for Problem entity."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.problem_data = {
            'title': 'Test Problem',
            'description': 'A test problem',
            'input_spec': 'int -> int',
            'output_spec': 'int'
        }
    
    def test_problem_creation(self):
        """Test creating a problem."""
        problem = Problem(**self.problem_data)
        self.assertEqual(problem.title, 'Test Problem')
        self.assertEqual(problem.description, 'A test problem')


class TestEvaluationResult(unittest.TestCase):
    """Test cases for EvaluationResult entity."""
    
    def test_result_creation(self):
        """Test creating an evaluation result."""
        result = EvaluationResult(
            problem_id='p1',
            solution='test_code',
            status='passed'
        )
        self.assertEqual(result.problem_id, 'p1')
        self.assertEqual(result.status, 'passed')


class TestMetric(unittest.TestCase):
    """Test cases for Metric entity."""
    
    def test_metric_creation(self):
        """Test creating a metric."""
        metric = Metric(name='pass_rate', value=0.85)
        self.assertEqual(metric.name, 'pass_rate')
        self.assertEqual(metric.value, 0.85)


class TestError(unittest.TestCase):
    """Test cases for Error entity."""
    
    def test_error_creation(self):
        """Test creating an error."""
        error = Error(
            type='SyntaxError',
            message='Invalid syntax',
            traceback='...'
        )
        self.assertEqual(error.type, 'SyntaxError')
        self.assertEqual(error.message, 'Invalid syntax')


class TestReport(unittest.TestCase):
    """Test cases for Report entity."""
    
    def test_report_creation(self):
        """Test creating a report."""
        report = Report(
            title='Evaluation Report',
            evaluation_id='eval1'
        )
        self.assertEqual(report.title, 'Evaluation Report')
        self.assertEqual(report.evaluation_id, 'eval1')


class TestBenchmark(unittest.TestCase):
    """Test cases for Benchmark entity."""
    
    def test_benchmark_creation(self):
        """Test creating a benchmark."""
        benchmark = Benchmark(
            name='HumanEval',
            description='HumanEval benchmark'
        )
        self.assertEqual(benchmark.name, 'HumanEval')


if __name__ == '__main__':
    unittest.main()

--------------------------------------------------------------------------------
FILE 70/71: tests\test_executors\__init__.py
--------------------------------------------------------------------------------

"""Test suite for executors."""
import unittest
from unittest.mock import Mock, patch, MagicMock
import time

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.executors import SandboxExecutor, ResourceManager


class TestSandboxExecutor(unittest.TestCase):
    """Test cases for SandboxExecutor."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.executor = SandboxExecutor()
    
    def test_executor_creation(self):
        """Test creating a sandbox executor."""
        self.assertIsNotNone(self.executor)
    
    def test_execute_simple_code(self):
        """Test executing simple Python code."""
        code = "x = 1 + 1"
        result = self.executor.execute(code)
        self.assertIsNotNone(result)
    
    def test_execute_with_timeout(self):
        """Test executing code with timeout."""
        code = "import time; time.sleep(1)"
        result = self.executor.execute_with_timeout(code, timeout=2)
        self.assertIsNotNone(result)
    
    def test_timeout_exceeded(self):
        """Test handling timeout exceeded."""
        code = "import time; time.sleep(10)"
        # Should handle timeout gracefully
        try:
            result = self.executor.execute_with_timeout(code, timeout=0.1)
        except Exception as e:
            # Expected to timeout
            self.assertIn('timeout', str(e).lower())
    
    def test_execute_with_error(self):
        """Test executing code that raises an error."""
        code = "raise ValueError('test error')"
        result = self.executor.execute(code)
        # Should capture the error
        self.assertIsNotNone(result)


class TestResourceManager(unittest.TestCase):
    """Test cases for ResourceManager."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.manager = ResourceManager()
    
    def test_resource_manager_creation(self):
        """Test creating a resource manager."""
        self.assertIsNotNone(self.manager)
    
    def test_set_memory_limit(self):
        """Test setting memory limit."""
        self.manager.set_memory_limit('4GB')
        # Verify it was set
        self.assertIsNotNone(self.manager)
    
    def test_set_cpu_limit(self):
        """Test setting CPU limit."""
        self.manager.set_cpu_limit('2')
        self.assertIsNotNone(self.manager)
    
    def test_set_timeout(self):
        """Test setting timeout."""
        self.manager.set_timeout(30)
        self.assertIsNotNone(self.manager)
    
    def test_multiple_resource_limits(self):
        """Test setting multiple resource limits."""
        self.manager.set_memory_limit('2GB')
        self.manager.set_cpu_limit('1')
        self.manager.set_timeout(60)
        
        # All limits should be set
        self.assertIsNotNone(self.manager)


if __name__ == '__main__':
    unittest.main()

--------------------------------------------------------------------------------
FILE 71/71: tests\test_managers\__init__.py
--------------------------------------------------------------------------------

"""Test suite for managers."""
import unittest
from unittest.mock import Mock, patch, MagicMock

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.managers import EvaluationManager, ResultAggregator


class TestEvaluationManager(unittest.TestCase):
    """Test cases for EvaluationManager."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.manager = EvaluationManager()
    
    def test_manager_creation(self):
        """Test creating an evaluation manager."""
        self.assertIsNotNone(self.manager)
    
    def test_run_evaluation_integration(self):
        """Test running an evaluation."""
        mock_loader = Mock()
        mock_loader.load.return_value = []
        mock_adapter = Mock()
        mock_adapter.generate.return_value = 'test_output'
        
        # This would run the actual evaluation
        result = self.manager.run_evaluation(mock_loader, mock_adapter)
        self.assertIsNotNone(result)
    
    def test_evaluation_with_timeout(self):
        """Test evaluation handles timeouts."""
        mock_loader = Mock()
        mock_adapter = Mock()
        
        # Set timeout
        results = self.manager.run_evaluation(
            mock_loader, 
            mock_adapter,
            timeout=5
        )
        self.assertIsNotNone(results)


class TestResultAggregator(unittest.TestCase):
    """Test cases for ResultAggregator."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.aggregator = ResultAggregator()
    
    def test_aggregator_creation(self):
        """Test creating a result aggregator."""
        self.assertIsNotNone(self.aggregator)
    
    def test_aggregate_results(self):
        """Test aggregating results."""
        mock_results = [
            {'status': 'passed', 'time': 1.5},
            {'status': 'passed', 'time': 2.0},
            {'status': 'failed', 'time': 0.5}
        ]
        
        aggregated = self.aggregator.aggregate(mock_results)
        self.assertIsNotNone(aggregated)
        self.assertIn('pass_rate', str(aggregated).lower() or 'pass' in aggregated)
    
    def test_calculate_statistics(self):
        """Test calculating statistics."""
        mock_results = [
            {'metric': 0.8},
            {'metric': 0.9},
            {'metric': 0.7}
        ]
        
        stats = self.aggregator.calculate_statistics(mock_results)
        self.assertIsNotNone(stats)


if __name__ == '__main__':
    unittest.main()

